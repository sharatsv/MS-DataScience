{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam files: 1500\n",
      "Number of ham files: 3672\n"
     ]
    }
   ],
   "source": [
    "# Final Project\n",
    "# Email SPAM filter\n",
    "import os\n",
    "\n",
    "# Read all the data from the spam and ham directories\n",
    "\n",
    "# start lists for spam and ham email texts\n",
    "hamtexts = []\n",
    "spamtexts = []\n",
    "    \n",
    "for file in os.listdir(\"./EmailSpamCorpora/corpus/spam\"):\n",
    "    if (file.endswith(\".txt\")):\n",
    "        f = open(\"EmailSpamCorpora/corpus/spam/\" + file, 'r', encoding=\"latin-1\")\n",
    "        spamtexts.append(f.read())\n",
    "        f.close()\n",
    "        \n",
    "for file in os.listdir(\"./EmailSpamCorpora/corpus/ham\"):\n",
    "    if (file.endswith(\".txt\")):\n",
    "        f = open(\"./EmailSpamCorpora/corpus/ham/\" + file, 'r', encoding=\"latin-1\")\n",
    "        hamtexts.append(f.read())\n",
    "        f.close()\n",
    "\n",
    "print(\"Number of spam files:\",len(spamtexts))\n",
    "print(\"Number of ham files:\", len(hamtexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens(All) = 355375 vs Tokens(Filtered) = 142146\n",
      "Overall reduction = 60.001125571579315\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Combine all the spam and ham into a single data-structure\n",
    "emaildocs = []\n",
    "\n",
    "# Before we create word tokens, let's filter the tokenized words for:\n",
    "# - stop-words (using nltk.stopwords)\n",
    "# - remove common email characters like Subject..\n",
    "# - remove characters like :.,$#?* etc.\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "my_words = ['Subject', 'com', 'http', 'www']\n",
    "my_chars = [',', '.', '?', '%', '#', '*', '$', \n",
    "            ':', '/', '\\\\', ';', '&', '@', '-',\n",
    "            '\\'', '_', '[', ']', '(', ')',\n",
    "            '!', '\\'\\'', '``', '{', '}']\n",
    "stopwords.extend(my_words)\n",
    "stopwords.extend(my_chars)\n",
    "\n",
    "# Use the stopwords + filters and measure reduction benefit\n",
    "tokens_all_count = 0\n",
    "tokens_filter_count = 0\n",
    "for spam in spamtexts:\n",
    "    # word tokenize it\n",
    "    tokens_all = nltk.word_tokenize(spam)\n",
    "    # first, we will put all unique words in the email\n",
    "    tokens_unique = set(tokens_all)\n",
    "    tokens_filter = list(tokens_unique - set(stopwords))\n",
    "    tokens_all_count += len(tokens_all)\n",
    "    tokens_filter_count += len(tokens_filter)\n",
    "    # print(len(tokens_filter))\n",
    "    # break\n",
    "    #emaildocs.append((tokens, 'spam'))\n",
    "\n",
    "reduction_pct = (tokens_all_count - tokens_filter_count) / tokens_all_count * 100\n",
    "print('Tokens(All) = %d vs Tokens(Filtered) = %d' %(tokens_all_count, tokens_filter_count))\n",
    "\n",
    "print('Overall reduction = %s' %reduction_pct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['time', 'road', 'naturalgolden', 'ress', 'companion', 'find', 'terrific', 'youll', 'www', 'turn', 'bio', 'love', 'cam', 'developed', 'meganbang', 'site', 'movie', 'line', 'try', 'help', 'amazed', 'date', 'encomia', 'form', 'matter', 'catatonia', 'fashioned', 'intervenor', 'catfish', 'every', 'see', 'acc', 'brandywine', 'good', 'preemptive', 'babe', 'plz', 'war', 'shoehorn', 'word', 'sense', 'browser', 'ole', 'skeleton', 'retract', 'satisfaction', 'electrocardiograph', 'counterattack', 'biz', 'quick', 'scaup', 'evening', 'brand', 'lookup', 'created', 'copy', 'step', 'byrne', 'friendship', 'may', 'looking', 'pietism', 'anyone', 'pa', 'monster', 'ste', 'come', 'honeycomb', 'new', 'bld', 'add', 'aitken'], 'spam'), (['random', 'turnkey', 'want', 'money', 'future', 'like', 'mailings', 'tracking', 'userid', 'receive', 'would', 'complete', 'word', 'clck', 'express', 'learn', 'federal', 'videos', 'number', 'system', 'make', 'shipped', 'registering', 'fortune', 'via', 'thank', 'order', 'confirmation', 'fedex', 'january', 'ebay', 'software', 'turorials'], 'spam'), (['thoughts', 'take', 'food', 'free', 'removed', 'click', 'join', 'tour'], 'spam'), (['less', 'consultants', 'annum', '45', 'helpdesk', 'optionsaustralian', 'sulphide', 'cobalt', 'sq', 'submissions', 'mark', 'upgrade', 'grade', 'miner', 'ravenswood', 'wins', 'diligence', 'rich', '+', 'limited', 'new', 'oceania', 'mst', '13', 'prices', 'html', '140', 'assessments', 'ties', 'provide', 'march', 'resource', 'numbers', 'aiming', 'general', 'rig', 'optimisation', 'self', 'ore', '1848', 'subscriptions', 'brazilian', 'event', 'equivalent', 'rely', 'per', 'africa', 'news', 'abn', 'wheaton', 'expects', 'kaolin', 'consortium', 'dairi', 'operators', 'herald', 'project', 'reason', 'miningnews', 'wish', '78', 'program', 'safety', 'moves', 'exploration', 'delivery', 'chromium', 'visit', 'name', 'return', 'completed', 'heralds', 'resourcestocks', 'targets', 'cancel', 'today', 'conclude', 'pgm', '900', 'cash', 'stories', 'set', 'mines', 'contract', 'metal', 'eyes', 'removed', 'output', 'boost', 'focussed', 'mine', 'accept', 'dykes', 'commodities', 'service', 'underway', 'port', 'forgotten', 'aker', '2006', 'options', 'oilshale', 'mineralisation', 'km', 'could', 'significantly', 'facility', 'ounce', 'cases', 'events', 'boom', 'plain', 'orebody', 'send', 'estimate', '7', 'agreement', 'rc', 'attractive', 'hit', 'canada', 'fortitude', 'week', 'south', 'option', 'entered', 'alumina', 'manganese', 'offer', 'kpmg', '12', '1', 'receive', 'know', 'eritrea', 'text', 'miners', 'fox', 'minehaul', 'ipos', 'amapari', 'moonta', 'strong', 'river', 'net', 'password', '200', 'vanadium', 'debarwa', 'layout', 'massive', 'business', 'second', 'steel', 'drilling', 'newsletter', 'lodged', 'north', 'next', 'environment', 'kalgoorlie', 'ensure', 'china', 'please', 'midas', 'lifts', 'aspermont', 'minerals', 'due', 'increase', 'january', 'hillgrove', 'primex', 'follow', 'rare', 'businesses', 'diamonds', 'large', 'based', 'contracta', 'read', 'sees', 'po', 'passed', 'previously', 'arrive', '2004', 'western', 'goldstream', 'extended', 'releases', 'regions', 'yesterday', 'dryblower', 'production', 'bulk', 'warned', 'details', 'access', '2500', 'campaign', 'month', 'visually', 'risk', 'major', 'tracked', 'us', '8', 'subscription', 'email', 'best', 'head', 'full', 'country', 'format', 'resources', 'cost', 'begin', 'rab', 'economics', 'xstrata', 'reserve', 'drive', 'global', 'act', 'sahara', 'show', 'propelled', 'industry', 'sand', 'queensland', '9100', 'bauxite', 'bfp', 'ozpa', 'completion', 'australian', 'end', 'able', 'confined', 'allow', 'indonesia', 'copper', 'contractors', 'oz', 'pit', 'investment', 'tin', 'alum', 'security', 'fax', 'radio', 'strengthening', 'haulage', '90', 'services', 'e', 'website', 'mail', 'lead', 'features', 'room', 'americ', 'office', 'forecast', '048', 'expansion', 'belief', 'thought', 'breaks', 'omme', 'tuesday', 'story', 'support', 'emailing', 'postal', 'alunorte', 'marks', 'ground', 'west', 'relating', 'discontinue', 'prospect', 'monitor', 'section', 'resolute', 'acquisition', 'aware', 'mining', 'complimentary', 'australia', 'company', 'brazil', 'soil', 'supply', 'th', 'click', 'asia', 'may', 'deal', 'enhanced', 'anglo', 'earths', 'owned', 'iron', 'found', 'mineral', 'leederville', 'says', 'transport', 'boosts', 'including', 'tungsten', 'address', 'african', 'arranged', 'one', 'ounces', 'uranium', '375', 'wa', 'contact', 'alternatively', 'jumping', 'announcing', 'within', 'assets', 'region', 'sub', 'plans', 'need', 'space', '477', 'rise', 'silver', 'management', 'nickel', 'westonia', 'kvaerner', 'industries', 'surface', 'days', 'gemstone', 'let', 'home', 'walleroo', 'www', '9381', 'ajm', '61', 'gold', 'box', 'uses', 'conference', 'american', 'firm', 'dolerite', 'million', 'lift', 'shipping', 'expo', '9489', 'product', 'truncated', 'magnesium', 'conjunction', 'lmoz', 'europe', 'media', 'zinc', 'buy', '6902', 'base', 'hill', 'computer', 'mineable', 'commodity', 'press', 'coal', 'health', 'annual', 'portfolio', 'homecoming', 'high', 'available', '66', 'tel', 'continent', 'salt', '000', 'tantalum'], 'spam'), (['prlce', 'cheap', 'www', 'ta', 'perscriptions', 'pharmacy', 'want', 'would'], 'spam')]\n",
      "[(['p', 'transport', 'get', 'texas', 'farmer', 'sent', 'dudley', '08', 'working', 'markets', 'real', 'reliant', 'ran', 'help', 'could', 'scott', 'thanks', 'pat', 'one', 'hl', '7', 'agreement', 'something', 'exactly', 'begin', 'pipeline', 'lp', 'illinois', 'h', 'customers', 'waiting', 'looking', 'order', 'show', 'corporation', 'behalf', 'need', 'spot', 'new', 'southern', 'suggestions', 'customer', 'volumes', 'activity', 'sells', 'entered', 'someone', 'nowadays', 'showing', 'trying', 'contracts', 'hplc', 'ena', 'report', 'inc', 'missing', 'field', 'offer', 'brenda', '07', 'allow', 'anything', 'know', 'city', 'mentioned', 'hpl', 'subject', '99', 'fuel', 'make', 'sure', 'entex', 'clynes', 'marketing', 'else', 'panther', 'union', '11', 'right', 'j', 'pipe', 'ces', 'christi', 'long', 'tell', 'project', 'deals', 'services', 'run', '10', 'response', 'update', 'cheryl', 'counterparties', 'give', 'track', 'would', 'wondering', 'suggested', 'everything', 'altrade', 'c', 'transmission', 'chemicals', 'transports', 'king', 'term', '04', 'gulf', 'power', 'ideas', 'brazoria', 'carbide', 'ones', 'light', 'daren', 'pm', 'sitara', 'unit', '2000', 'mary', 'receptive', 'come', 'storage', 'gathering', 'status', 'ect', 'sell', 'check', 'following', 'buy', 'buys', 'utilities', 'based', 'forwarded', 'done', 'gas', 'familiar', 'accomplish', 'needs', '56', 'like', 'smith', 'since', 'sales', 'txu', 'energy', 'move', '05', 'draft', 'l', 'distribution', 'corpus', 'gets', 'selling', 'happen', 'company', 'mills', 'central', 'praxair', 'desk', 'eliminate', 'advance', 'cc', 'duke', 'contractual', 'hou', 'herod', 'transaction', 'equistar'], 'ham'), (['103', 'month', '3', '11', '29165', 'meters', '6', '1997', '078', 'k', '30100', 'help', 'thanks', '4', '12', '1', 'referenced', '1567', 'issues', 'placed', 'meter', '98', '9638', 'note', '089', '6736', 'ends', 'please', 'also', '97', 'deal', '101', '9497', 'jackie', 'need', 'cpr', '5', 'activity', 'ua', 'information'], 'ham'), (['attached', 'hpll', 'nominations', 'xls', '228', '1999', 'see', 'hpl', '28', 'december', 'file'], 'ham'), (['ect', '148', '9', 'farmer', 'want', 'forwarded', 'enron', '11', 'j', 'kcs', 'america', '8', '29', 'cotten', 'adjust', 'corp', 'north', 'meter', 'resources', 'nom', 'revise', '381', '9658', '03', 'revised', '35', '22', 'subject', 'robert', 'cc', 'orig', 'daren', 'pm', '2000', 'bob', '000', 'hou', 'wants', '06'], 'ham'), (['month', 'attached', 'p', 'less', 'mack', 'tracked', '0', 'winn', '100', 'melissa', 'mmbtu', 'counterparty', '14', 'thanks', 'meter', '7', '9847', 'wellhead', 'reinhardt', 'limited', 'new', 'committed', 'volumes', 'donald', '9845', '13', 'entered', '21', 'needed', 'hesco', 'hughes', 'line', 'inc', '6353', 'remaining', '800', 'lone', '07', 'corp', 'ticket', 'samson', '31', 'subject', '600', '3000', 'withers', '35', 'fuel', 'bob', 'reserves', 'im', 'taylor', '25', 'tickets', '3', 'daily', 'enron', 'lisa', 'vols', 'deals', '10', 'use', 'producer', '9835', 'exploration', 'firm', 'co', 'vance', 'term', 'svcs', 'additionally', 'robert', 'daren', 'pm', 'sitara', '2000', 'fyi', 'gathering', 'ect', '96', 'heidi', 'following', 'forwarded', 'based', 'price', 'gas', '6', '24', 'susan', 'smith', 'contract', 'cotten', 'oil', 'portfolio', 'x', 'l', 'hesse', 'star', 'august', 'graves', 'del', '01', 'cc', 'production', 'created', 'deal', 'period', 'trisha', 'hou', '28', 'beginning', 'vlt', 'submitted', 'hillary'], 'ham')]\n"
     ]
    }
   ],
   "source": [
    "# Let's now make this a function so, we can form our data-set:\n",
    "# [(<tokenize-words), <spam/ham>)...]\n",
    "# list of tuples\n",
    "\n",
    "def create_dataset(rawtext, tag):\n",
    "    tmpdocs = []\n",
    "    for text in rawtext:\n",
    "        tokens_all = nltk.word_tokenize(text)\n",
    "        tokens_unique = set(tokens_all)\n",
    "        tokens_filter = list(tokens_unique - set(stopwords))\n",
    "        tmpdocs.append((tokens_filter, tag))\n",
    "    return tmpdocs\n",
    "\n",
    "spam = create_dataset(spamtexts, 'spam')\n",
    "ham = create_dataset(hamtexts, 'ham')\n",
    "\n",
    "# Check a few of them before we combine the lists\n",
    "print(spam[:5])\n",
    "print(ham[:5])\n",
    "\n",
    "spam_n_ham = spam + ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['remote',\n",
       "  'www',\n",
       "  'back',\n",
       "  'sport',\n",
       "  'special',\n",
       "  'avocation',\n",
       "  'channels',\n",
       "  'allow',\n",
       "  'receive',\n",
       "  'control',\n",
       "  'despoil',\n",
       "  '8006',\n",
       "  'events',\n",
       "  'axxxmovies',\n",
       "  'hosting',\n",
       "  'cable',\n",
       "  'emile',\n",
       "  'payperviews',\n",
       "  'order',\n",
       "  'cablefilterz'],\n",
       " 'spam')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the data prior modelling it\n",
    "import random\n",
    "random.shuffle(spam_n_ham)\n",
    "\n",
    "# See the format:\n",
    "#.  list of tuples where the tuple is (<tokenized-words>, category = <spam/ham>)\n",
    "spam_n_ham[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['remote', 'www', 'back', 'sport', 'special'] ['please', '2000', 'subject', 'enron', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "# We'll find the 2000 most common words and use them as an \n",
    "# important feature of the whole corpus\n",
    "def unigram_freq(docs):\n",
    "    all_words = []\n",
    "    # Write a regex to pull only the word portion & leave \n",
    "    # out any punctuation marks etc.\n",
    "    for (word_tokens, category) in docs:\n",
    "        for word in word_tokens:\n",
    "            # Not writing a regex here since we want to know if \n",
    "            # random patterns or words would cause an email to be spam\n",
    "            all_words.append(word)\n",
    "    top_words = nltk.FreqDist(all_words)\n",
    "    most_common_words = top_words.most_common(2000)\n",
    "    word_features = [word for (word,count) in most_common_words]\n",
    "    return all_words, word_features\n",
    "\n",
    "# uni_features now has the top-2000 most common words\n",
    "# across the entire spam and ham data\n",
    "all_words, uni_features = unigram_freq(spam_n_ham)\n",
    "\n",
    "print(all_words[:5], uni_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email classified spam has the following words: ['back', 'www', 'order', 'receive', 'special', 'control', 'allow', 'events']\n"
     ]
    }
   ],
   "source": [
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    # we open a Pytnon dictionary instead of a list\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        #checking if the word from word_features matches a word in the document\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# Essentially, when we call document_features(), we should have a feature-set:\n",
    "# contains(<word>): <True/False>, category = spam/ham\n",
    "# .\n",
    "# .\n",
    "# upto 2k words & this repeats for every tokenized word in spam_n_ham\n",
    "# which is based on uni-gram tokens.\n",
    "\n",
    "uni_featuresets = [(document_features(d, uni_features), c) for (d, c) in spam_n_ham]\n",
    "# print(uni_featuresets[0])\n",
    "\n",
    "# Also, curious about the category and what words were True for\n",
    "# the first featureset\n",
    "words = []\n",
    "for feature, _bool in uni_featuresets[0][0].items():\n",
    "    if _bool == True:\n",
    "        _word = feature.split('(')[1].split(')')[0]\n",
    "        words.append(_word)\n",
    "print('Email classified %s has the following words: %s' %(uni_featuresets[0][1], words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9566889991496836\n"
     ]
    }
   ],
   "source": [
    "# Let's run the Naive Bayes classification algorithm\n",
    "# and measure the accuracy.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def ml_nb(featuresets):\n",
    "    kf = KFold(n_splits = 5)\n",
    "    sum = 0\n",
    "\n",
    "    for train, test in kf.split(featuresets):\n",
    "        train_data = np.array(featuresets)[train]\n",
    "        test_data = np.array(featuresets)[test]\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_data)\n",
    "        sum += nltk.classify.accuracy(classifier, test_data)\n",
    "        \n",
    "        \n",
    "    #storing the score in a variable \n",
    "    acc1 = sum/5\n",
    "    \n",
    "    return  classifier, acc1\n",
    "\n",
    "# Let's call the function ml_nb which splits the data based on\n",
    "# cross-validation and fold/k = 5\n",
    "uni_classifier, uni_accuracy = ml_nb(uni_featuresets)\n",
    "print(uni_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('01', 'cc'), ('xls', '2000'), ('pm', '2000'), ('daren', 'pm'), ('see', 'hpl')]\n",
      "Email classified spam has the following words: [\"contains(('receive', 'control'))\", \"contains(('allow', 'receive'))\"]\n"
     ]
    }
   ],
   "source": [
    "# Let's try the same with bi-grams to see if we get better\n",
    "# accuracy.\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "def bigram_freq(all_words):\n",
    "    #creating bigrams features for the corpus and applying cleaning steps\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(all_words)\n",
    "    scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    \n",
    "    #extracting clean bigrams (no frequency information)\n",
    "    bigram_features = [bigram for (bigram, count) in scored[:2000]]\n",
    "    \n",
    "    return bigram_features\n",
    "\n",
    "\n",
    "bi_features = bigram_freq(all_words)\n",
    "print(bi_features[:5])\n",
    "\n",
    "def bi_document_features(document, bigram_features):\n",
    "    document_words = list(nltk.bigrams(document))\n",
    "    features = {}\n",
    "    for word in bigram_features:\n",
    "        #boolean logic will return 'True' if there is a match, or 'False' if not\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "\n",
    "bi_featuresets = [(bi_document_features(d, bi_features), c) for (d, c) in spam_n_ham]\n",
    "\n",
    "words = []\n",
    "for feature, _bool in bi_featuresets[0][0].items():\n",
    "    if _bool == True:\n",
    "        words.append(feature)\n",
    "\n",
    "print('Email classified %s has the following words: %s' %(bi_featuresets[0][1], words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8383556190956746\n"
     ]
    }
   ],
   "source": [
    "# Let's call the function ml_nb which splits the data based on\n",
    "# cross-validation and fold/k = 5\n",
    "bi_classifier, bi_accuracy = ml_nb(bi_featuresets)\n",
    "print(bi_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham'] ['spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham']\n"
     ]
    }
   ],
   "source": [
    "# Given, the accuracy is really good especially for uni-gram classifier,\n",
    "# let us also fetch the f-measure \n",
    "\n",
    "# Also, we will use the classifier we obtained from the model for both \n",
    "# the uni-gram & bi-gram classifiers. Further, since we don't have separate \n",
    "# test-data, we will sample 20% of the bottom end of the data as test.\n",
    "\n",
    "test_len = int(0.2 * len(spam_n_ham))\n",
    "test_data = spam_n_ham[:test_len]\n",
    "\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for words, cat in test_data:\n",
    "    predict = uni_classifier.classify(document_features(words, uni_features))\n",
    "    predicted.append(predict)\n",
    "    actual.append(cat)\n",
    "\n",
    "print(actual[:10], predicted[:10])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrecision\tRecall\t\tF1\n",
      "spam \t      0.993      0.882      0.935\n",
      "ham \t      0.945      0.997      0.971\n"
     ]
    }
   ],
   "source": [
    "# Utilizing a function from labs let's now obtain the eval_measures\n",
    "def eval_measures(gold, predicted):\n",
    "    # get a list of labels\n",
    "    labels = list(set(gold))\n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)\n",
    "        precision = TP / (TP + FN)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "\n",
    "# Eval measures for uni-gram classifier\n",
    "eval_measures(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham'] ['spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'ham']\n"
     ]
    }
   ],
   "source": [
    "# And repeat the same for bi-gram classifier too\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for words, cat in test_data:\n",
    "    predict = bi_classifier.classify(bi_document_features(words, bi_features))\n",
    "    predicted.append(predict)\n",
    "    actual.append(cat)\n",
    "\n",
    "print(actual[:10], predicted[:10])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrecision\tRecall\t\tF1\n",
      "spam \t      0.993      0.682      0.809\n",
      "ham \t      0.809      0.997      0.893\n"
     ]
    }
   ],
   "source": [
    "# Eval measures for bi-gram classifier\n",
    "eval_measures(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
