{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aysnc Labs\n",
    "# Week-1 Lab\n",
    "import nltk\n",
    "\n",
    "# Optional\n",
    "# nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "191785"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('gutenberg')\n",
    "\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "# Get the first book/author\n",
    "file0 = nltk.corpus.gutenberg.fileids( ) [0]\n",
    "print(file0)\n",
    "\n",
    "# Get the all the words/text\n",
    "emmatext = nltk.corpus.gutenberg.raw(file0)\n",
    "\n",
    "# Tokenize\n",
    "emmatokens = nltk.word_tokenize(emmatext)\n",
    "len(emmatokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191785"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emmawords = [w.lower( ) for w in emmatokens]\n",
    "len(emmawords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7944"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emmavocab = sorted(set(emmawords))\n",
    "len(emmavocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'emma',\n",
       " 'by',\n",
       " 'jane',\n",
       " 'austen',\n",
       " '1816',\n",
       " ']',\n",
       " 'volume',\n",
       " 'i',\n",
       " 'chapter',\n",
       " 'woodhouse',\n",
       " ',',\n",
       " 'handsome',\n",
       " 'clever',\n",
       " 'and',\n",
       " 'rich',\n",
       " 'with',\n",
       " 'a',\n",
       " 'comfortable',\n",
       " 'home',\n",
       " 'happy',\n",
       " 'disposition',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'unite',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'best',\n",
       " 'blessings',\n",
       " 'existence',\n",
       " ';',\n",
       " 'had',\n",
       " 'lived',\n",
       " 'nearly',\n",
       " 'twenty-one',\n",
       " 'years',\n",
       " 'in',\n",
       " 'world',\n",
       " 'very',\n",
       " 'little',\n",
       " 'distress',\n",
       " 'or',\n",
       " 'vex',\n",
       " 'her',\n",
       " '.',\n",
       " 'she',\n",
       " 'was',\n",
       " 'youngest',\n",
       " 'two']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word frequencies or frequency distribution\n",
    "from nltk import FreqDist\n",
    "\n",
    "fdist = FreqDist(emmawords)\n",
    "fdistkeys = list(fdist.keys())\n",
    "fdistkeys[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Has a dict of word:count\n",
    "fdist['emma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(',', 12016)\n",
      "('.', 6355)\n",
      "('the', 5201)\n",
      "('to', 5181)\n",
      "('and', 4877)\n",
      "('of', 4284)\n",
      "('i', 3177)\n",
      "('a', 3124)\n",
      "('--', 3100)\n",
      "('it', 2503)\n",
      "(\"''\", 2452)\n",
      "('her', 2448)\n",
      "('was', 2396)\n",
      "(';', 2353)\n",
      "('she', 2336)\n",
      "('not', 2281)\n",
      "('in', 2173)\n",
      "('be', 1970)\n",
      "('you', 1967)\n",
      "('he', 1806)\n",
      "('that', 1805)\n",
      "('``', 1735)\n",
      "('had', 1623)\n",
      "('but', 1441)\n",
      "('as', 1436)\n",
      "('for', 1346)\n",
      "('have', 1320)\n",
      "('is', 1241)\n",
      "('with', 1215)\n",
      "('very', 1202)\n"
     ]
    }
   ],
   "source": [
    "# Use the most_common func sorted in ascending order\n",
    "topkeys = fdist.most_common(30)\n",
    "for pair in topkeys:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(',', 0.06265349219177725)\n",
      "('.', 0.03313606382146675)\n",
      "('the', 0.027118909195192532)\n",
      "('to', 0.0270146257527961)\n",
      "('and', 0.02542951742837031)\n",
      "('of', 0.022337513361316057)\n",
      "('i', 0.016565424824673464)\n",
      "('a', 0.016289073702322913)\n",
      "('--', 0.016163933571447194)\n",
      "('it', 0.013051072815913653)\n",
      "(\"''\", 0.012785150037802747)\n",
      "('her', 0.012764293349323462)\n",
      "('was', 0.012493156399092735)\n",
      "(';', 0.012268946997940402)\n",
      "('she', 0.012180306071903433)\n",
      "('not', 0.011893526605313242)\n",
      "('in', 0.0113303960163725)\n",
      "('be', 0.010271919076048701)\n",
      "('you', 0.010256276559689236)\n",
      "('he', 0.009416794848397945)\n",
      "('that', 0.009411580676278125)\n",
      "('``', 0.009046588627890607)\n",
      "('had', 0.00846260135047058)\n",
      "('but', 0.007513622024663034)\n",
      "('as', 0.007487551164063926)\n",
      "('for', 0.007018275673279975)\n",
      "('have', 0.0068827071981646115)\n",
      "('is', 0.006470787600698699)\n",
      "('with', 0.006335219125583336)\n",
      "('very', 0.0062674348880256536)\n",
      "('his', 0.005949370388716532)\n",
      "('mr.', 0.005688661782725448)\n",
      "('!', 0.005542664963370441)\n",
      "('at', 0.0053705972834163255)\n",
      "('so', 0.005047318611987382)\n",
      "(\"'s\", 0.004515473055765571)\n",
      "('emma', 0.004458117162447532)\n",
      "('all', 0.004385118752770029)\n",
      "('could', 0.004359047892170921)\n",
      "('would', 0.004265192794014131)\n",
      "('been', 0.0039366999504653645)\n",
      "('him', 0.0039054149177464347)\n",
      "('no', 0.0038637015407878613)\n",
      "('my', 0.0037959173032301795)\n",
      "('on', 0.0035925645905571344)\n",
      "('mrs.', 0.003483066976040879)\n",
      "('any', 0.0034100685663633755)\n",
      "('do', 0.0033996402221237324)\n",
      "('?', 0.0032380008864092602)\n",
      "('miss', 0.0031232890997731837)\n"
     ]
    }
   ],
   "source": [
    "numwords = len(emmawords)\n",
    "topkeysnormalized = [(word, freq / numwords) for (word, freq) in topkeys]\n",
    "for pair in topkeysnormalized:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Week-2 Labs\n",
    "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "print(len(nltkstopwords))\n",
    "print(nltkstopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched non-alphabetical\n",
      "['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', 'indulgent', 'father', 'and', 'had', 'in', 'consequence', 'of', 'her', 'sister', \"'s\", 'marriage', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', 'her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses']\n",
      "161456\n"
     ]
    }
   ],
   "source": [
    "##  Regular Expression to match non-alphabetic characters\n",
    "import re\n",
    "\n",
    "# this regular expression pattern matches any word that contains all non-alphabetical\n",
    "#   lower-case characters [^a-z]+\n",
    "# the beginning ^ and ending $ require the match to begin and end on a word boundary \n",
    "pattern = re.compile('^[^a-z]+$')\n",
    "\n",
    "nonAlphaMatch = pattern.match('**')\n",
    "#  if it matched, print a message\n",
    "if nonAlphaMatch: print ('matched non-alphabetical')\n",
    "\n",
    "# function that takes a word and returns true if it consists only\n",
    "#   of non-alphabetic characters  (assumes import re)\n",
    "def alpha_filter(w):\n",
    "  # pattern to match word of non-alphabetical characters\n",
    "  pattern = re.compile('^[^a-z]+$')\n",
    "  if (pattern.match(w)):\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "# apply the function to emmawords\n",
    "alphaemmawords = [w for w in emmawords if not alpha_filter(w)]\n",
    "print(alphaemmawords[:100])\n",
    "print(len(alphaemmawords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'s\", 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.', 'her', 'mother', 'had', 'died']\n",
      "['likenesses', '?', 'you', 'know', 'nothing', 'of', 'drawing', '.', 'do', \"n't\"]\n",
      "195\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'could', 'would', 'might', 'must', 'need', 'sha', 'wo', 'y', \"'s\", \"'d\", \"'ll\", \"'t\", \"'m\", \"'re\", \"'ve\", \"n't\"]\n",
      "70556\n"
     ]
    }
   ],
   "source": [
    "# get a list of stopwords from nltk\n",
    "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "print(len(nltkstopwords))\n",
    "print(nltkstopwords)\n",
    "\n",
    "# check tokenization in emmawords\n",
    "print(emmawords[:100])\n",
    "print(emmawords[15300:15310])\n",
    "\n",
    "morestopwords = ['could','would','might','must','need','sha','wo','y',\"'s\",\"'d\",\"'ll\",\"'t\",\"'m\",\"'re\",\"'ve\", \"n't\"]\n",
    "\n",
    "stopwords = nltkstopwords + morestopwords\n",
    "print(len(stopwords))\n",
    "print(stopwords)\n",
    "\n",
    "stoppedemmawords = [w for w in alphaemmawords if not w in stopwords]\n",
    "print(len(stoppedemmawords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mr.', 1091)\n",
      "('emma', 855)\n",
      "('mrs.', 668)\n",
      "('miss', 599)\n",
      "('harriet', 496)\n",
      "('much', 484)\n",
      "('said', 483)\n",
      "('one', 447)\n",
      "('every', 435)\n",
      "('weston', 430)\n",
      "('thing', 394)\n",
      "('think', 383)\n",
      "('well', 378)\n",
      "('elton', 378)\n",
      "('knightley', 373)\n",
      "('little', 359)\n",
      "('never', 358)\n",
      "('know', 335)\n",
      "('good', 313)\n",
      "('say', 310)\n",
      "('woodhouse', 308)\n",
      "('jane', 301)\n",
      "('quite', 282)\n",
      "('time', 275)\n",
      "('great', 263)\n",
      "('nothing', 252)\n",
      "('dear', 241)\n",
      "('always', 238)\n",
      "('man', 232)\n",
      "('fairfax', 232)\n"
     ]
    }
   ],
   "source": [
    "# use this list for a better frequency distribution\n",
    "emmadist = FreqDist(stoppedemmawords)\n",
    "emmaitems = emmadist.most_common(30)\n",
    "for item in emmaitems:\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',']\n",
      "[('[', 'emma'), ('emma', 'by'), ('by', 'jane'), ('jane', 'austen'), ('austen', '1816'), ('1816', ']'), (']', 'volume'), ('volume', 'i'), ('i', 'chapter'), ('chapter', 'i'), ('i', 'emma'), ('emma', 'woodhouse'), ('woodhouse', ','), (',', 'handsome'), ('handsome', ','), (',', 'clever'), ('clever', ','), (',', 'and'), ('and', 'rich'), ('rich', ',')]\n"
     ]
    }
   ],
   "source": [
    "# Bigrams and Bigram frequency distribution\n",
    "emmabigrams = list(nltk.bigrams(emmawords))\n",
    "print(emmawords[:21])\n",
    "print(emmabigrams[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'tuple'>\n",
      "((',', 'and'), 0.009813071929504393)\n"
     ]
    }
   ],
   "source": [
    "# setup for bigrams and bigram measures\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder = BigramCollocationFinder.from_words(emmawords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "# scored is a list of bigram pairs with their score\n",
    "print(type(scored))\n",
    "first = scored[0]\n",
    "print(type(first))\n",
    "print(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((',', 'and'), 0.009813071929504393)\n",
      "(('.', \"''\"), 0.006032797142633679)\n",
      "((\"''\", '``'), 0.005000391062908987)\n",
      "((';', 'and'), 0.004520687227885393)\n",
      "(('to', 'be'), 0.0031545741324921135)\n",
      "((',', \"''\"), 0.0030450765179758582)\n",
      "(('.', 'i'), 0.002972078108298355)\n",
      "((',', 'i'), 0.0029668639361785333)\n",
      "(('of', 'the'), 0.0029147222149803163)\n",
      "(('in', 'the'), 0.0023203065933206455)\n",
      "(('it', 'was'), 0.0023046640769611806)\n",
      "((';', 'but'), 0.002226451495163855)\n",
      "(('.', '``'), 0.002169095601845817)\n",
      "(('.', 'she'), 0.002153453085486352)\n",
      "(('i', 'am'), 0.00205438381520974)\n",
      "((',', 'that'), 0.001877101963135803)\n",
      "(('!', '--'), 0.0017936752092186563)\n",
      "(('--', 'and'), 0.0017415334880204396)\n",
      "(('she', 'had'), 0.0017311051437807962)\n",
      "(('she', 'was'), 0.0017102484553015095)\n",
      "(('had', 'been'), 0.0016007508407852543)\n",
      "((',', 'she'), 0.0015851083244257892)\n",
      "((',', 'but'), 0.0015798941523059676)\n",
      "(('.', 'he'), 0.0015798941523059676)\n",
      "(('it', 'is'), 0.0015538232917068592)\n",
      "((',', 'as'), 0.0015225382589879291)\n",
      "(('i', 'have'), 0.0014651823656698908)\n",
      "(('could', 'not'), 0.0014495398493104257)\n",
      "(('mr.', 'knightley'), 0.0014234689887113175)\n",
      "(('.', 'it'), 0.0013869697838725656)\n"
     ]
    }
   ],
   "source": [
    "# scores are sorted in decreasing frequency\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('to', 'be'), 0.0031545741324921135)\n",
      "(('of', 'the'), 0.0029147222149803163)\n",
      "(('in', 'the'), 0.0023203065933206455)\n",
      "(('it', 'was'), 0.0023046640769611806)\n",
      "(('i', 'am'), 0.00205438381520974)\n",
      "(('she', 'had'), 0.0017311051437807962)\n",
      "(('she', 'was'), 0.0017102484553015095)\n",
      "(('had', 'been'), 0.0016007508407852543)\n",
      "(('it', 'is'), 0.0015538232917068592)\n",
      "(('i', 'have'), 0.0014651823656698908)\n",
      "(('could', 'not'), 0.0014495398493104257)\n",
      "(('mr.', 'knightley'), 0.0014234689887113175)\n",
      "(('of', 'her'), 0.0013556847511536356)\n",
      "(('mrs.', 'weston'), 0.0012826863414761322)\n",
      "(('have', 'been'), 0.0012566154808770237)\n",
      "(('he', 'had'), 0.0012514013087572022)\n",
      "(('to', 'the'), 0.001235758792397737)\n",
      "(('do', 'not'), 0.0012253304481580937)\n",
      "(('and', 'the'), 0.0011679745548400552)\n",
      "(('he', 'was'), 0.0011575462106004119)\n",
      "(('would', 'be'), 0.0011210470057616603)\n",
      "(('mr.', 'elton'), 0.0011001903172823736)\n",
      "(('such', 'a'), 0.001042834423964335)\n",
      "(('a', 'very'), 0.0010324060797246917)\n",
      "(('of', 'his'), 0.0009906927027661184)\n",
      "(('that', 'she'), 0.0009594076700471882)\n",
      "(('to', 'have'), 0.0009594076700471882)\n",
      "(('to', 'her'), 0.0009594076700471882)\n",
      "(('did', 'not'), 0.0009541934979273665)\n",
      "(('and', 'i'), 0.0009489793258075449)\n"
     ]
    }
   ],
   "source": [
    "# apply a filter to remove non-alphabetical tokens from the emma bigram finder\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('mr.', 'knightley'), 0.0014234689887113175)\n",
      "(('mrs.', 'weston'), 0.0012826863414761322)\n",
      "(('mr.', 'elton'), 0.0011001903172823736)\n",
      "(('miss', 'woodhouse'), 0.0008864092603696848)\n",
      "(('mr.', 'weston'), 0.0008238391949318247)\n",
      "(('frank', 'churchill'), 0.0007508407852543212)\n",
      "(('mrs.', 'elton'), 0.0007299840967750345)\n",
      "(('mr.', 'woodhouse'), 0.0006830565476966395)\n",
      "(('every', 'thing'), 0.0006465573428578878)\n",
      "(('miss', 'fairfax'), 0.0006361289986182444)\n",
      "(('miss', 'bates'), 0.0005787731053002059)\n",
      "(('every', 'body'), 0.0005683447610605626)\n",
      "(('jane', 'fairfax'), 0.0005474880725812759)\n",
      "(('young', 'man'), 0.00043277628594519906)\n",
      "(('great', 'deal'), 0.00033370701566858723)\n",
      "(('said', 'emma'), 0.0003076361550694788)\n",
      "(('miss', 'smith'), 0.00029720781082983547)\n",
      "(('john', 'knightley'), 0.00028677946659019213)\n",
      "(('mrs.', 'goddard'), 0.0002711369502307271)\n",
      "(('dare', 'say'), 0.00026592277811090544)\n"
     ]
    }
   ],
   "source": [
    "# apply a filter to remove stop words\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:20]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((',', 'and'), 0.009813071929504393)\n",
      "(('.', \"''\"), 0.006032797142633679)\n",
      "((\"''\", '``'), 0.005000391062908987)\n",
      "((';', 'and'), 0.004520687227885393)\n",
      "(('to', 'be'), 0.0031545741324921135)\n",
      "((',', \"''\"), 0.0030450765179758582)\n",
      "(('.', 'i'), 0.002972078108298355)\n",
      "((',', 'i'), 0.0029668639361785333)\n",
      "(('of', 'the'), 0.0029147222149803163)\n",
      "(('in', 'the'), 0.0023203065933206455)\n",
      "(('it', 'was'), 0.0023046640769611806)\n",
      "((';', 'but'), 0.002226451495163855)\n",
      "(('.', '``'), 0.002169095601845817)\n",
      "(('.', 'she'), 0.002153453085486352)\n",
      "(('i', 'am'), 0.00205438381520974)\n",
      "((',', 'that'), 0.001877101963135803)\n",
      "(('!', '--'), 0.0017936752092186563)\n",
      "(('--', 'and'), 0.0017415334880204396)\n",
      "(('she', 'had'), 0.0017311051437807962)\n",
      "(('she', 'was'), 0.0017102484553015095)\n"
     ]
    }
   ],
   "source": [
    "# apply a filter (on a new finder) to remove low frequency words\n",
    "finder2 = BigramCollocationFinder.from_words(emmawords)\n",
    "finder2.apply_freq_filter(2)\n",
    "scored = finder2.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:20]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((\"''\", '``'), 0.005000391062908987)\n",
      "(('to', 'be'), 0.0031545741324921135)\n",
      "(('of', 'the'), 0.0029147222149803163)\n",
      "(('in', 'the'), 0.0023203065933206455)\n",
      "(('it', 'was'), 0.0023046640769611806)\n",
      "(('--', 'and'), 0.0017415334880204396)\n",
      "(('she', 'had'), 0.0017311051437807962)\n",
      "(('she', 'was'), 0.0017102484553015095)\n",
      "(('had', 'been'), 0.0016007508407852543)\n",
      "(('it', 'is'), 0.0015538232917068592)\n",
      "(('could', 'not'), 0.0014495398493104257)\n",
      "(('mr.', 'knightley'), 0.0014234689887113175)\n",
      "((\"''\", 'said'), 0.0013817556117527439)\n",
      "(('``', 'i'), 0.0013608989232734572)\n",
      "(('of', 'her'), 0.0013556847511536356)\n",
      "(('--', 'i'), 0.0013400422347941705)\n",
      "(('mrs.', 'weston'), 0.0012826863414761322)\n",
      "(('have', 'been'), 0.0012566154808770237)\n",
      "(('he', 'had'), 0.0012514013087572022)\n",
      "(('to', 'the'), 0.001235758792397737)\n"
     ]
    }
   ],
   "source": [
    "# apply a filter on both words of the ngram\n",
    "finder2.apply_ngram_filter(lambda w1, w2: len(w1) < 2)\n",
    "scored = finder2.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:20]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('sore', 'throat'), 14.089698743664066)\n",
      "(('brunswick', 'square'), 13.952195219914131)\n",
      "(('william', 'larkins'), 13.089698743664067)\n",
      "(('baked', 'apples'), 12.964167861580208)\n",
      "(('box', 'hill'), 12.736061789049367)\n",
      "(('sixteen', 'miles'), 12.613670614496076)\n",
      "(('maple', 'grove'), 12.594934051914487)\n",
      "(('hair', 'cut'), 12.063703535131124)\n",
      "(('south', 'end'), 11.96416786158021)\n",
      "(('colonel', 'campbell'), 11.412234161246522)\n",
      "(('robert', 'martin'), 11.093935736550536)\n",
      "(('five', 'couple'), 10.841771230220482)\n",
      "(('vast', 'deal'), 10.76253400041056)\n",
      "(('ready', 'wit'), 10.652293431356767)\n",
      "(('donwell', 'abbey'), 10.519383018907313)\n",
      "(('musical', 'society'), 10.509114683453486)\n",
      "(('infinitely', 'superior'), 10.23081352096638)\n",
      "(('married', 'women'), 10.05727726597169)\n",
      "(('five', 'minutes'), 10.032714012931878)\n",
      "(('years', 'ago'), 9.957504131299197)\n",
      "(('three', 'months'), 9.941800048551753)\n",
      "(('depend', 'upon'), 9.928125111654678)\n",
      "(('ten', 'minutes'), 9.867013597351292)\n",
      "(('hurrying', 'away'), 9.603101372785888)\n",
      "(('lovely', 'woman'), 9.400399583128175)\n",
      "(('ten', 'years'), 9.37254163057804)\n",
      "(('last', 'night'), 9.368910380131172)\n",
      "(('frank', 'churchill'), 9.284101419843202)\n",
      "(('take', 'care'), 9.18038141066101)\n",
      "(('worthy', 'people'), 9.146544604068778)\n"
     ]
    }
   ],
   "source": [
    "# to get good results, must first apply frequency filter\n",
    "finder.apply_freq_filter(5)\n",
    "scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-persuasion.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97920"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 2.5.6\n",
    "\n",
    "# Get the first book/author\n",
    "file1 = nltk.corpus.gutenberg.fileids( ) [1]\n",
    "print(file1)\n",
    "\n",
    "# Get the all the words/text\n",
    "persuasiontext = nltk.corpus.gutenberg.raw(file1)\n",
    "\n",
    "# Tokenize\n",
    "persuasiontokens = nltk.word_tokenize(persuasiontext)\n",
    "len(persuasiontokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97920"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persuasionwords = [w.lower( ) for w in persuasiontokens]\n",
    "len(persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((',', 'and'), 0.012939133986928104)\n",
      "((';', 'and'), 0.0048304738562091505)\n",
      "(('of', 'the'), 0.004340277777777778)\n",
      "(('to', 'be'), 0.003860294117647059)\n",
      "(('.', \"''\"), 0.0037683823529411765)\n",
      "(('in', 'the'), 0.003278186274509804)\n",
      "(('.', 'she'), 0.0028901143790849675)\n",
      "((',', \"''\"), 0.00261437908496732)\n",
      "(('had', 'been'), 0.002593954248366013)\n",
      "((\"''\", '``'), 0.002573529411764706)\n",
      "(('.', 'i'), 0.0024509803921568627)\n",
      "((';', 'but'), 0.0023794934640522874)\n",
      "(('she', 'had'), 0.002318218954248366)\n",
      "(('it', 'was'), 0.00221609477124183)\n",
      "((',', 'i'), 0.002165032679738562)\n",
      "((',', 'but'), 0.002042483660130719)\n",
      "(('.', '``'), 0.002022058823529412)\n",
      "(('.', 'he'), 0.002022058823529412)\n",
      "((',', 'that'), 0.0020016339869281044)\n",
      "(('captain', 'wentworth'), 0.001991421568627451)\n",
      "(('he', 'had'), 0.0019505718954248366)\n",
      "((',', 'she'), 0.001889297385620915)\n",
      "((',', 'as'), 0.0018075980392156864)\n",
      "((',', 'in'), 0.001787173202614379)\n",
      "(('to', 'the'), 0.001787173202614379)\n",
      "(('mr', 'elliot'), 0.0017565359477124183)\n",
      "(('.', 'the'), 0.0016952614379084968)\n",
      "(('she', 'was'), 0.0016748366013071894)\n",
      "((',', 'to'), 0.0016544117647058823)\n",
      "(('could', 'not'), 0.001511437908496732)\n"
     ]
    }
   ],
   "source": [
    "# Bi-gram - Raw frequency with no filters\n",
    "finder = BigramCollocationFinder.from_words(persuasionwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('captain', 'wentworth'), 0.001991421568627451)\n",
      "(('mr', 'elliot'), 0.0017565359477124183)\n",
      "(('lady', 'russell'), 0.0015012254901960785)\n",
      "(('sir', 'walter'), 0.0013174019607843138)\n",
      "(('mrs', 'clay'), 0.0006638071895424837)\n",
      "(('mrs', 'musgrove'), 0.0006638071895424837)\n",
      "(('mrs', 'smith'), 0.00065359477124183)\n",
      "(('captain', 'benwick'), 0.0005616830065359477)\n",
      "(('miss', 'elliot'), 0.0004901960784313725)\n",
      "(('mrs', 'croft'), 0.00041870915032679736)\n",
      "(('captain', 'harville'), 0.000377859477124183)\n",
      "(('great', 'deal'), 0.00034722222222222224)\n",
      "(('charles', 'hayter'), 0.0003370098039215686)\n",
      "(('camden', 'place'), 0.0002961601307189542)\n",
      "(('mr', 'shepherd'), 0.00026552287581699344)\n",
      "(('kellynch', 'hall'), 0.0002553104575163399)\n",
      "(('lady', 'dalrymple'), 0.0002553104575163399)\n",
      "(('mrs', 'harville'), 0.00024509803921568627)\n",
      "(('anne', 'elliot'), 0.00023488562091503269)\n",
      "(('colonel', 'wallis'), 0.00023488562091503269)\n",
      "(('miss', 'musgroves'), 0.00022467320261437907)\n",
      "(('young', 'man'), 0.00022467320261437907)\n",
      "(('mr', 'musgrove'), 0.0002144607843137255)\n",
      "(('miss', 'anne'), 0.0001940359477124183)\n",
      "(('said', 'anne'), 0.0001838235294117647)\n",
      "(('walter', 'elliot'), 0.0001633986928104575)\n",
      "(('louisa', 'musgrove'), 0.00015318627450980392)\n",
      "(('charles', 'musgrove'), 0.00014297385620915033)\n",
      "(('admiral', 'croft'), 0.00013276143790849672)\n",
      "(('great', 'house'), 0.00013276143790849672)\n"
     ]
    }
   ],
   "source": [
    "# Apply a filter to remove stop-words/apply regex function to remove punctuation marks etc. \n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    \n",
    "finder.apply_word_filter(alpha_filter)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('1818', ']'), 16.579315937580013)\n",
      "(('a.', 'e.'), 16.579315937580013)\n",
      "(('accustomary', 'intervention'), 16.579315937580013)\n",
      "(('anyone', 'intending'), 16.579315937580013)\n",
      "(('apples', 'stolen'), 16.579315937580013)\n",
      "(('attribute', 'guile'), 16.579315937580013)\n",
      "(('au', 'fait'), 16.579315937580013)\n",
      "(('austen', '1818'), 16.579315937580013)\n",
      "(('authentic', 'oral'), 16.579315937580013)\n",
      "(('bathing', 'machines'), 16.579315937580013)\n",
      "(('breakfast-room', 'chimney'), 16.579315937580013)\n",
      "(('brown', 'velvet'), 16.579315937580013)\n",
      "(('bustling', 'housekeepers'), 16.579315937580013)\n",
      "(('captiously', 'irritable'), 16.579315937580013)\n",
      "(('chimney', 'smokes'), 16.579315937580013)\n",
      "(('choicest', 'gift'), 16.579315937580013)\n",
      "(('clumsy', 'wrist'), 16.579315937580013)\n",
      "(('coat', 'pockets'), 16.579315937580013)\n",
      "(('constantly', 'exceeding'), 16.579315937580013)\n",
      "(('contracted', 'debts'), 16.579315937580013)\n",
      "(('counted', 'eighty-seven'), 16.579315937580013)\n",
      "(('dashing', 'felow'), 16.579315937580013)\n",
      "(('deplorable-looking', 'personage'), 16.579315937580013)\n",
      "(('dry', 'sunny'), 16.579315937580013)\n",
      "(('duodecimo', 'pages'), 16.579315937580013)\n",
      "(('fainter', 'self-threatenings'), 16.579315937580013)\n",
      "(('famous', 'set-to'), 16.579315937580013)\n",
      "(('far-famed', 'isle'), 16.579315937580013)\n",
      "(('five-and-thirty', 'frights'), 16.579315937580013)\n",
      "(('german', 'artist'), 16.579315937580013)\n"
     ]
    }
   ],
   "source": [
    "# Bi-gram - Mutual information with no filters\n",
    "finder2 = BigramCollocationFinder.from_words(persuasionwords)\n",
    "scored = finder2.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('west', 'indies'), 13.508926609688618)\n",
      "(('dr', 'shirley'), 12.935459747805288)\n",
      "(('marlborough', 'buildings'), 12.672425341971497)\n",
      "(('westgate', 'buildings'), 12.672425341971497)\n",
      "(('milsom', 'street'), 11.878876219438924)\n",
      "(('colonel', 'wallis'), 11.491853096329676)\n",
      "(('eldest', 'son'), 11.316281531746222)\n",
      "(('five', 'minutes'), 11.293913718717766)\n",
      "(('poor', 'richard'), 10.797956224055355)\n",
      "(('ten', 'minutes'), 10.61584181360513)\n",
      "(('eight', 'years'), 10.399406847565078)\n",
      "(('kellynch', 'hall'), 10.225992646417119)\n",
      "(('camden', 'place'), 10.169925001442312)\n",
      "(('laura', 'place'), 10.169925001442312)\n",
      "(('setting', 'off'), 10.055753981523003)\n",
      "(('depend', 'upon'), 9.949959317500403)\n",
      "(('dare', 'say'), 9.79914902696819)\n",
      "(('sitting', 'down'), 9.774109482100986)\n",
      "(('anybody', 'else'), 9.75403910752515)\n",
      "(('miss', 'carteret'), 9.613531652917928)\n",
      "(('few', 'minutes'), 9.509642409773203)\n",
      "(('sat', 'down'), 9.42618617868068)\n",
      "(('years', 'ago'), 9.306297443173598)\n",
      "(('sir', 'walter'), 9.2524335998176)\n",
      "(('good', 'humour'), 9.169925001442314)\n",
      "(('great', 'deal'), 9.163698427077426)\n",
      "(('charles', 'hayter'), 9.009460329249064)\n",
      "(('beg', 'your'), 8.947047722080503)\n",
      "(('your', 'pardon'), 8.947047722080503)\n",
      "(('too', 'late'), 8.912559345695211)\n"
     ]
    }
   ],
   "source": [
    "# Since most of the scores are the same, it is recommended to apply a filter\n",
    "finder2.apply_freq_filter(5)\n",
    "scored = finder2.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/venkatasharatsripada/Documents/Masters@Syracuse/Course-Related(Study)/IST-664'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Week-3 Async Labs\n",
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: Produced by John Bickers ; and Dagny CRIME...>\n",
      "Displaying 25 of 42 matches:\n",
      "y time he went out he was obliged to pass her kitchen , the door of which inva\n",
      "at would it be if it somehow came to pass that I were really going to do it ? \n",
      "oom , she said , letting her visitor pass in front of her : '' Step in , my go\n",
      "d Katerina Ivanovna would not let it pass , she stood up for her ... and so th\n",
      "ng into the next room , as he had to pass through hers to get there . Taking n\n",
      "this scandal , and it came to such a pass that Dounia and I dared not even go \n",
      "for you . Oh , if only this comes to pass ! This would be such a benefit that \n",
      "irst place , because it will come to pass of itself , later on , and he will n\n",
      "hen the hour struck , it all came to pass quite differently , as it were accid\n",
      "g in the doorway not allowing him to pass , he advanced straight upon her . Sh\n",
      "im -- all was lost ; if they let him pass -- all was lost too ; they would rem\n",
      "t once that it would be loathsome to pass that seat on which after the girl wa\n",
      "h , but it 's nothing much , it will pass and you will be all right . Zossimov\n",
      " could you let things come to such a pass that she gave up sending you your di\n",
      "Yes , on my word ! Well , now let us pass to the United States of America , as\n",
      " , for all , and helping to bring to pass my neighbour 's getting a little mor\n",
      "azumihin got up this time to let him pass . Without glancing at anyone , and n\n",
      "o ! '' said Raskolnikov and tried to pass him . This was too much for Razumihi\n",
      " . For the family had come to such a pass that they were practically without c\n",
      "n his way home ; Raskolnikov let him pass , exchanging a silent greeting with \n",
      "y ashamed of it , and he hastened to pass to the other more practical cares an\n",
      " I ran away to take a flat he let it pass .... I put that in cleverly about a \n",
      "not live at all . I simply could n't pass by my mother starving , keeping my r\n",
      " in your eyes ... I can not let this pass considering the relationship and ...\n",
      "her almost to frenzy , and she would pass in an instant from the brightest hop\n"
     ]
    }
   ],
   "source": [
    "f = open('CrimeAndPunishment.txt')\n",
    "rawtext = f.read()\n",
    "\n",
    "# Tokenize\n",
    "crimetokens = nltk.word_tokenize(rawtext)\n",
    "text = nltk.Text(crimetokens)\n",
    "\n",
    "# Make a quick index - given a word, it gives you a few words/sentence structure around that word\n",
    "text.concordance('pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['produc', 'by', 'john', 'bicker', ';', 'and', 'dagni', 'crime', 'and', 'punish', 'By', 'fyodor', 'dostoevski', 'translat', 'By', 'constanc', 'garnett', 'translat', \"'S\", 'prefac', 'A', 'few', 'word', 'about', 'dostoevski', 'himself', 'may', 'help', 'the', 'english', 'reader', 'to', 'understand', 'hi', 'work', '.', 'dostoevski', 'wa', 'the', 'son', 'of', 'a', 'doctor', '.', 'hi', 'parent', 'were', 'veri', 'hard-work', 'and', 'deepli', 'religi', 'peopl', ',', 'but', 'so', 'poor', 'that', 'they', 'live', 'with', 'their', 'five', 'children', 'in', 'onli', 'two', 'room', '.', 'the', 'father', 'and', 'mother', 'spent', 'their', 'even', 'in', 'read', 'aloud', 'to', 'their', 'children', ',', 'gener', 'from', 'book', 'of', 'a', 'seriou', 'charact', '.', 'though', 'alway', 'sickli', 'and', 'delic', 'dostoevski', 'came', 'out', 'third', 'in', 'the', 'final', 'examin', 'of', 'the', 'petersburg', 'school', 'of', 'engin', '.', 'there', 'he', 'had', 'alreadi', 'begun', 'hi', 'first', 'work', ',', '``', 'poor', 'folk', '.', \"''\", 'thi', 'stori', 'wa', 'publish', 'by', 'the', 'poet', 'nekrassov', 'in', 'hi', 'review', 'and', 'wa', 'receiv', 'with', 'acclam', '.', 'the', 'shi', ',', 'unknown', 'youth', 'found', 'himself', 'instantli', 'someth', 'of', 'a', 'celebr', '.', 'A', 'brilliant', 'and', 'success', 'career', 'seem', 'to', 'open', 'befor', 'him', ',', 'but', 'those', 'hope', 'were', 'soon', 'dash', '.', 'In', '1849', 'he', 'wa', 'arrest', '.', 'though', 'neither', 'by', 'tempera', 'nor', 'convict', 'a', 'revolutionist', ',', 'dostoevski', 'wa', 'one', 'of', 'a', 'littl', 'group', 'of', 'young', 'men', 'who', 'met']\n",
      "['produc', 'by', 'john', 'bick', ';', 'and', 'dagny', 'crim', 'and', 'pun', 'by', 'fyod', 'dostoevsky', 'transl', 'by', 'const', 'garnet', 'transl', \"'s\", 'prefac', 'a', 'few', 'word', 'about', 'dostoevsky', 'himself', 'may', 'help', 'the', 'engl', 'read', 'to', 'understand', 'his', 'work', '.', 'dostoevsky', 'was', 'the', 'son', 'of', 'a', 'doct', '.', 'his', 'par', 'wer', 'very', 'hard-working', 'and', 'deeply', 'religy', 'peopl', ',', 'but', 'so', 'poor', 'that', 'they', 'liv', 'with', 'their', 'fiv', 'childr', 'in', 'on', 'two', 'room', '.', 'the', 'fath', 'and', 'moth', 'spent', 'their', 'ev', 'in', 'read', 'aloud', 'to', 'their', 'childr', ',', 'gen', 'from', 'book', 'of', 'a', 'sery', 'charact', '.', 'though', 'alway', 'sick', 'and', 'del', 'dostoevsky', 'cam', 'out', 'third', 'in', 'the', 'fin', 'examin', 'of', 'the', 'petersburg', 'school', 'of', 'engin', '.', 'ther', 'he', 'had', 'already', 'begun', 'his', 'first', 'work', ',', '``', 'poor', 'folk', '.', \"''\", 'thi', 'story', 'was', 'publ', 'by', 'the', 'poet', 'nekrassov', 'in', 'his', 'review', 'and', 'was', 'receiv', 'with', 'acclam', '.', 'the', 'shy', ',', 'unknown', 'you', 'found', 'himself', 'inst', 'someth', 'of', 'a', 'celebr', '.', 'a', 'bril', 'and', 'success', 'car', 'seem', 'to', 'op', 'bef', 'him', ',', 'but', 'thos', 'hop', 'wer', 'soon', 'dash', '.', 'in', '1849', 'he', 'was', 'arrest', '.', 'though', 'neith', 'by', 'tempera', 'nor', 'convict', 'a', 'revolv', ',', 'dostoevsky', 'was', 'on', 'of', 'a', 'littl', 'group', 'of', 'young', 'men', 'who', 'met']\n"
     ]
    }
   ],
   "source": [
    "# Stemming/Lemmatization\n",
    "len(crimetokens)\n",
    "crimetokens[:100]\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "# Compare the two stemmers \n",
    "crimePstem = [porter.stem(t) for t in crimetokens]\n",
    "print(crimePstem[:200])\n",
    "\n",
    "crimeLstem = [lancaster.stem(t) for t in crimetokens]\n",
    "print(crimeLstem[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/venkatasharatsripada/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Produced', 'by', 'John', 'Bickers', ';', 'and', 'Dagny', 'CRIME', 'AND', 'PUNISHMENT', 'By', 'Fyodor', 'Dostoevsky', 'Translated', 'By', 'Constance', 'Garnett', 'TRANSLATOR', \"'S\", 'PREFACE', 'A', 'few', 'word', 'about', 'Dostoevsky', 'himself', 'may', 'help', 'the', 'English', 'reader', 'to', 'understand', 'his', 'work', '.', 'Dostoevsky', 'wa', 'the', 'son', 'of', 'a', 'doctor', '.', 'His', 'parent', 'were', 'very', 'hard-working', 'and', 'deeply', 'religious', 'people', ',', 'but', 'so', 'poor', 'that', 'they', 'lived', 'with', 'their', 'five', 'child', 'in', 'only', 'two', 'room', '.', 'The', 'father', 'and', 'mother', 'spent', 'their', 'evening', 'in', 'reading', 'aloud', 'to', 'their', 'child', ',', 'generally', 'from', 'book', 'of', 'a', 'serious', 'character', '.', 'Though', 'always', 'sickly', 'and', 'delicate', 'Dostoevsky', 'came', 'out', 'third', 'in', 'the', 'final', 'examination', 'of', 'the', 'Petersburg', 'school', 'of', 'Engineering', '.', 'There', 'he', 'had', 'already', 'begun', 'his', 'first', 'work', ',', '``', 'Poor', 'Folk', '.', \"''\", 'This', 'story', 'wa', 'published', 'by', 'the', 'poet', 'Nekrassov', 'in', 'his', 'review', 'and', 'wa', 'received', 'with', 'acclamation', '.', 'The', 'shy', ',', 'unknown', 'youth', 'found', 'himself', 'instantly', 'something', 'of', 'a', 'celebrity', '.', 'A', 'brilliant', 'and', 'successful', 'career', 'seemed', 'to', 'open', 'before', 'him', ',', 'but', 'those', 'hope', 'were', 'soon', 'dashed', '.', 'In', '1849', 'he', 'wa', 'arrested', '.', 'Though', 'neither', 'by', 'temperament', 'nor', 'conviction', 'a', 'revolutionist', ',', 'Dostoevsky', 'wa', 'one', 'of', 'a', 'little', 'group', 'of', 'young', 'men', 'who', 'met']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "crimeLemma = [wnl.lemmatize(t) for t in crimetokens]\n",
    "print(crimeLemma[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1364"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My exercise with desert.txt \n",
    "\n",
    "f = open('desert.txt')\n",
    "rawtext = f.read()\n",
    "\n",
    "# Tokenize\n",
    "deserttokens = nltk.word_tokenize(rawtext)\n",
    "len(deserttokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['three', 'calgarian', 'have', 'found', 'a', 'rather', 'unusu', 'way', 'of', 'leav', 'snow', 'and', 'ice', 'behind', '.', 'they', 'set', 'off', 'thi', 'week', 'on', 'foot', 'and', 'by', 'camel', 'on', 'a', 'gruel', 'trek', 'across', 'the', 'burn', 'arabian', 'desert', '.', 'when', 'they', 'were', 'still', 'in', 'canada', ',', 'plan', 'their', 'trip', ',', 'they', 'expect', 'they', 'would', 'feel', 'lone', '.', 'but', 'after', 'two', 'day', 'into', 'the', '1,200', 'kilometr', 'journey', ',', 'the', 'caravan', 'ha', 'won', 'celebr', 'statu', 'among', 'the', 'nativ', 'bedouin', 'peopl', 'and', 'govern', 'offici', 'of', 'oman', '.', 'some', 'have', 'excitedli', 'tag', 'along', ',', 'say', 'expedit', 'leader', 'jami', 'clark', '.', 'mr.', 'clark', 'is', 'make', 'the', 'trek', 'with', 'hi', 'older', 'brother', 'leigh', 'and', 'their', 'friend', 'bruce', 'kirbi', '.', 'they', 'have', 'hire', 'three', 'guid', '.', 'they', 'are', 'now', 'in', 'the', 'omani', 'citi', 'of', 'solalah', 'on', 'the', 'arabian', 'sea', '.', 'the', 'group', 'wa', 'forc', 'to', 'return', 'briefli', 'to', 'replac', 'some', 'broken', 'equip', ',', 'notabl', 'camel', 'saddl', ',', '40', 'kilometr', 'into', 'the', 'odyssey', '.', 'the', 'adventur', 'expect', 'to', 'travel', 'for', 'up', 'to', '55', 'day', 'through', 'the', 'scorch', 'heat', 'and', 'tower', 'dune', 'of', 'oman', ',', 'saudi', 'arabia', 'and', 'the', 'unit', 'arab', 'emir', '.', 'they', 'are', 'attempt', 'to', 'becom', 'the', 'first', 'western', 'in', 'more', 'than', '50', 'year', 'to', 'cross', 'the', 'empti', 'quarter', 'of', 'arabia']\n",
      "['three', 'calg', 'hav', 'found', 'a', 'rath', 'unus', 'way', 'of', 'leav', 'snow', 'and', 'ic', 'behind', '.', 'they', 'set', 'off', 'thi', 'week', 'on', 'foot', 'and', 'by', 'camel', 'on', 'a', 'gruel', 'trek', 'across', 'the', 'burn', 'arab', 'desert', '.', 'when', 'they', 'wer', 'stil', 'in', 'canad', ',', 'plan', 'their', 'trip', ',', 'they', 'expect', 'they', 'would', 'feel', 'lon', '.', 'but', 'aft', 'two', 'day', 'into', 'the', '1,200', 'kilomet', 'journey', ',', 'the', 'carav', 'has', 'won', 'celebr', 'stat', 'among', 'the', 'nat', 'bedouin', 'peopl', 'and', 'govern', 'off', 'of', 'om', '.', 'som', 'hav', 'excit', 'tag', 'along', ',', 'say', 'expedit', 'lead', 'jamy', 'clark', '.', 'mr.', 'clark', 'is', 'mak', 'the', 'trek', 'with', 'his', 'old', 'broth', 'leigh', 'and', 'their', 'friend', 'bruc', 'kirby', '.', 'they', 'hav', 'hir', 'three', 'guid', '.', 'they', 'ar', 'now', 'in', 'the', 'oman', 'city', 'of', 'solalah', 'on', 'the', 'arab', 'sea', '.', 'the', 'group', 'was', 'forc', 'to', 'return', 'brief', 'to', 'replac', 'som', 'brok', 'equip', ',', 'not', 'camel', 'saddl', ',', '40', 'kilomet', 'into', 'the', 'odyssey', '.', 'the', 'adv', 'expect', 'to', 'travel', 'for', 'up', 'to', '55', 'day', 'through', 'the', 'scorch', 'heat', 'and', 'tow', 'dun', 'of', 'om', ',', 'saud', 'arab', 'and', 'the', 'unit', 'arab', 'emir', '.', 'they', 'ar', 'attempt', 'to', 'becom', 'the', 'first', 'western', 'in', 'mor', 'than', '50', 'year', 'to', 'cross', 'the', 'empty', 'quart', 'of', 'arab']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Compare the two stemmers \n",
    "desertPstem = [porter.stem(t) for t in deserttokens]\n",
    "print(desertPstem[:200])\n",
    "\n",
    "desertLstem = [lancaster.stem(t) for t in deserttokens]\n",
    "print(desertLstem[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quit\n",
      "quit\n",
      "they\n",
      "they\n",
      "''\n",
      "''\n",
      "hold\n",
      "hold\n",
      "of\n",
      "of\n",
      "met\n",
      "met\n",
      "al\n",
      "al\n",
      "everyth\n",
      "everyth\n",
      "for\n",
      "for\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    random.seed(i)\n",
    "    rand_index = random.randint(0, len(deserttokens)-1)\n",
    "    print(desertPstem[rand_index])\n",
    "    print(desertLstem[rand_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize regular expr\n",
    "pattern = \"Mr. Black and Mrs. Brown attended the lecture by Dr. Gray, but Gov. White wasn’t there.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')]]\n",
      "[('The', 'AT'), ('Fulton', 'NP-TL')]\n",
      "Tagged-words: %s ('The', 'AT')\n"
     ]
    }
   ],
   "source": [
    "# Week-4 Async Labs\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# tagged_sents gives a list of sentences, each sentence is a list of (word, tag) tuples\n",
    "print(brown.tagged_sents()[:2])\n",
    "\n",
    "print(brown.tagged_words()[:2])\n",
    "\n",
    "wordtag = brown.tagged_words()[0]\n",
    "print(wordtag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRON'),\n",
       " ('was', 'VERB'),\n",
       " ('among', 'ADP'),\n",
       " ('these', 'DET'),\n",
       " ('that', 'ADP'),\n",
       " ('Hinkle', 'NOUN'),\n",
       " ('identified', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('photograph', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('Barco', 'NOUN'),\n",
       " ('!', '.'),\n",
       " ('!', '.'),\n",
       " ('For', 'ADP'),\n",
       " ('it', 'PRON'),\n",
       " ('seems', 'VERB'),\n",
       " ('that', 'ADP'),\n",
       " ('Barco', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('fancying', 'VERB'),\n",
       " ('himself', 'PRON'),\n",
       " ('a', 'DET'),\n",
       " (\"ladies'\", 'NOUN'),\n",
       " ('man', 'NOUN'),\n",
       " ('(', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('why', 'ADV'),\n",
       " ('not', 'ADV'),\n",
       " (',', '.'),\n",
       " ('after', 'ADP'),\n",
       " ('seven', 'NUM'),\n",
       " ('marriages', 'NOUN'),\n",
       " ('?', '.'),\n",
       " ('?', '.'),\n",
       " (')', '.'),\n",
       " (',', '.'),\n",
       " ('had', 'VERB'),\n",
       " ('listed', 'VERB'),\n",
       " ('himself', 'PRON'),\n",
       " ('for', 'ADP'),\n",
       " ('Mormon', 'NOUN'),\n",
       " ('Beard', 'NOUN'),\n",
       " ('roles', 'NOUN'),\n",
       " ('at', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('instigation', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('his', 'DET'),\n",
       " ('fourth', 'ADJ'),\n",
       " ('murder', 'NOUN')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('universal_tagset')\n",
    "\n",
    "brown.categories()\n",
    "brown_humor_tagged = brown.tagged_words(categories='humor', tagset='universal')\n",
    "brown_humor_tagged[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/venkatasharatsripada/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "( (S \n",
      "    (NP-SBJ \n",
      "      (NP (NNP Pierre) (NNP Vinken) )\n",
      "      (, ,) \n",
      "      (ADJP \n",
      "        (NP (CD 61) (NNS years) )\n",
      "        (JJ old) )\n",
      "      (, ,) ) \n",
      "\n",
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.', 'Mr.', 'Vinken']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('treebank')\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "treebank_text = treebank.raw()\n",
    "print(treebank_text[:150], '\\n')\n",
    "treebank_tokens = treebank.words()\n",
    "print(treebank_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100676\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.'), ('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.'), ('Rudolph', 'NNP'), ('Agnew', 'NNP'), (',', ','), ('55', 'CD'), ('years', 'NNS'), ('old', 'JJ'), ('and', 'CC'), ('former', 'JJ'), ('chairman', 'NN'), ('of', 'IN'), ('Consolidated', 'NNP'), ('Gold', 'NNP'), ('Fields', 'NNP'), ('PLC', 'NNP'), (',', ','), ('was', 'VBD'), ('named', 'VBN'), ('*-1', '-NONE-'), ('a', 'DT')]\n",
      "3914\n",
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "treebank_tagged_words = treebank.tagged_words()[:50]\n",
    "print(len(treebank.tagged_words()))\n",
    "print(treebank_tagged_words[:50])\n",
    "\n",
    "treebank_tagged = treebank.tagged_sents()[:2]\n",
    "print(len(treebank.tagged_sents()))\n",
    "print(treebank_tagged[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['NNP', ',', 'CD', 'NNS', 'JJ', 'MD', 'VB', 'DT', 'NN', 'IN', '.', 'VBZ', 'VBG', 'CC', 'VBD', 'VBN', '-NONE-']) \n",
      "\n",
      "NNP 14\n",
      ", 5\n",
      "NN 5\n",
      "JJ 4\n",
      "DT 4\n",
      "CD 3\n",
      "IN 3\n",
      "NNS 2\n",
      ". 2\n",
      "MD 1\n",
      "VB 1\n",
      "VBZ 1\n",
      "VBG 1\n",
      "CC 1\n",
      "VBD 1\n",
      "VBN 1\n",
      "-NONE- 1\n"
     ]
    }
   ],
   "source": [
    "tag_fd = nltk.FreqDist(tag for (word, tag) in treebank_tagged_words)\n",
    "print(tag_fd.keys(), '\\n')\n",
    "for tag,freq in tag_fd.most_common():\n",
    "    print (tag, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['N', ',', 'C', 'J', 'M', 'V', 'D', 'I', '.', '-']) \n",
      "\n",
      "N 21\n",
      ", 5\n",
      "V 5\n",
      "C 4\n",
      "J 4\n",
      "D 4\n",
      "I 3\n",
      ". 2\n",
      "M 1\n",
      "- 1\n"
     ]
    }
   ],
   "source": [
    "# use the first letter of the POS tag to get classes of tags\n",
    "tag_classes_fd = nltk.FreqDist(tag[0] for (word, tag) in treebank_tagged_words)\n",
    "print(tag_classes_fd.keys(), '\\n')\n",
    "for tag,freq in tag_classes_fd.most_common():\n",
    "    print (tag, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NN'),\n",
       " ('Vinken', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('61', 'NN'),\n",
       " ('years', 'NN'),\n",
       " ('old', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('will', 'NN'),\n",
       " ('join', 'NN'),\n",
       " ('the', 'NN'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'NN'),\n",
       " ('a', 'NN'),\n",
       " ('nonexecutive', 'NN'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NN'),\n",
       " ('29', 'NN'),\n",
       " ('.', 'NN'),\n",
       " ('Mr.', 'NN'),\n",
       " ('Vinken', 'NN'),\n",
       " ('is', 'NN'),\n",
       " ('chairman', 'NN'),\n",
       " ('of', 'NN'),\n",
       " ('Elsevier', 'NN'),\n",
       " ('N.V.', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('the', 'NN'),\n",
       " ('Dutch', 'NN'),\n",
       " ('publishing', 'NN'),\n",
       " ('group', 'NN'),\n",
       " ('.', 'NN'),\n",
       " ('Rudolph', 'NN'),\n",
       " ('Agnew', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('55', 'NN'),\n",
       " ('years', 'NN'),\n",
       " ('old', 'NN'),\n",
       " ('and', 'NN'),\n",
       " ('former', 'NN'),\n",
       " ('chairman', 'NN'),\n",
       " ('of', 'NN'),\n",
       " ('Consolidated', 'NN'),\n",
       " ('Gold', 'NN'),\n",
       " ('Fields', 'NN'),\n",
       " ('PLC', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('was', 'NN'),\n",
       " ('named', 'NN'),\n",
       " ('*-1', 'NN'),\n",
       " ('a', 'NN')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The POS Tagging Task and Training a default tagger\n",
    "size = int(len(treebank_tagged) * 0.9)\n",
    "treebank_train = treebank_tagged[:size]\n",
    "treebank_test = treebank_tagged[size:]\n",
    "\n",
    "# creates the tagger\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "\n",
    "# show the effect of the tagger by tagging the first 50 words\n",
    "t0.tag(treebank_tokens[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615385"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0.evaluate(treebank_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.'),\n",
       " ('Mr.', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('chairman', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Elsevier', 'NNP'),\n",
       " ('N.V.', 'NNP'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('Dutch', 'NNP'),\n",
       " ('publishing', 'VBG'),\n",
       " ('group', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Rudolph', None),\n",
       " ('Agnew', None),\n",
       " (',', ','),\n",
       " ('55', None),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " ('and', None),\n",
       " ('former', None),\n",
       " ('chairman', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Consolidated', None),\n",
       " ('Gold', None),\n",
       " ('Fields', None),\n",
       " ('PLC', None),\n",
       " (',', ','),\n",
       " ('was', None),\n",
       " ('named', None),\n",
       " ('*-1', None),\n",
       " ('a', 'DT')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 3:  Training the N-Gram Tagger\n",
    "\n",
    "t1 = nltk.UnigramTagger(treebank_tagged)\n",
    "t1.tag(treebank_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3076923076923077"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = nltk.UnigramTagger(treebank_train)\n",
    "t1.evaluate(treebank_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46153846153846156"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(treebank_train, backoff=t0)\n",
    "t2 = nltk.BigramTagger(treebank_train, backoff=t1)\n",
    "t2.evaluate(treebank_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.', \"She was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.\"]\n",
      "[('[', 'NNS'), ('Emma', 'NNP'), ('by', 'IN'), ('Jane', 'NNP'), ('Austen', 'NNP'), ('1816', 'CD'), (']', 'NNP'), ('VOLUME', 'NNP'), ('I', 'PRP'), ('CHAPTER', 'VBP')]\n",
      "dict_keys(['NNS', 'NNP', 'IN', 'CD', 'PRP', 'VBP', ',', 'NN', 'CC', 'JJ', 'DT', 'VBD', 'TO', 'VB', 'JJS', ':', 'VBN', 'RB', '.', 'RBS', 'PRP$', 'POS', 'JJR', 'WP', 'RBR', 'VBG', 'WDT', 'EX', 'MD', 'WRB', 'PDT', '(', ')', 'RP', \"''\", 'VBZ', '``', 'UH', 'WP$', 'NNPS', 'FW', '$']) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using corpus austen-emma.txt\n",
    "\n",
    "# Step-1: Tokenize the raw_text into sentences and then words\n",
    "emma_sent = nltk.sent_tokenize(emmatext)\n",
    "print(emma_sent[:2])\n",
    "\n",
    "emma_tagged_words = []\n",
    "\n",
    "# Step-2: Then tokenize sentences to words\n",
    "for sentence in emma_sent:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    emma_tagged_words.append(nltk.pos_tag(words))\n",
    "\n",
    "#NOTE: emma_tagged_words is list of lists - so, we have to flatten it before accessing the (word, tag)    \n",
    "emma_tagged_words_flatten = []\n",
    "for word_list in emma_tagged_words:\n",
    "    emma_tagged_words_flatten.extend(word_list)\n",
    "    \n",
    "print(emma_tagged_words_flatten[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['NNS', 'NNP', 'IN', 'CD', 'PRP', 'VBP', ',', 'NN', 'CC', 'JJ', 'DT', 'VBD', 'TO', 'VB', 'JJS', ':', 'VBN', 'RB', '.', 'RBS', 'PRP$', 'POS', 'JJR', 'WP', 'RBR', 'VBG', 'WDT', 'EX', 'MD', 'WRB', 'PDT', '(', ')', 'RP', \"''\", 'VBZ', '``', 'UH', 'WP$', 'NNPS', 'FW', '$']) \n",
      "\n",
      "NN 19330\n",
      "IN 17880\n",
      "PRP 15619\n",
      "RB 12997\n",
      "DT 12743\n",
      ", 12016\n",
      "JJ 10249\n",
      "NNP 9095\n",
      "VBD 9049\n",
      "VB 8941\n",
      ". 8041\n",
      "CC 6930\n",
      ": 5627\n",
      "TO 5181\n",
      "PRP$ 4698\n",
      "VBN 4637\n",
      "MD 4426\n",
      "NNS 3560\n",
      "VBP 3389\n",
      "VBG 3060\n",
      "'' 2558\n",
      "VBZ 2221\n",
      "`` 1847\n",
      "WP 907\n",
      "WRB 897\n",
      "POS 864\n",
      "CD 794\n",
      "WDT 727\n",
      "JJR 543\n",
      "RP 466\n",
      "EX 456\n",
      "JJS 435\n",
      "PDT 431\n",
      "RBR 361\n",
      "UH 344\n",
      "RBS 184\n",
      "( 107\n",
      ") 107\n",
      "WP$ 39\n",
      "FW 17\n",
      "NNPS 13\n",
      "$ 1\n"
     ]
    }
   ],
   "source": [
    "# Step-3: Run frequency distribution\n",
    "tags_fd = nltk.FreqDist(tag for (word, tag) in emma_tagged_words_flatten)\n",
    "print(tags_fd.keys(), '\\n')\n",
    "\n",
    "# Step-4: Find most common tags\n",
    "for tag, freq in tags_fd.most_common():\n",
    "    print(tag, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/venkatasharatsripada/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'isdigit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-0f156b39d122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'averaged_perceptron_tagger'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memma_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#t0 = nltk.DefaultTagger('NN')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \"\"\"\n\u001b[1;32m    160\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    116\u001b[0m         )\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Maps to the specified tagset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             tag, conf = (\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             tag, conf = (\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"-\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"!HYPHEN\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"!YEAR\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'isdigit'"
     ]
    }
   ],
   "source": [
    "# Step-2: Run the POS tagging t2 on the tokenized words\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(emma_word)\n",
    "\n",
    "#t0 = nltk.DefaultTagger('NN')\n",
    "#t1 = nltk.UnigramTagger(emma_word, backoff=t0)\n",
    "#t2 = nltk.BigramTagger(emma_word, backoff=t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary', 'saw', 'Bob']\n",
      "<class 'nltk.tree.Tree'>\n",
      "(S (NP (Prop Mary)) (VP (V saw) (NP (Prop Bob))))\n",
      "(S\n",
      "  (NP (Prop John))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP\n",
      "      (Det the)\n",
      "      (N man)\n",
      "      (PP\n",
      "        (P in)\n",
      "        (NP\n",
      "          (Det the)\n",
      "          (N park)\n",
      "          (PP (P with) (NP (Det a) (N telescope))))))))\n",
      "(S\n",
      "  (NP (Prop John))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det the) (N man))\n",
      "    (PP\n",
      "      (P in)\n",
      "      (NP\n",
      "        (Det the)\n",
      "        (N park)\n",
      "        (PP (P with) (NP (Det a) (N telescope)))))))\n",
      "(S\n",
      "  (NP (Prop John))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det the) (N man) (PP (P in) (NP (Det the) (N park))))\n",
      "    (PP (P with) (NP (Det a) (N telescope)))))\n"
     ]
    }
   ],
   "source": [
    "# Week-5: Parsing and context-free grammars (CFG)\n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\"\n",
    "  NP -> Prop | Det N | Det N PP\n",
    "  Prop -> \"John\" | \"Mary\" | \"Bob\" \n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")\n",
    "\n",
    "# Top-down parser\n",
    "rd_parser = nltk.RecursiveDescentParser(grammar)\n",
    "\n",
    "# Sample sentence\n",
    "senttext = \"Mary saw Bob\"\n",
    "sentlist = senttext.split()\n",
    "print(sentlist)\n",
    "\n",
    "trees = rd_parser.parse(sentlist)\n",
    "\n",
    "# convert the generator to a list\n",
    "treelist = list(trees)\n",
    "\n",
    "# what type is an individual tree?\n",
    "print(type(treelist[0]))\n",
    "\n",
    "for tree in treelist:\n",
    "    print (tree)\n",
    "    \n",
    "sent2list = \"John saw the man in the park with a telescope\".split()\n",
    "\n",
    "# Shows three different parsers based on the grammar\n",
    "for tree in rd_parser.parse(sent2list):\n",
    "    print (tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Prop I))\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n",
      "(S\n",
      "  (NP (Prop I))\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n"
     ]
    }
   ],
   "source": [
    "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\" | \"shot\"\n",
    "  NP -> Prop | Det N | Det N PP\n",
    "  Prop -> \"John\" | \"Mary\" | \"Bob\" | \"I\"\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\" | \"elephant\" | \"pajamas\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")\n",
    "\n",
    "rd_parser = nltk.RecursiveDescentParser(groucho_grammar)\n",
    "\n",
    "sent4list = \"I shot an elephant in my pajamas\".split()\n",
    "for tree in rd_parser.parse(sent4list):\n",
    "\tprint (tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (VP (V book) (NP (Det that) (N flight))))\n"
     ]
    }
   ],
   "source": [
    "# extend the grammar for the flight grammar:  adding a rule to S and some words\n",
    "flight_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP | VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\" | \"shot\" | \"book\"\n",
    "  NP -> Prop | Det N | Det N PP\n",
    "  Prop -> \"John\" | \"Mary\" | \"Bob\" | \"I\"\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\" | \"that\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\" | \"elephant\" | \"pajamas\" | \"flight\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")\n",
    "\n",
    "# make a recursive descent parser and parse the sentence\n",
    "rd_parser = nltk.RecursiveDescentParser(flight_grammar)\n",
    "\n",
    "sent5list = 'book that flight'.split()\n",
    "for tree in rd_parser.parse(sent5list):\n",
    "\tprint (tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'prefer', 'through', 'Houston'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-ebd175f64dfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msent6list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'I prefer a flight through Houston'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrd_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent6list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/parse/recursivedescent.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Start a recursive descent parse, with an initial tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    678\u001b[0m                 \u001b[0;34m\"Grammar does not cover some of the \"\u001b[0m \u001b[0;34m\"input words: %r.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'prefer', 'through', 'Houston'\"."
     ]
    }
   ],
   "source": [
    "# Exercise 5.6.3\n",
    "\n",
    "# Start by using the flight_grammar defined above\n",
    "\n",
    "sent6list = 'I prefer a flight through Houston'.split()\n",
    "for tree in rd_parser.parse(sent6list):\n",
    "    print (tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Prop I))\n",
      "  (VP\n",
      "    (V prefer)\n",
      "    (NP (Det a) (N flight) (PP (P through) (NP (Prop Houston))))))\n",
      "(S\n",
      "  (NP (Prop I))\n",
      "  (VP\n",
      "    (V prefer)\n",
      "    (NP (Det a) (N flight))\n",
      "    (PP (P through) (NP (Prop Houston)))))\n"
     ]
    }
   ],
   "source": [
    "# Note that, we miss the words prefer, through and Houston. So, update the grammar\n",
    "flight_grammar_1 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP | VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\" | \"shot\" | \"book\" | \"prefer\"\n",
    "  NP -> Prop | Det N | Det N PP\n",
    "  Prop -> \"John\" | \"Mary\" | \"Bob\" | \"I\" | \"Houston\"\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\" | \"that\" \n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\" | \"elephant\" | \"pajamas\" | \"flight\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\" | \"through\"\n",
    "  \"\"\")\n",
    "\n",
    "# Update the flight_grammar/parser\n",
    "rd_parser_1 = nltk.RecursiveDescentParser(flight_grammar_1)\n",
    "\n",
    "for tree in rd_parser_1.parse(sent6list):\n",
    "    print (tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot', 'Abbott', 'Abby', 'Abdel', 'Abdul', 'Abdulkarim', 'Abdullah', 'Abe', 'Abel', 'Abelard', 'Abner', 'Abraham', 'Abram', 'Ace', 'Adair', 'Adam']\n",
      "['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', 'Abby', 'Abigael', 'Abigail', 'Abigale', 'Abra', 'Acacia', 'Ada', 'Adah', 'Adaline', 'Adara', 'Addie', 'Addis', 'Adel', 'Adela']\n",
      "\n",
      "Size = 7944, Type = <class 'list'>\n",
      "\n",
      "[('Aamir', 'male'), ('Aaron', 'male'), ('Abbey', 'male'), ('Abbie', 'male'), ('Abbot', 'male'), ('Abbott', 'male'), ('Abby', 'male'), ('Abdel', 'male'), ('Abdul', 'male'), ('Abdulkarim', 'male'), ('Abdullah', 'male'), ('Abe', 'male'), ('Abel', 'male'), ('Abelard', 'male'), ('Abner', 'male'), ('Abraham', 'male'), ('Abram', 'male'), ('Ace', 'male'), ('Adair', 'male'), ('Adam', 'male')]\n"
     ]
    }
   ],
   "source": [
    "# Week-7 labs\n",
    "import nltk\n",
    "\n",
    "# nltk.download('names')\n",
    "\n",
    "from nltk.corpus import names\n",
    "print(names.words('male.txt')[:20])\n",
    "print(names.words('female.txt')[:20])\n",
    "\n",
    "namesgender = ([(name, 'male') for name in names.words('male.txt')] + \n",
    "               [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "print('\\nSize = %d, Type = %s' %(len(namesgender), type(namesgender)))\n",
    "print('\\n%s' %namesgender[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Arvind', 'male'), ('Gennie', 'female'), ('Letta', 'female'), ('Wood', 'male'), ('Dannye', 'female'), ('Agneta', 'female'), ('Cordula', 'female'), ('Georgiamay', 'female'), ('Lars', 'male'), ('Ellene', 'female'), ('Rodina', 'female'), ('Stace', 'female'), ('Darla', 'female'), ('Baird', 'male'), ('Bryn', 'female'), ('Nyssa', 'female'), ('Leonardo', 'male'), ('Cristi', 'female'), ('Carolyne', 'female'), ('Sandy', 'male')]\n"
     ]
    }
   ],
   "source": [
    "# Randomize the list\n",
    "import random\n",
    "random.shuffle(namesgender)\n",
    "print(namesgender[:20])\n",
    "\n",
    "# Split the data into train and test\n",
    "train_names = namesgender[500:]\n",
    "test_names = namesgender[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'last_letter': 'n'}, 'male'), ({'last_letter': 'a'}, 'male'), ({'last_letter': 'a'}, 'female'), ({'last_letter': 'h'}, 'male'), ({'last_letter': 'a'}, 'female'), ({'last_letter': 'k'}, 'male'), ({'last_letter': 'e'}, 'female'), ({'last_letter': 'e'}, 'female'), ({'last_letter': 'a'}, 'female'), ({'last_letter': 'r'}, 'female'), ({'last_letter': 'h'}, 'male'), ({'last_letter': 'i'}, 'female'), ({'last_letter': 'u'}, 'female'), ({'last_letter': 'a'}, 'female'), ({'last_letter': 'e'}, 'female'), ({'last_letter': 'r'}, 'male'), ({'last_letter': 'n'}, 'male'), ({'last_letter': 'a'}, 'female'), ({'last_letter': 'e'}, 'female'), ({'last_letter': 'e'}, 'female')]\n",
      "male\n",
      "female\n"
     ]
    }
   ],
   "source": [
    "# define a feature extraction function for each name\n",
    "def gender_features(word):\n",
    "    return{'last_letter': word[-1]}\n",
    "\n",
    "train_set = [(gender_features(n), g) for (n, g) in train_names]\n",
    "test_set = [(gender_features(n), g) for (n, g) in test_names]\n",
    "\n",
    "print(train_set[:20])\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(classifier.classify(gender_features('Neo')))\n",
    "print(classifier.classify(gender_features('Trinity')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.794\n"
     ]
    }
   ],
   "source": [
    "# classify accuracy function runs the classifier on the test set and reports\n",
    "#   comparisons between predicted labels and actual/gold labels\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'k'              male : female =     45.4 : 1.0\n",
      "             last_letter = 'a'            female : male   =     33.1 : 1.0\n",
      "             last_letter = 'f'              male : female =     15.4 : 1.0\n",
      "             last_letter = 'p'              male : female =     11.9 : 1.0\n",
      "             last_letter = 'v'              male : female =     10.6 : 1.0\n",
      "             last_letter = 'd'              male : female =      9.6 : 1.0\n",
      "             last_letter = 'm'              male : female =      7.8 : 1.0\n",
      "             last_letter = 'o'              male : female =      7.6 : 1.0\n",
      "             last_letter = 'r'              male : female =      6.5 : 1.0\n",
      "             last_letter = 'g'              male : female =      5.1 : 1.0\n",
      "             last_letter = 'w'              male : female =      4.5 : 1.0\n",
      "             last_letter = 'z'              male : female =      4.4 : 1.0\n",
      "             last_letter = 's'              male : female =      4.3 : 1.0\n",
      "             last_letter = 't'              male : female =      4.3 : 1.0\n",
      "             last_letter = 'j'              male : female =      4.0 : 1.0\n",
      "             last_letter = 'b'              male : female =      3.9 : 1.0\n",
      "             last_letter = 'i'            female : male   =      3.8 : 1.0\n",
      "             last_letter = 'u'              male : female =      3.3 : 1.0\n",
      "             last_letter = 'n'              male : female =      2.1 : 1.0\n",
      "             last_letter = 'e'            female : male   =      1.8 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# this function available for naive bayes classifiers\n",
    "print(classifier.show_most_informative_features(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "correct=female   guess=male     name=Abagael                       \n",
      "correct=female   guess=male     name=Adel                          \n",
      "correct=female   guess=male     name=Anett                         \n",
      "correct=female   guess=male     name=Ardys                         \n",
      "correct=female   guess=male     name=Barb                          \n",
      "correct=female   guess=male     name=Beitris                       \n",
      "correct=female   guess=male     name=Bryn                          \n",
      "correct=female   guess=male     name=Caryl                         \n",
      "correct=female   guess=male     name=Charo                         \n",
      "correct=female   guess=male     name=Cloris                        \n",
      "correct=female   guess=male     name=Cybill                        \n",
      "correct=female   guess=male     name=Eden                          \n",
      "correct=female   guess=male     name=Ellyn                         \n",
      "correct=female   guess=male     name=Evelyn                        \n",
      "correct=female   guess=male     name=Gabriel                       \n",
      "correct=female   guess=male     name=Gael                          \n",
      "correct=female   guess=male     name=Gertrudis                     \n",
      "correct=female   guess=male     name=Glad                          \n",
      "correct=female   guess=male     name=Gladis                        \n",
      "correct=female   guess=male     name=Gwendolen                     \n",
      "correct=female   guess=male     name=Harriott                      \n",
      "correct=female   guess=male     name=Jess                          \n",
      "correct=female   guess=male     name=Jonell                        \n",
      "correct=female   guess=male     name=Juliet                        \n",
      "correct=female   guess=male     name=Kara-Lynn                     \n",
      "correct=female   guess=male     name=Karin                         \n",
      "correct=female   guess=male     name=Kat                           \n",
      "correct=female   guess=male     name=Kaylil                        \n",
      "correct=female   guess=male     name=Kaylyn                        \n",
      "correct=female   guess=male     name=Keriann                       \n",
      "correct=female   guess=male     name=Kris                          \n",
      "correct=female   guess=male     name=Kristien                      \n",
      "correct=female   guess=male     name=Lark                          \n",
      "correct=female   guess=male     name=Lillis                        \n",
      "correct=female   guess=male     name=Linn                          \n",
      "correct=female   guess=male     name=Linnell                       \n",
      "correct=female   guess=male     name=Madlen                        \n",
      "correct=female   guess=male     name=Margret                       \n",
      "correct=female   guess=male     name=Mariel                        \n",
      "correct=female   guess=male     name=Meggan                        \n",
      "correct=female   guess=male     name=Merilyn                       \n",
      "correct=female   guess=male     name=Moreen                        \n",
      "correct=female   guess=male     name=Nat                           \n",
      "correct=female   guess=male     name=Ninon                         \n",
      "correct=female   guess=male     name=Piper                         \n",
      "correct=female   guess=male     name=Rosaleen                      \n",
      "correct=female   guess=male     name=Rosamond                      \n",
      "correct=female   guess=male     name=Saraann                       \n",
      "correct=female   guess=male     name=Shel                          \n",
      "correct=female   guess=male     name=Sibel                         \n",
      "correct=female   guess=male     name=Wallis                        \n",
      "correct=male     guess=female   name=Alexei                        \n",
      "correct=male     guess=female   name=Barnie                        \n",
      "correct=male     guess=female   name=Blaine                        \n",
      "correct=male     guess=female   name=Brady                         \n",
      "correct=male     guess=female   name=Bruce                         \n",
      "correct=male     guess=female   name=Butch                         \n",
      "correct=male     guess=female   name=Chevy                         \n",
      "correct=male     guess=female   name=Claire                        \n",
      "correct=male     guess=female   name=Clyde                         \n",
      "correct=male     guess=female   name=Cy                            \n",
      "correct=male     guess=female   name=Darby                         \n",
      "correct=male     guess=female   name=Davey                         \n",
      "correct=male     guess=female   name=Davide                        \n",
      "correct=male     guess=female   name=Fletch                        \n",
      "correct=male     guess=female   name=Frankie                       \n",
      "correct=male     guess=female   name=Georgy                        \n",
      "correct=male     guess=female   name=Giavani                       \n",
      "correct=male     guess=female   name=Granville                     \n",
      "correct=male     guess=female   name=Griffith                      \n",
      "correct=male     guess=female   name=Grove                         \n",
      "correct=male     guess=female   name=Iggy                          \n",
      "correct=male     guess=female   name=Irvine                        \n",
      "correct=male     guess=female   name=Jere                          \n",
      "correct=male     guess=female   name=Joe                           \n",
      "correct=male     guess=female   name=Kenny                         \n",
      "correct=male     guess=female   name=Locke                         \n",
      "correct=male     guess=female   name=Lonny                         \n",
      "correct=male     guess=female   name=Lorrie                        \n",
      "correct=male     guess=female   name=Luigi                         \n",
      "correct=male     guess=female   name=Martie                        \n",
      "correct=male     guess=female   name=Mendie                        \n",
      "correct=male     guess=female   name=Micah                         \n",
      "correct=male     guess=female   name=Morley                        \n",
      "correct=male     guess=female   name=Morrie                        \n",
      "correct=male     guess=female   name=Nikki                         \n",
      "correct=male     guess=female   name=Noe                           \n",
      "correct=male     guess=female   name=Pete                          \n",
      "correct=male     guess=female   name=Prentice                      \n",
      "correct=male     guess=female   name=Reilly                        \n",
      "correct=male     guess=female   name=Rickey                        \n",
      "correct=male     guess=female   name=Rodney                        \n",
      "correct=male     guess=female   name=Rube                          \n",
      "correct=male     guess=female   name=Sandy                         \n",
      "correct=male     guess=female   name=Sawyere                       \n",
      "correct=male     guess=female   name=Shay                          \n",
      "correct=male     guess=female   name=Skelly                        \n",
      "correct=male     guess=female   name=Skye                          \n",
      "correct=male     guess=female   name=Solly                         \n",
      "correct=male     guess=female   name=Spike                         \n",
      "correct=male     guess=female   name=Stanly                        \n",
      "correct=male     guess=female   name=Weslie                        \n",
      "correct=male     guess=female   name=Zacherie                      \n"
     ]
    }
   ],
   "source": [
    "# define a function that will compare the classifier labels with the gold standard labels\n",
    "def geterrors(test):\n",
    "    errors = []\n",
    "    for (name, tag) in test:\n",
    "        guess = classifier.classify(gender_features(name))\n",
    "        if guess != tag:\n",
    "            errors.append( (tag, guess, name) )\n",
    "    return errors\n",
    "\n",
    "errors = geterrors(test_names)\n",
    "print(len(errors))\n",
    "\n",
    "# define a function to print the errors\n",
    "def printerrors(errors):\n",
    "    for (tag, guess, name) in sorted(errors):\n",
    "        print('correct={:<8s} guess={:<8s} name={:<30s}'.format(tag, guess, name))\n",
    "\n",
    "printerrors(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'suffix1': 'n', 'suffix2': 'i'}, 'male'), ({'suffix1': 'a', 'suffix2': 'r'}, 'male'), ({'suffix1': 'a', 'suffix2': 'n'}, 'female'), ({'suffix1': 'h', 'suffix2': 'c'}, 'male'), ({'suffix1': 'a', 'suffix2': 'd'}, 'female'), ({'suffix1': 'k', 'suffix2': 'c'}, 'male'), ({'suffix1': 'e', 'suffix2': 'c'}, 'female'), ({'suffix1': 'e', 'suffix2': 'd'}, 'female'), ({'suffix1': 'a', 'suffix2': 'd'}, 'female'), ({'suffix1': 'r', 'suffix2': 'e'}, 'female'), ({'suffix1': 'h', 'suffix2': 'a'}, 'male'), ({'suffix1': 'i', 'suffix2': 'l'}, 'female'), ({'suffix1': 'u', 'suffix2': 'r'}, 'female'), ({'suffix1': 'a', 'suffix2': 'n'}, 'female'), ({'suffix1': 'e', 'suffix2': 't'}, 'female'), ({'suffix1': 'r', 'suffix2': 'o'}, 'male'), ({'suffix1': 'n', 'suffix2': 'o'}, 'male'), ({'suffix1': 'a', 'suffix2': 'n'}, 'female'), ({'suffix1': 'e', 'suffix2': 'i'}, 'female'), ({'suffix1': 'e', 'suffix2': 'i'}, 'female')]\n"
     ]
    }
   ],
   "source": [
    "# Try the same with last 2-letters \n",
    "def gender_features_last2(word):\n",
    "    return{'suffix1': word[-1], 'suffix2': word[-2]}\n",
    "\n",
    "train_set = [(gender_features_last2(n), g) for (n, g) in train_names]\n",
    "test_set = [(gender_features_last2(n), g) for (n, g) in test_names]\n",
    "\n",
    "print(train_set[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.814\n"
     ]
    }
   ],
   "source": [
    "classifier_last2 = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier_last2, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "correct=female   guess=male     name=Abagael                       \n",
      "correct=female   guess=male     name=Adah                          \n",
      "correct=female   guess=male     name=Adel                          \n",
      "correct=female   guess=male     name=Adey                          \n",
      "correct=female   guess=male     name=Anett                         \n",
      "correct=female   guess=male     name=Barb                          \n",
      "correct=female   guess=male     name=Beitris                       \n",
      "correct=female   guess=male     name=Charo                         \n",
      "correct=female   guess=male     name=Cloris                        \n",
      "correct=female   guess=male     name=Courtney                      \n",
      "correct=female   guess=male     name=Dacy                          \n",
      "correct=female   guess=male     name=Eden                          \n",
      "correct=female   guess=male     name=Fay                           \n",
      "correct=female   guess=male     name=Fey                           \n",
      "correct=female   guess=male     name=Gabriel                       \n",
      "correct=female   guess=male     name=Gael                          \n",
      "correct=female   guess=male     name=Georgiamay                    \n",
      "correct=female   guess=male     name=Gertrudis                     \n",
      "correct=female   guess=male     name=Glad                          \n",
      "correct=female   guess=male     name=Gladis                        \n",
      "correct=female   guess=male     name=Gwendolen                     \n",
      "correct=female   guess=male     name=Harriott                      \n",
      "correct=female   guess=male     name=Jess                          \n",
      "correct=female   guess=male     name=Juliet                        \n",
      "correct=female   guess=male     name=Karin                         \n",
      "correct=female   guess=male     name=Kat                           \n",
      "correct=female   guess=male     name=Kris                          \n",
      "correct=female   guess=male     name=Kristien                      \n",
      "correct=female   guess=male     name=Lark                          \n",
      "correct=female   guess=male     name=Lillis                        \n",
      "correct=female   guess=male     name=Madlen                        \n",
      "correct=female   guess=male     name=Margret                       \n",
      "correct=female   guess=male     name=Mariel                        \n",
      "correct=female   guess=male     name=May                           \n",
      "correct=female   guess=male     name=Meggan                        \n",
      "correct=female   guess=male     name=Moreen                        \n",
      "correct=female   guess=male     name=Nat                           \n",
      "correct=female   guess=male     name=Ninon                         \n",
      "correct=female   guess=male     name=Piper                         \n",
      "correct=female   guess=male     name=Rosaleen                      \n",
      "correct=female   guess=male     name=Rosamond                      \n",
      "correct=female   guess=male     name=Sheilah                       \n",
      "correct=female   guess=male     name=Shel                          \n",
      "correct=female   guess=male     name=Shirley                       \n",
      "correct=female   guess=male     name=Sibel                         \n",
      "correct=female   guess=male     name=Wallis                        \n",
      "correct=male     guess=female   name=Alexei                        \n",
      "correct=male     guess=female   name=Barnie                        \n",
      "correct=male     guess=female   name=Blaine                        \n",
      "correct=male     guess=female   name=Brady                         \n",
      "correct=male     guess=female   name=Brent                         \n",
      "correct=male     guess=female   name=Bruce                         \n",
      "correct=male     guess=female   name=Chevy                         \n",
      "correct=male     guess=female   name=Claire                        \n",
      "correct=male     guess=female   name=Clyde                         \n",
      "correct=male     guess=female   name=Darby                         \n",
      "correct=male     guess=female   name=Davide                        \n",
      "correct=male     guess=female   name=Frankie                       \n",
      "correct=male     guess=female   name=Giavani                       \n",
      "correct=male     guess=female   name=Gil                           \n",
      "correct=male     guess=female   name=Granville                     \n",
      "correct=male     guess=female   name=Griffith                      \n",
      "correct=male     guess=female   name=Grove                         \n",
      "correct=male     guess=female   name=Harwell                       \n",
      "correct=male     guess=female   name=Irvine                        \n",
      "correct=male     guess=female   name=Jere                          \n",
      "correct=male     guess=female   name=Kenny                         \n",
      "correct=male     guess=female   name=Locke                         \n",
      "correct=male     guess=female   name=Lonny                         \n",
      "correct=male     guess=female   name=Lorrie                        \n",
      "correct=male     guess=female   name=Luigi                         \n",
      "correct=male     guess=female   name=Martie                        \n",
      "correct=male     guess=female   name=Mendie                        \n",
      "correct=male     guess=female   name=Morrie                        \n",
      "correct=male     guess=female   name=Nikki                         \n",
      "correct=male     guess=female   name=Oswell                        \n",
      "correct=male     guess=female   name=Pete                          \n",
      "correct=male     guess=female   name=Powell                        \n",
      "correct=male     guess=female   name=Prentice                      \n",
      "correct=male     guess=female   name=Reilly                        \n",
      "correct=male     guess=female   name=Rube                          \n",
      "correct=male     guess=female   name=Sandy                         \n",
      "correct=male     guess=female   name=Sawyere                       \n",
      "correct=male     guess=female   name=Shell                         \n",
      "correct=male     guess=female   name=Skelly                        \n",
      "correct=male     guess=female   name=Skye                          \n",
      "correct=male     guess=female   name=Solly                         \n",
      "correct=male     guess=female   name=Spike                         \n",
      "correct=male     guess=female   name=Stanly                        \n",
      "correct=male     guess=female   name=Tirrell                       \n",
      "correct=male     guess=female   name=Tull                          \n",
      "correct=male     guess=female   name=Weslie                        \n",
      "correct=male     guess=female   name=Zacherie                      \n"
     ]
    }
   ],
   "source": [
    "# define a function that will compare the classifier labels with the gold standard labels\n",
    "def geterrors(test):\n",
    "    errors = []\n",
    "    for (name, tag) in test:\n",
    "        guess = classifier_last2.classify(gender_features_last2(name))\n",
    "        if guess != tag:\n",
    "            errors.append( (tag, guess, name) )\n",
    "    return errors\n",
    "\n",
    "errors = geterrors(test_names)\n",
    "print(len(errors))\n",
    "\n",
    "# define a function to print the errors\n",
    "def printerrors(errors):\n",
    "    for (tag, guess, name) in sorted(errors):\n",
    "        print('correct={:<8s} guess={:<8s} name={:<30s}'.format(tag, guess, name))\n",
    "\n",
    "printerrors(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'suffix1': 'n', 'suffix2': 'i', 'suffix3': 'l'}, 'male'), ({'suffix1': 'a', 'suffix2': 'r', 'suffix3': 'd'}, 'male'), ({'suffix1': 'a', 'suffix2': 'n', 'suffix3': 'i'}, 'female'), ({'suffix1': 'h', 'suffix2': 'c', 'suffix3': 'i'}, 'male'), ({'suffix1': 'a', 'suffix2': 'd', 'suffix3': 'l'}, 'female'), ({'suffix1': 'k', 'suffix2': 'c', 'suffix3': 'i'}, 'male'), ({'suffix1': 'e', 'suffix2': 'c', 'suffix3': 'n'}, 'female'), ({'suffix1': 'e', 'suffix2': 'd', 'suffix3': 'n'}, 'female'), ({'suffix1': 'a', 'suffix2': 'd', 'suffix3': 'A'}, 'female'), ({'suffix1': 'r', 'suffix2': 'e', 'suffix3': 't'}, 'female'), ({'suffix1': 'h', 'suffix2': 'a', 'suffix3': 'i'}, 'male'), ({'suffix1': 'i', 'suffix2': 'l', 'suffix3': 'l'}, 'female'), ({'suffix1': 'u', 'suffix2': 'r', 'suffix3': 'P'}, 'female'), ({'suffix1': 'a', 'suffix2': 'n', 'suffix3': 'i'}, 'female'), ({'suffix1': 'e', 'suffix2': 't', 'suffix3': 't'}, 'female'), ({'suffix1': 'r', 'suffix2': 'o', 'suffix3': 'l'}, 'male'), ({'suffix1': 'n', 'suffix2': 'o', 'suffix3': 'm'}, 'male'), ({'suffix1': 'a', 'suffix2': 'n', 'suffix3': 'i'}, 'female'), ({'suffix1': 'e', 'suffix2': 'i', 'suffix3': 'h'}, 'female'), ({'suffix1': 'e', 'suffix2': 'i', 'suffix3': 'k'}, 'female')]\n"
     ]
    }
   ],
   "source": [
    "# Try the same with last 3-letters\n",
    "def gender_features_last3(word):\n",
    "    try:\n",
    "        return{'suffix1': word[-1], 'suffix2': word[-2], 'suffix3': word[-3]}\n",
    "    except IndexError:\n",
    "        return gender_features_last2(word)\n",
    "\n",
    "train_set = [(gender_features_last3(n), g) for (n, g) in train_names]\n",
    "test_set = [(gender_features_last3(n), g) for (n, g) in test_names]\n",
    "\n",
    "print(train_set[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816\n"
     ]
    }
   ],
   "source": [
    "classifier_last3 = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier_last3, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "investigation\n",
      "{'suffix(1)': 'n', 'suffix(2)': 'on', 'suffix(3)': 'ion', 'prev-word': 'an'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN'),\n",
       " (\"Atlanta's\", 'NP$'),\n",
       " ('recent', 'JJ'),\n",
       " ('primary', 'NN'),\n",
       " ('election', 'NN'),\n",
       " ('produced', 'VBD'),\n",
       " ('``', '``'),\n",
       " ('no', 'AT'),\n",
       " ('evidence', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('that', 'CS'),\n",
       " ('any', 'DTI'),\n",
       " ('irregularities', 'NNS'),\n",
       " ('took', 'VBD'),\n",
       " ('place', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a classifier for POS tagging\n",
    "\n",
    "## classify part of speech based on sentence context\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# define features for the \"i\"th word in the sentence, including three types of suffix \n",
    "#     and one pre-word\n",
    "# the pos features function takes the sentence of untagged words and the index of a word i\n",
    "#   it creates features for word i, including the previous word i-1\n",
    "def pos_features(sentence, i):    \n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "\t\t    \"suffix(2)\": sentence[i][-2:],\n",
    "\t\t    \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features \n",
    "\n",
    "# look at features of a specific word in a specific sentence\n",
    "# first sentence of brown corpus\n",
    "sentence0 = brown.sents()[0]\n",
    "print(sentence0)\n",
    "# word 8 of sentence 0\n",
    "print(sentence0[8])\n",
    "\n",
    "# pos features of the word 8 \n",
    "print(pos_features(sentence0, 8))\n",
    "\n",
    "# get the POS tagged sentences with categories of news\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "tag_sent0 = tagged_sents[0]\n",
    "tag_sent0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 The AT\n",
      "1 Fulton NP-TL\n",
      "2 County NN-TL\n",
      "3 Grand JJ-TL\n",
      "4 Jury NN-TL\n",
      "5 said VBD\n",
      "6 Friday NR\n",
      "7 an AT\n",
      "8 investigation NN\n",
      "9 of IN\n",
      "10 Atlanta's NP$\n",
      "11 recent JJ\n",
      "12 primary NN\n",
      "13 election NN\n",
      "14 produced VBD\n",
      "15 `` ``\n",
      "16 no AT\n",
      "17 evidence NN\n",
      "18 '' ''\n",
      "19 that CS\n",
      "20 any DTI\n",
      "21 irregularities NNS\n",
      "22 took VBD\n",
      "23 place NN\n",
      "24 . .\n",
      "({'suffix(1)': 'e', 'suffix(2)': 'he', 'suffix(3)': 'The', 'prev-word': '<START>'}, 'AT')\n",
      "({'suffix(1)': 'n', 'suffix(2)': 'on', 'suffix(3)': 'ton', 'prev-word': 'The'}, 'NP-TL')\n",
      "({'suffix(1)': 'y', 'suffix(2)': 'ty', 'suffix(3)': 'nty', 'prev-word': 'Fulton'}, 'NN-TL')\n",
      "({'suffix(1)': 'd', 'suffix(2)': 'nd', 'suffix(3)': 'and', 'prev-word': 'County'}, 'JJ-TL')\n",
      "({'suffix(1)': 'y', 'suffix(2)': 'ry', 'suffix(3)': 'ury', 'prev-word': 'Grand'}, 'NN-TL')\n",
      "({'suffix(1)': 'd', 'suffix(2)': 'id', 'suffix(3)': 'aid', 'prev-word': 'Jury'}, 'VBD')\n",
      "({'suffix(1)': 'y', 'suffix(2)': 'ay', 'suffix(3)': 'day', 'prev-word': 'said'}, 'NR')\n",
      "({'suffix(1)': 'n', 'suffix(2)': 'an', 'suffix(3)': 'an', 'prev-word': 'Friday'}, 'AT')\n",
      "({'suffix(1)': 'n', 'suffix(2)': 'on', 'suffix(3)': 'ion', 'prev-word': 'an'}, 'NN')\n",
      "({'suffix(1)': 'f', 'suffix(2)': 'of', 'suffix(3)': 'of', 'prev-word': 'investigation'}, 'IN')\n"
     ]
    }
   ],
   "source": [
    "# the function nltk.tag.untag will take the tags off\n",
    "nltk.tag.untag(tag_sent0)\n",
    "\n",
    "# the python enumerate function generates an index number for each item in a list\n",
    "for i,(word,tag) in enumerate(tag_sent0):\n",
    "    print (i, word, tag)\n",
    "    \n",
    "# get feature sets of words appearing in the corpus, from untagged sentences.\n",
    "# and then get their tags from corresponding tagged sentence\n",
    "# use the Python function enumerate to pair the index numbers with sentence words \n",
    "#   for the pos features function\n",
    "featuresets = []\n",
    "for tagged_sent in tagged_sents:\n",
    "\tuntagged_sent = nltk.tag.untag(tagged_sent)\n",
    "\tfor i, (word, tag) in enumerate(tagged_sent):\n",
    "\t\tfeaturesets.append( (pos_features(untagged_sent, i), tag) )\n",
    "        \n",
    "# look at the feature sets of the first 10 words\n",
    "for f in featuresets[:10]:\n",
    "\tprint (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90499\n",
      "10055\n",
      "0.7891596220785678\n"
     ]
    }
   ],
   "source": [
    "# using naive Bayesian as classifier\n",
    "# split data into a training set and a test set, using a 90%/10% split\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "\n",
    "# train classifier on the training set\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# evaluate the accuracy (this will take a little while)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "### classify documents based on keywords\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# nltk.download('movie_reviews')\n",
    "\n",
    "import random\n",
    "\n",
    "# movie reviews are labeled either positive or negative (by human annotators)\n",
    "print(movie_reviews.categories())\n",
    "\n",
    "# for each document in movie_reviews, get its words and category (positive/negative)\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['fact', 'that', 'charles', 'bronson', 'represents', 'one', 'of', 'the', 'most', 'important', 'movie', 'icons', 'of', 'the', '1980s', 'represents', 'one', 'of', 'the', 'biggest', 'and', 'almost', 'tragic', 'ironies', 'of', 'that', 'decade', '.', 'tragedy', 'lies', 'in', 'the', 'fact', 'that', 'the', 'icon', 'status', 'was', 'earned', 'less', 'by', 'quality', 'of', 'his', 'work', 'in', 'movies', ',', 'but', 'the', 'quantity', '.', 'most', 'of', 'those', 'movies', 'were', 'produced', 'by', 'cannon', 'group', ',', 'company', 'led', 'by', 'israeli', 'producers', 'menahem', 'golan', 'and', 'yoram', 'globus', '.', 'those', 'two', 'men', 'probably', 'thought', 'that', 'they', 'could', 'be', 'the', 'next', 'roger', 'corman', ',', 'b', '-', 'movie', 'mentors', 'of', 'future', 'hollywood', 'legends', '.', 'unfortunately', ',', 'that', 'didn', \"'\", 't', 'happened', ',', 'and', 'when', 'cannon', 'finally', 'went', 'bankrupt', 'at', 'the', 'end', 'of', 'the', 'decade', ',', 'behind', 'it', 'stood', 'the', 'huge', 'pile', 'of', 'cinematic', 'garbage', ',', 'that', 'would', 'require', 'at', 'least', 'few', 'centuries', 'before', 'it', 'reaches', 'the', 'camp', 'appeal', '.', 'sadly', 'for', 'bronson', ',', 'that', 'garbage', 'also', 'contained', 'numerous', 'movies', 'in', 'which', 'that', 'capable', 'character', 'actor', 'and', 'action', 'hero', 'of', 'the', '1970s', 'tried', 'to', 'raise', 'their', 'worth', 'simply', 'by', 'being', 'the', 'main', 'lead', ',', 'and', 'lowering', 'his', 'own', 'reputation', 'in', 'process', '.', 'on', 'the', 'other', 'hand', ',', 'bronson', 'could', 'take', 'comfort', 'in', 'a', 'fact', 'that', 'those', 'movies', 'were', 'extremely', 'popular', ',', 'especially', 'among', 'the', 'audience', '3', 'or', '4', 'times', 'younger', 'than', 'bronson', 'himself', '.', 'one', 'of', 'such', 'movies', 'that', 'seriously', 'marred', 'bronson', \"'\", 's', 'reputation', 'is', 'death', 'wish', '3', ',', 'third', 'sequel', 'in', 'the', 'series', 'which', 'began', 'with', 'death', 'wish', 'in', '1974', '.', 'in', 'the', 'original', 'movie', ',', 'bronson', 'played', 'paul', 'kersey', ',', 'mild', '-', 'mannered', 'new', 'york', 'architect', 'who', 'turns', 'into', 'deadly', 'street', 'vigilante', 'after', 'his', 'family', 'fell', 'victim', 'to', 'urban', 'violence', '.', 'that', 'movie', 'was', 'far', 'from', 'masterpiece', ';', 'yet', ',', 'in', 'it', 'the', 'director', 'michael', 'winner', 'was', 'skillfully', 'offering', 'the', 'cinematic', 'remedy', 'for', 'very', 'real', 'disease', 'of', 'growing', 'crime', 'rates', 'of', 'the', 'time', '(', 'on', 'the', 'same', 'lines', 'like', 'siegel', 'in', 'dirty', 'harry', ')', '.', 'unfortunately', ',', 'six', 'years', 'later', 'cannon', 'group', 'got', 'rights', 'to', 'the', 'character', 'of', 'paul', 'kersey', 'and', 'began', 'destroying', 'it', 'by', 'pumping', 'out', 'sequels', ';', 'even', 'the', 'presence', 'of', 'its', 'original', 'director', 'didn', \"'\", 't', 'stop', 'the', 'rapid', 'decline', 'of', 'the', 'quality', '.', 'death', 'wish', '3', 'begins', 'when', 'kersey', 'comes', 'to', 'visit', 'an', 'old', 'friend', ',', 'living', 'in', 'the', 'urban', 'wasteland', 'of', 'east', 'new', 'york', ',', 'populated', 'by', 'young', 'criminals', 'and', 'people', 'too', 'old', 'or', 'too', 'poor', 'to', 'move', 'out', '.', 'before', 'the', 'reunion', ',', 'kersey', \"'\", 's', 'friend', 'falls', 'victim', 'to', 'the', 'street', 'gang', 'led', 'by', 'evil', 'fraker', '(', 'played', 'by', 'gavan', 'o', \"'\", 'herlihy', ',', 'probably', 'the', 'only', 'noteworthy', 'role', 'in', 'the', 'film', ')', '.', 'kersey', 'decides', 'to', 'avenge', 'his', 'death', 'and', 'slowly', 'prepares', 'for', 'his', 'crusade', ',', 'while', 'the', 'police', 'inspector', 'shriker', '(', 'ed', 'lauter', ')', ',', 'ants', 'to', 'use', 'him', 'as', 'a', 'secret', 'weapon', 'in', 'his', 'losing', 'war', 'against', 'the', 'urban', 'crime', '.', 'bronson', ',', 'the', 'main', 'asset', 'in', 'this', 'movie', ',', 'plays', 'the', 'character', 'who', 'is', 'nothing', 'more', 'than', 'an', 'efficient', 'killing', 'machine', '.', 'although', 'bronson', \"'\", 's', 'charisma', 'does', 'help', 'in', 'overcoming', 'some', 'implausibilities', '(', 'single', 'man', 'in', 'his', '60s', 'and', 'armed', 'with', 'a', 'single', 'pistol', 'manages', 'to', 'wipe', 'out', 'dozens', 'of', 'opponents', 'with', 'superior', 'firepower', ')', ',', 'the', 'lack', 'of', 'emotions', 'or', 'bronson', \"'\", 's', 'own', 'commitment', 'could', 'be', 'seen', 'in', 'a', 'very', 'few', 'lines', 'spoken', 'in', 'a', 'film', '.', 'the', 'movie', 'authors', 'were', 'somewhat', 'aware', 'of', 'that', 'emotional', 'shallowness', ',', 'so', 'they', 'added', 'romantic', 'interest', 'for', 'their', 'hero', '-', 'public', 'defender', 'played', 'by', 'deborah', 'raffin', 'and', 'conveniently', 'terminated', 'in', 'order', 'to', 'give', 'some', 'more', 'motives', 'for', 'kersey', \"'\", 's', 'crusade', '.', 'on', 'the', 'other', 'hand', ',', 'emotions', 'are', 'much', 'better', 'played', 'by', 'confronting', 'law', '-', 'abiding', ',', 'yet', 'ethnically', 'stereotyped', 'citizens', 'with', 'their', 'daily', 'nemesis', 'of', 'street', 'punks', '-', 'ruthless', 'enough', 'to', 'exercise', 'their', 'reign', 'of', 'terror', 'on', 'the', 'entire', 'city', 'blocks', ',', 'and', 'stupid', 'enough', 'to', 'be', 'killed', 'in', 'droves', 'by', 'kersey', '.', 'unfortunately', ',', 'michael', 'winner', 'doesn', \"'\", 't', 'know', 'how', 'to', 'work', 'out', 'the', 'plot', ',', 'and', 'after', 'torturing', 'the', 'viewers', 'with', 'mostly', 'uninteresting', 'characters', 'and', 'cliched', 'and', 'formulaic', 'situations', ',', 'ends', 'this', 'movie', 'with', 'a', 'bang', '.', 'the', 'big', 'showdown', 'at', 'the', 'end', '-', 'that', 'turns', 'east', 'new', 'york', 'into', 'the', 'sarajevo', '-', 'like', 'battle', 'zone', '-', 'is', 'probably', 'the', 'worst', 'part', 'of', 'the', 'movie', ',', 'because', 'of', 'the', 'poor', 'editing', 'and', 'the', 'cheap', 'sets', 'and', 'props', 'that', 'give', 'away', 'the', 'low', 'budget', '.', 'in', 'short', ',', 'this', 'movie', 'could', 'be', 'recommended', 'only', 'to', 'the', 'most', 'fanatical', 'charles', 'bronson', 'fans', 'or', 'for', 'the', 'people', 'who', 'are', 'already', 'desperate', 'for', '1980s', 'nostalgia', '.', '(', 'special', 'note', 'for', 'trekkies', ':', 'marina', 'sirtis', ',', 'the', 'actress', 'who', 'played', 'counsellor', 'deanna', 'troi', 'in', 'star', 'trek', ':', 'the', 'next', 'generation', 'could', 'be', 'spotted', 'in', 'a', 'small', 'role', 'of', 'portorican', 'wife', ')', '.'], 'neg')\n",
      "39768\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(documents)\n",
    "# look at the first document - consists of a list of all the words in the review\n",
    "# followed by the category\n",
    "print(documents[0])\n",
    "\n",
    "## use words from all documents to define the word vector for features\n",
    "# get all words from all movie_reviews and put into a frequency distribution\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "print(len(all_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'the', '.', 'a', 'and', 'of', 'to', \"'\", 'is', 'in', 's', '\"', 'it', 'that', '-', ')', '(', 'as', 'with', 'for', 'his', 'this', 'film', 'i', 'he', 'but', 'on', 'are', 't', 'by', 'be', 'one', 'movie', 'an', 'who', 'not', 'you', 'from', 'at', 'was', 'have', 'they', 'has', 'her', 'all', '?', 'there', 'like', 'so', 'out', 'about', 'up', 'more', 'what', 'when', 'which', 'or', 'she', 'their', ':', 'some', 'just', 'can', 'if', 'we', 'him', 'into', 'even', 'only', 'than', 'no', 'good', 'time', 'most', 'its', 'will', 'story', 'would', 'been', 'much', 'character', 'also', 'get', 'other', 'do', 'two', 'well', 'them', 'very', 'characters', ';', 'first', '--', 'after', 'see', '!', 'way', 'because', 'make', 'life']\n"
     ]
    }
   ],
   "source": [
    "# get the 2000 most frequently appearing keywords in the corpus\n",
    "word_items = all_words.most_common(2500)\n",
    "word_features = [word for (word, freq) in word_items]   # just the words\n",
    "\n",
    "# look at the first 100 words\n",
    "print(word_features[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features (keywords) of a document\n",
    "# each feature is 'contains(keyword)' and is true or false depending\n",
    "# on whether that keyword is in the document\n",
    "def document_features(document, word_features):\n",
    "\tdocument_words = set(document)\n",
    "\tfeatures = {}\n",
    "\tfor word in word_features:\n",
    "\t\tfeatures['V_%s' % word] = (word in document_words)\n",
    "\treturn features\n",
    "\n",
    "# get features sets for a document, including keyword features and category feature\n",
    "featuresets = [(document_features(d, word_features), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'V_,': True, 'V_the': True, 'V_.': True, 'V_a': True, 'V_and': True, 'V_of': True, 'V_to': True, \"V_'\": True, 'V_is': True, 'V_in': True, 'V_s': True, 'V_\"': False, 'V_it': True, 'V_that': True, 'V_-': True, 'V_)': True, 'V_(': True, 'V_as': True, 'V_with': True, 'V_for': True, 'V_his': True, 'V_this': True, 'V_film': True, 'V_i': False, 'V_he': False, 'V_but': True, 'V_on': True, 'V_are': True, 'V_t': True, 'V_by': True, 'V_be': True, 'V_one': True, 'V_movie': True, 'V_an': True, 'V_who': True, 'V_not': False, 'V_you': False, 'V_from': True, 'V_at': True, 'V_was': True, 'V_have': False, 'V_they': True, 'V_has': False, 'V_her': False, 'V_all': False, 'V_?': False, 'V_there': False, 'V_like': True, 'V_so': True, 'V_out': True, 'V_about': False, 'V_up': False, 'V_more': True, 'V_what': False, 'V_when': True, 'V_which': True, 'V_or': True, 'V_she': False, 'V_their': True, 'V_:': True, 'V_some': True, 'V_just': False, 'V_can': False, 'V_if': False, 'V_we': False, 'V_him': True, 'V_into': True, 'V_even': True, 'V_only': True, 'V_than': True, 'V_no': False, 'V_good': False, 'V_time': True, 'V_most': True, 'V_its': True, 'V_will': False, 'V_story': False, 'V_would': True, 'V_been': False, 'V_much': True, 'V_character': True, 'V_also': True, 'V_get': False, 'V_other': True, 'V_do': False, 'V_two': True, 'V_well': False, 'V_them': False, 'V_very': True, 'V_characters': True, 'V_;': True, 'V_first': False, 'V_--': False, 'V_after': True, 'V_see': False, 'V_!': False, 'V_way': False, 'V_because': True, 'V_make': False, 'V_life': False, 'V_off': False, 'V_too': True, 'V_any': False, 'V_does': True, 'V_really': False, 'V_had': False, 'V_while': True, 'V_films': False, 'V_how': True, 'V_plot': True, 'V_little': False, 'V_where': False, 'V_people': True, 'V_over': False, 'V_could': True, 'V_then': False, 'V_me': False, 'V_scene': False, 'V_man': True, 'V_bad': False, 'V_my': False, 'V_never': False, 'V_being': True, 'V_best': False, 'V_these': False, 'V_don': False, 'V_new': True, 'V_doesn': True, 'V_scenes': False, 'V_many': False, 'V_director': True, 'V_such': True, 'V_know': True, 'V_were': True, 'V_movies': True, 'V_through': False, 'V_here': False, 'V_action': True, 'V_great': False, 'V_re': False, 'V_another': False, 'V_love': False, 'V_go': False, 'V_made': False, 'V_us': False, 'V_big': True, 'V_end': True, 'V_something': False, 'V_back': False, 'V_*': False, 'V_still': False, 'V_world': False, 'V_seems': False, 'V_work': True, 'V_those': True, 'V_makes': False, 'V_now': False, 'V_before': True, 'V_however': False, 'V_between': False, 'V_few': True, 'V_/': False, 'V_down': False, 'V_every': False, 'V_though': False, 'V_better': True, 'V_real': True, 'V_audience': True, 'V_enough': True, 'V_seen': True, 'V_take': True, 'V_around': False, 'V_both': False, 'V_going': False, 'V_year': False, 'V_performance': False, 'V_why': False, 'V_should': False, 'V_role': True, 'V_isn': False, 'V_same': True, 'V_old': True, 'V_gets': False, 'V_your': False, 'V_may': False, 'V_things': False, 'V_think': False, 'V_years': True, 'V_last': False, 'V_comedy': False, 'V_funny': False, 'V_actually': False, 'V_ve': False, 'V_long': False, 'V_look': False, 'V_almost': True, 'V_own': True, 'V_thing': False, 'V_fact': True, 'V_nothing': True, 'V_say': False, 'V_right': False, 'V_john': False, 'V_although': True, 'V_played': True, 'V_find': False, 'V_script': False, 'V_come': False, 'V_ever': False, 'V_cast': False, 'V_since': False, 'V_did': False, 'V_star': True, 'V_plays': True, 'V_young': True, 'V_show': False, 'V_comes': True, 'V_m': False, 'V_part': True, 'V_original': True, 'V_actors': False, 'V_screen': False, 'V_without': False, 'V_again': False, 'V_acting': False, 'V_three': False, 'V_day': False, 'V_each': False, 'V_point': False, 'V_lot': False, 'V_least': True, 'V_takes': False, 'V_guy': False, 'V_quite': False, 'V_himself': True, 'V_away': True, 'V_during': False, 'V_family': True, 'V_effects': False, 'V_course': False, 'V_goes': False, 'V_minutes': False, 'V_interesting': False, 'V_might': False, 'V_far': True, 'V_high': False, 'V_rather': False, 'V_once': False, 'V_must': False, 'V_anything': False, 'V_place': False, 'V_set': False, 'V_yet': True, 'V_watch': False, 'V_d': False, 'V_making': False, 'V_our': False, 'V_wife': True, 'V_hard': False, 'V_always': False, 'V_fun': False, 'V_didn': True, 'V_ll': False, 'V_seem': False, 'V_special': True, 'V_bit': False, 'V_times': True, 'V_trying': False, 'V_hollywood': True, 'V_instead': False, 'V_give': True, 'V_want': False, 'V_picture': False, 'V_kind': False, 'V_american': False, 'V_job': False, 'V_sense': False, 'V_woman': False, 'V_home': False, 'V_having': False, 'V_series': True, 'V_actor': True, 'V_probably': True, 'V_help': True, 'V_half': False, 'V_along': False, 'V_men': True, 'V_everything': False, 'V_pretty': False, 'V_becomes': False, 'V_sure': False, 'V_black': False, 'V_together': False, 'V_dialogue': False, 'V_money': False, 'V_become': False, 'V_gives': False, 'V_given': False, 'V_looking': False, 'V_whole': False, 'V_watching': False, 'V_father': False, 'V_`': False, 'V_feel': False, 'V_everyone': False, 'V_music': False, 'V_wants': False, 'V_sex': False, 'V_less': True, 'V_done': False, 'V_horror': False, 'V_got': True, 'V_death': True, 'V_perhaps': False, 'V_city': True, 'V_next': True, 'V_especially': True, 'V_play': False, 'V_girl': False, 'V_mind': False, 'V_10': False, 'V_moments': False, 'V_looks': False, 'V_completely': False, 'V_2': False, 'V_reason': False, 'V_mother': False, 'V_whose': False, 'V_line': False, 'V_night': False, 'V_human': False, 'V_until': False, 'V_rest': False, 'V_performances': False, 'V_different': False, 'V_evil': True, 'V_small': True, 'V_james': False, 'V_simply': True, 'V_couple': False, 'V_put': False, 'V_let': False, 'V_anyone': False, 'V_ending': False, 'V_case': False, 'V_several': False, 'V_dead': False, 'V_michael': True, 'V_left': False, 'V_thought': True, 'V_school': False, 'V_shows': False, 'V_humor': False, 'V_true': False, 'V_lost': False, 'V_written': False, 'V_itself': False, 'V_friend': True, 'V_entire': True, 'V_getting': False, 'V_town': False, 'V_turns': True, 'V_soon': False, 'V_someone': False, 'V_second': False, 'V_main': True, 'V_stars': False, 'V_found': False, 'V_use': True, 'V_problem': False, 'V_friends': False, 'V_tv': False, 'V_top': False, 'V_name': False, 'V_begins': True, 'V_called': False, 'V_based': False, 'V_comic': False, 'V_david': False, 'V_head': False, 'V_else': False, 'V_idea': False, 'V_either': False, 'V_wrong': False, 'V_unfortunately': True, 'V_later': True, 'V_final': False, 'V_hand': True, 'V_alien': False, 'V_house': False, 'V_group': True, 'V_full': False, 'V_used': False, 'V_tries': False, 'V_often': False, 'V_against': True, 'V_war': True, 'V_sequence': False, 'V_keep': False, 'V_turn': False, 'V_playing': False, 'V_boy': False, 'V_behind': True, 'V_named': False, 'V_certainly': False, 'V_live': False, 'V_believe': False, 'V_under': False, 'V_works': False, 'V_relationship': False, 'V_face': False, 'V_hour': False, 'V_run': False, 'V_style': False, 'V_said': False, 'V_despite': False, 'V_person': False, 'V_finally': True, 'V_shot': False, 'V_book': False, 'V_doing': False, 'V_tell': False, 'V_maybe': False, 'V_nice': False, 'V_son': False, 'V_perfect': False, 'V_side': False, 'V_seeing': False, 'V_able': False, 'V_finds': False, 'V_children': False, 'V_days': False, 'V_past': False, 'V_summer': False, 'V_camera': False, 'V_won': False, 'V_including': False, 'V_mr': False, 'V_kids': False, 'V_lives': False, 'V_directed': False, 'V_moment': False, 'V_game': False, 'V_running': False, 'V_fight': False, 'V_supposed': False, 'V_video': False, 'V_car': False, 'V_matter': False, 'V_kevin': False, 'V_joe': False, 'V_lines': True, 'V_worth': True, 'V_=': False, 'V_daughter': False, 'V_earth': False, 'V_starts': False, 'V_need': False, 'V_entertaining': False, 'V_white': False, 'V_start': False, 'V_writer': False, 'V_dark': False, 'V_short': True, 'V_self': False, 'V_worst': True, 'V_nearly': False, 'V_opening': False, 'V_try': False, 'V_upon': False, 'V_care': False, 'V_early': False, 'V_violence': True, 'V_throughout': False, 'V_team': False, 'V_production': False, 'V_example': False, 'V_beautiful': False, 'V_title': False, 'V_exactly': False, 'V_jack': False, 'V_review': False, 'V_major': False, 'V_drama': False, 'V_&': False, 'V_problems': False, 'V_sequences': False, 'V_obvious': False, 'V_version': False, 'V_screenplay': False, 'V_known': False, 'V_killer': False, 'V_wasn': False, 'V_robert': False, 'V_disney': False, 'V_already': True, 'V_close': False, 'V_classic': False, 'V_others': False, 'V_hit': False, 'V_kill': False, 'V_deep': False, 'V_five': False, 'V_order': True, 'V_act': False, 'V_simple': False, 'V_fine': False, 'V_themselves': False, 'V_heart': False, 'V_roles': False, 'V_jackie': False, 'V_direction': False, 'V_eyes': False, 'V_four': False, 'V_question': False, 'V_sort': False, 'V_sometimes': False, 'V_knows': False, 'V_supporting': False, 'V_coming': False, 'V_voice': False, 'V_women': False, 'V_truly': False, 'V_save': False, 'V_jokes': False, 'V_computer': False, 'V_child': False, 'V_o': True, 'V_boring': False, 'V_tom': False, 'V_level': False, 'V_1': False, 'V_body': False, 'V_guys': False, 'V_genre': False, 'V_brother': False, 'V_strong': False, 'V_stop': True, 'V_room': False, 'V_space': False, 'V_lee': False, 'V_ends': True, 'V_beginning': False, 'V_ship': False, 'V_york': True, 'V_attempt': False, 'V_thriller': False, 'V_scream': False, 'V_peter': False, 'V_aren': False, 'V_husband': False, 'V_fiction': False, 'V_happens': False, 'V_hero': True, 'V_novel': False, 'V_note': True, 'V_hope': False, 'V_king': False, 'V_yes': False, 'V_says': False, 'V_tells': False, 'V_quickly': False, 'V_romantic': True, 'V_dog': False, 'V_oscar': False, 'V_stupid': True, 'V_possible': False, 'V_saw': False, 'V_lead': True, 'V_career': False, 'V_murder': False, 'V_extremely': True, 'V_manages': True, 'V_god': False, 'V_mostly': True, 'V_wonder': False, 'V_particularly': False, 'V_future': True, 'V_fans': True, 'V_sound': False, 'V_worse': False, 'V_piece': False, 'V_involving': False, 'V_de': False, 'V_appears': False, 'V_planet': False, 'V_paul': True, 'V_involved': False, 'V_mean': False, 'V_none': False, 'V_taking': False, 'V_hours': False, 'V_laugh': False, 'V_police': True, 'V_sets': True, 'V_attention': False, 'V_co': False, 'V_hell': False, 'V_eventually': False, 'V_single': True, 'V_fall': False, 'V_falls': True, 'V_material': False, 'V_emotional': True, 'V_power': False, 'V_late': False, 'V_lack': True, 'V_dr': False, 'V_van': False, 'V_result': False, 'V_elements': False, 'V_meet': False, 'V_smith': False, 'V_science': False, 'V_experience': False, 'V_bring': False, 'V_wild': False, 'V_living': True, 'V_theater': False, 'V_interest': True, 'V_leads': False, 'V_word': False, 'V_feature': False, 'V_battle': True, 'V_girls': False, 'V_alone': False, 'V_obviously': False, 'V_george': False, 'V_within': False, 'V_usually': False, 'V_enjoy': False, 'V_guess': False, 'V_among': True, 'V_taken': False, 'V_feeling': False, 'V_laughs': False, 'V_aliens': False, 'V_talk': False, 'V_chance': False, 'V_talent': False, 'V_3': True, 'V_middle': False, 'V_number': False, 'V_easy': False, 'V_across': False, 'V_needs': False, 'V_attempts': False, 'V_happen': False, 'V_television': False, 'V_chris': False, 'V_deal': False, 'V_poor': True, 'V_form': False, 'V_girlfriend': False, 'V_viewer': False, 'V_release': False, 'V_killed': True, 'V_forced': False, 'V_whether': False, 'V_wonderful': False, 'V_feels': False, 'V_oh': False, 'V_tale': False, 'V_serious': False, 'V_expect': False, 'V_except': False, 'V_light': False, 'V_success': False, 'V_features': False, 'V_premise': False, 'V_happy': False, 'V_words': False, 'V_leave': False, 'V_important': True, 'V_meets': False, 'V_history': False, 'V_giving': False, 'V_crew': False, 'V_type': False, 'V_call': False, 'V_turned': False, 'V_released': False, 'V_parents': False, 'V_art': False, 'V_impressive': False, 'V_mission': False, 'V_working': False, 'V_seemed': False, 'V_score': False, 'V_told': False, 'V_recent': False, 'V_robin': False, 'V_basically': False, 'V_entertainment': False, 'V_america': False, 'V_$': False, 'V_surprise': False, 'V_apparently': False, 'V_easily': False, 'V_ryan': False, 'V_cool': False, 'V_stuff': False, 'V_cop': False, 'V_change': False, 'V_williams': False, 'V_crime': True, 'V_office': False, 'V_parts': False, 'V_somehow': False, 'V_sequel': True, 'V_william': False, 'V_cut': False, 'V_die': False, 'V_jones': False, 'V_credits': False, 'V_batman': False, 'V_suspense': False, 'V_brings': False, 'V_events': False, 'V_reality': False, 'V_whom': False, 'V_local': False, 'V_talking': False, 'V_difficult': False, 'V_using': False, 'V_went': True, 'V_writing': False, 'V_remember': False, 'V_near': False, 'V_straight': False, 'V_hilarious': False, 'V_ago': False, 'V_certain': False, 'V_ben': False, 'V_kid': False, 'V_wouldn': False, 'V_slow': False, 'V_blood': False, 'V_mystery': False, 'V_complete': False, 'V_red': False, 'V_popular': True, 'V_effective': False, 'V_am': False, 'V_fast': False, 'V_flick': False, 'V_due': False, 'V_runs': False, 'V_gone': False, 'V_return': False, 'V_presence': True, 'V_quality': True, 'V_dramatic': False, 'V_filmmakers': False, 'V_age': False, 'V_brothers': False, 'V_business': False, 'V_general': False, 'V_rock': False, 'V_sexual': False, 'V_present': False, 'V_surprisingly': False, 'V_anyway': False, 'V_uses': False, 'V_4': True, 'V_personal': False, 'V_figure': False, 'V_smart': False, 'V_ways': False, 'V_decides': True, 'V_annoying': False, 'V_begin': False, 'V_couldn': False, 'V_somewhat': True, 'V_shots': False, 'V_rich': False, 'V_minute': False, 'V_law': True, 'V_previous': False, 'V_jim': False, 'V_successful': False, 'V_harry': True, 'V_water': False, 'V_similar': False, 'V_absolutely': False, 'V_motion': False, 'V_former': False, 'V_strange': False, 'V_came': False, 'V_follow': False, 'V_read': False, 'V_project': False, 'V_million': False, 'V_secret': True, 'V_starring': False, 'V_clear': False, 'V_familiar': False, 'V_romance': False, 'V_intelligent': False, 'V_third': True, 'V_excellent': False, 'V_amazing': False, 'V_party': False, 'V_budget': True, 'V_eye': False, 'V_actress': True, 'V_prison': False, 'V_latest': False, 'V_means': False, 'V_company': True, 'V_towards': False, 'V_predictable': False, 'V_powerful': False, 'V_nor': False, 'V_bob': False, 'V_beyond': False, 'V_visual': False, 'V_leaves': False, 'V_r': False, 'V_nature': False, 'V_following': False, 'V_villain': False, 'V_leaving': False, 'V_animated': False, 'V_low': True, 'V_myself': False, 'V_b': True, 'V_bill': False, 'V_sam': False, 'V_filled': False, 'V_wars': False, 'V_questions': False, 'V_cinema': False, 'V_message': False, 'V_box': False, 'V_moving': False, 'V_herself': False, 'V_country': False, 'V_usual': False, 'V_martin': False, 'V_definitely': False, 'V_add': False, 'V_large': False, 'V_clever': False, 'V_create': False, 'V_felt': False, 'V_stories': False, 'V_brilliant': False, 'V_ones': False, 'V_giant': False, 'V_situation': False, 'V_murphy': False, 'V_break': False, 'V_opens': False, 'V_scary': False, 'V_doubt': False, 'V_drug': False, 'V_bunch': False, 'V_thinking': False, 'V_solid': False, 'V_effect': False, 'V_learn': False, 'V_move': True, 'V_force': False, 'V_potential': False, 'V_seriously': True, 'V_follows': False, 'V_above': False, 'V_saying': False, 'V_huge': True, 'V_class': False, 'V_plan': False, 'V_agent': False, 'V_created': False, 'V_unlike': False, 'V_pay': False, 'V_non': False, 'V_married': False, 'V_mark': False, 'V_sweet': False, 'V_perfectly': False, 'V_ex': False, 'V_realize': False, 'V_audiences': False, 'V_took': False, 'V_decent': False, 'V_likely': False, 'V_dream': False, 'V_view': False, 'V_scott': False, 'V_subject': False, 'V_understand': False, 'V_happened': True, 'V_enjoyable': False, 'V_studio': False, 'V_immediately': False, 'V_open': False, 'V_e': False, 'V_points': False, 'V_heard': False, 'V_viewers': True, 'V_cameron': False, 'V_truman': False, 'V_bruce': False, 'V_frank': False, 'V_private': False, 'V_stay': False, 'V_fails': False, 'V_impossible': False, 'V_cold': False, 'V_richard': False, 'V_overall': False, 'V_merely': False, 'V_exciting': False, 'V_mess': False, 'V_chase': False, 'V_free': False, 'V_ten': False, 'V_neither': False, 'V_wanted': False, 'V_gun': False, 'V_appear': False, 'V_carter': False, 'V_escape': False, 'V_ultimately': False, 'V_+': False, 'V_fan': False, 'V_inside': False, 'V_favorite': False, 'V_haven': False, 'V_modern': False, 'V_l': False, 'V_wedding': False, 'V_stone': False, 'V_trek': True, 'V_brought': False, 'V_trouble': False, 'V_otherwise': False, 'V_tim': False, 'V_5': False, 'V_allen': False, 'V_bond': False, 'V_society': False, 'V_liked': False, 'V_dumb': False, 'V_musical': False, 'V_stand': False, 'V_political': False, 'V_various': False, 'V_talented': False, 'V_particular': False, 'V_west': False, 'V_state': False, 'V_keeps': False, 'V_english': False, 'V_silly': False, 'V_u': False, 'V_situations': True, 'V_park': False, 'V_teen': False, 'V_rating': False, 'V_slightly': False, 'V_steve': False, 'V_truth': False, 'V_air': False, 'V_element': False, 'V_joke': False, 'V_spend': False, 'V_key': False, 'V_biggest': True, 'V_members': False, 'V_effort': False, 'V_government': False, 'V_focus': False, 'V_eddie': False, 'V_soundtrack': False, 'V_hands': False, 'V_earlier': False, 'V_chan': False, 'V_purpose': False, 'V_today': False, 'V_showing': False, 'V_memorable': False, 'V_six': True, 'V_cannot': False, 'V_max': False, 'V_offers': False, 'V_rated': False, 'V_mars': False, 'V_heavy': False, 'V_totally': False, 'V_control': False, 'V_credit': False, 'V_fi': False, 'V_woody': False, 'V_ideas': False, 'V_sci': False, 'V_wait': False, 'V_sit': False, 'V_female': False, 'V_ask': False, 'V_waste': False, 'V_terrible': False, 'V_depth': False, 'V_simon': False, 'V_aspect': False, 'V_list': False, 'V_mary': False, 'V_sister': False, 'V_animation': False, 'V_entirely': False, 'V_fear': False, 'V_steven': False, 'V_moves': False, 'V_actual': False, 'V_army': False, 'V_british': False, 'V_constantly': False, 'V_fire': False, 'V_convincing': False, 'V_setting': False, 'V_gave': False, 'V_tension': False, 'V_street': True, 'V_8': False, 'V_brief': False, 'V_ridiculous': False, 'V_cinematography': False, 'V_typical': False, 'V_nick': False, 'V_screenwriter': False, 'V_ability': False, 'V_spent': False, 'V_quick': False, 'V_violent': False, 'V_atmosphere': False, 'V_subtle': False, 'V_expected': False, 'V_fairly': False, 'V_seven': False, 'V_killing': True, 'V_tone': False, 'V_master': False, 'V_disaster': False, 'V_lots': False, 'V_thinks': False, 'V_song': False, 'V_cheap': True, 'V_suddenly': False, 'V_background': False, 'V_club': False, 'V_willis': False, 'V_whatever': False, 'V_highly': False, 'V_sees': False, 'V_complex': False, 'V_greatest': False, 'V_impact': False, 'V_beauty': False, 'V_front': False, 'V_humans': False, 'V_indeed': False, 'V_flat': False, 'V_grace': False, 'V_wrote': False, 'V_amusing': False, 'V_ii': False, 'V_mike': False, 'V_further': False, 'V_cute': False, 'V_dull': False, 'V_minor': False, 'V_recently': False, 'V_hate': False, 'V_outside': False, 'V_plenty': False, 'V_wish': True, 'V_godzilla': False, 'V_college': False, 'V_titanic': False, 'V_sounds': False, 'V_telling': False, 'V_sight': False, 'V_double': False, 'V_cinematic': True, 'V_queen': False, 'V_hold': False, 'V_meanwhile': False, 'V_awful': False, 'V_clearly': False, 'V_theme': False, 'V_hear': False, 'V_x': False, 'V_amount': False, 'V_baby': False, 'V_approach': False, 'V_dreams': False, 'V_shown': False, 'V_island': False, 'V_reasons': False, 'V_charm': False, 'V_miss': False, 'V_longer': False, 'V_common': False, 'V_sean': False, 'V_carry': False, 'V_believable': False, 'V_realistic': False, 'V_chemistry': False, 'V_possibly': False, 'V_casting': False, 'V_carrey': False, 'V_french': False, 'V_trailer': False, 'V_tough': False, 'V_produced': True, 'V_imagine': False, 'V_choice': False, 'V_ride': False, 'V_somewhere': False, 'V_hot': False, 'V_race': False, 'V_road': False, 'V_leader': False, 'V_thin': False, 'V_jerry': False, 'V_slowly': True, 'V_delivers': False, 'V_detective': False, 'V_brown': False, 'V_jackson': False, 'V_member': False, 'V_provide': False, 'V_president': False, 'V_puts': False, 'V_asks': False, 'V_critics': False, 'V_appearance': False, 'V_famous': False, 'V_okay': False, 'V_intelligence': False, 'V_energy': False, 'V_sent': False, 'V_spielberg': False, 'V_development': False, 'V_etc': False, 'V_language': False, 'V_blue': False, 'V_proves': False, 'V_vampire': False, 'V_seemingly': False, 'V_basic': False, 'V_caught': False, 'V_decide': False, 'V_opportunity': False, 'V_incredibly': False, 'V_images': False, 'V_band': False, 'V_j': False, 'V_writers': False, 'V_knew': False, 'V_interested': False, 'V_considering': False, 'V_boys': False, 'V_thanks': False, 'V_remains': False, 'V_climax': False, 'V_event': False, 'V_directing': False, 'V_conclusion': False, 'V_leading': False, 'V_ground': False, 'V_lies': True, 'V_forget': False, 'V_alive': False, 'V_tarzan': False, 'V_century': False, 'V_provides': False, 'V_trip': False, 'V_partner': False, 'V_central': False, 'V_tarantino': False, 'V_period': False, 'V_pace': False, 'V_yourself': False, 'V_worked': False, 'V_ready': False, 'V_date': False, 'V_thus': False, 'V_1998': False, 'V_terrific': False, 'V_write': False, 'V_average': False, 'V_onto': False, 'V_songs': False, 'V_occasionally': False, 'V_doctor': False, 'V_stands': False, 'V_hardly': False, 'V_monster': False, 'V_led': True, 'V_mysterious': False, 'V_details': False, 'V_wasted': False, 'V_apart': False, 'V_aside': False, 'V_store': False, 'V_billy': False, 'V_boss': False, 'V_travolta': False, 'V_producer': False, 'V_pull': False, 'V_consider': False, 'V_pictures': False, 'V_becoming': False, 'V_cage': False, 'V_loud': False, 'V_looked': False, 'V_officer': False, 'V_twenty': False, 'V_system': False, 'V_contains': False, 'V_julia': False, 'V_subplot': False, 'V_missing': False, 'V_personality': False, 'V_building': False, 'V_learns': False, 'V_hong': False, 'V_la': False, 'V_apartment': False, 'V_7': False, 'V_bizarre': False, 'V_powers': False, 'V_flaws': False, 'V_catch': False, 'V_lawyer': False, 'V_shoot': False, 'V_student': False, 'V_unique': False, 'V_000': False, 'V_admit': False, 'V_concept': False, 'V_needed': False, 'V_thrown': False, 'V_christopher': False, 'V_laughing': False, 'V_green': False, 'V_twists': False, 'V_matthew': False, 'V_touch': False, 'V_waiting': False, 'V_victim': True, 'V_cover': False, 'V_machine': True, 'V_danny': False, 'V_mention': False, 'V_search': False, 'V_1997': False, 'V_win': False, 'V_door': False, 'V_manner': False, 'V_train': False, 'V_saving': False, 'V_share': False, 'V_image': False, 'V_discovers': False, 'V_normal': False, 'V_cross': False, 'V_fox': False, 'V_returns': False, 'V_adult': False, 'V_adds': False, 'V_answer': False, 'V_adventure': False, 'V_lame': False, 'V_male': False, 'V_odd': False, 'V_singer': False, 'V_deserves': False, 'V_gore': False, 'V_states': False, 'V_include': False, 'V_equally': False, 'V_months': False, 'V_barely': False, 'V_directors': False, 'V_introduced': False, 'V_fashion': False, 'V_social': False, 'V_1999': False, 'V_news': False, 'V_hair': False, 'V_dance': False, 'V_innocent': False, 'V_camp': True, 'V_teacher': False, 'V_became': False, 'V_sad': False, 'V_witch': False, 'V_includes': False, 'V_nights': False, 'V_jason': False, 'V_julie': False, 'V_latter': False, 'V_food': False, 'V_jennifer': False, 'V_land': False, 'V_menace': False, 'V_rate': False, 'V_storyline': False, 'V_contact': False, 'V_jean': False, 'V_elizabeth': False, 'V_fellow': False, 'V_changes': False, 'V_henry': False, 'V_hill': False, 'V_pulp': False, 'V_gay': False, 'V_tried': True, 'V_surprised': False, 'V_literally': False, 'V_walk': False, 'V_standard': False, 'V_90': False, 'V_forward': False, 'V_wise': False, 'V_enjoyed': False, 'V_discover': False, 'V_pop': False, 'V_anderson': False, 'V_offer': False, 'V_recommend': False, 'V_public': True, 'V_drive': False, 'V_c': False, 'V_toy': False, 'V_charming': False, 'V_fair': False, 'V_chinese': False, 'V_rescue': False, 'V_terms': False, 'V_mouth': False, 'V_lucas': False, 'V_accident': False, 'V_dies': False, 'V_decided': False, 'V_edge': False, 'V_footage': False, 'V_culture': False, 'V_weak': False, 'V_presented': False, 'V_blade': False, 'V_younger': True, 'V_douglas': False, 'V_natural': False, 'V_born': False, 'V_generally': False, 'V_teenage': False, 'V_older': False, 'V_horrible': False, 'V_addition': False, 'V_sadly': True, 'V_creates': False, 'V_disturbing': False, 'V_roger': True, 'V_detail': False, 'V_devil': False, 'V_debut': False, 'V_track': False, 'V_developed': False, 'V_week': False, 'V_russell': False, 'V_attack': False, 'V_explain': False, 'V_rarely': False, 'V_fully': False, 'V_prove': False, 'V_exception': False, 'V_jeff': False, 'V_twist': False, 'V_gang': True, 'V_winning': False, 'V_jr': False, 'V_species': False, 'V_issues': False, 'V_fresh': False, 'V_rules': False, 'V_meaning': False, 'V_inspired': False, 'V_heroes': False, 'V_desperate': True, 'V_fighting': False, 'V_filmed': False, 'V_faces': False, 'V_alan': False, 'V_bright': False, 'V_ass': False, 'V_flying': False, 'V_kong': False, 'V_rush': False, 'V_forces': False, 'V_charles': True, 'V_numerous': True, 'V_emotions': True, 'V_involves': False, 'V_patrick': False, 'V_weird': False, 'V_apparent': False, 'V_information': False, 'V_revenge': False, 'V_jay': False, 'V_toward': False, 'V_surprising': False, 'V_twice': False, 'V_editing': True, 'V_calls': False, 'V_lose': False, 'V_vegas': False, 'V_stage': False, 'V_intended': False, 'V_gags': False, 'V_opinion': False, 'V_likes': False, 'V_crazy': False, 'V_owner': False, 'V_places': False, 'V_pair': False, 'V_genuine': False, 'V_epic': False, 'V_speak': False, 'V_throw': False, 'V_appeal': True, 'V_gibson': False, 'V_captain': False, 'V_military': False, 'V_20': False, 'V_blair': False, 'V_nowhere': False, 'V_length': False, 'V_nicely': False, 'V_cause': False, 'V_pass': False, 'V_episode': False, 'V_kiss': False, 'V_arnold': False, 'V_please': False, 'V_hasn': False, 'V_phone': False, 'V_filmmaking': False, 'V_formula': False, 'V_boyfriend': False, 'V_talents': False, 'V_creating': False, 'V_kelly': False, 'V_buy': False, 'V_wide': False, 'V_fantasy': False, 'V_mood': False, 'V_heads': False, 'V_pathetic': False, 'V_lacks': False, 'V_loved': False, 'V_asked': False, 'V_mrs': False, 'V_witty': False, 'V_shakespeare': False, 'V_mulan': False, 'V_generation': True, 'V_affair': False, 'V_pieces': False, 'V_task': False, 'V_rare': False, 'V_kept': False, 'V_cameo': False, 'V_fascinating': False, 'V_ed': True, 'V_fbi': False, 'V_burton': False, 'V_incredible': False, 'V_accent': False, 'V_artist': False, 'V_superior': True, 'V_academy': False, 'V_thomas': False, 'V_spirit': False, 'V_technical': False, 'V_confusing': False, 'V_poorly': False, 'V_target': False, 'V_lover': False, 'V_woo': False, 'V_mentioned': False, 'V_theaters': False, 'V_plane': False, 'V_confused': False, 'V_dennis': False, 'V_rob': False, 'V_appropriate': False, 'V_christmas': False, 'V_considered': False, 'V_legend': False, 'V_shame': False, 'V_soul': False, 'V_matt': False, 'V_campbell': False, 'V_process': True, 'V_bottom': False, 'V_sitting': False, 'V_brain': False, 'V_creepy': False, 'V_13': False, 'V_forever': False, 'V_dude': False, 'V_crap': False, 'V_superb': False, 'V_speech': False, 'V_ice': False, 'V_journey': False, 'V_masterpiece': True, 'V_intriguing': False, 'V_names': False, 'V_pick': False, 'V_speaking': False, 'V_virtually': False, 'V_award': False, 'V_worthy': False, 'V_marriage': False, 'V_deliver': False, 'V_cash': False, 'V_magic': False, 'V_respect': False, 'V_product': False, 'V_necessary': False, 'V_suppose': False, 'V_silent': False, 'V_pointless': False, 'V_station': False, 'V_affleck': False, 'V_dimensional': False, 'V_charlie': False, 'V_allows': False, 'V_avoid': False, 'V_meant': False, 'V_cops': False, 'V_attitude': False, 'V_relationships': False, 'V_hits': False, 'V_stephen': False, 'V_spends': False, 'V_relief': False, 'V_physical': False, 'V_count': False, 'V_reviews': False, 'V_appreciate': False, 'V_cliches': False, 'V_holds': False, 'V_pure': False, 'V_plans': False, 'V_limited': False, 'V_failed': False, 'V_pain': False, 'V_impression': False, 'V_unless': False, 'V_sub': False, 'V_[': False, 'V_total': False, 'V_creature': False, 'V_viewing': False, 'V_loves': False, 'V_princess': False, 'V_kate': False, 'V_rising': False, 'V_woods': False, 'V_baldwin': False, 'V_angry': False, 'V_drawn': False, 'V_step': False, 'V_matrix': False, 'V_themes': False, 'V_satire': False, 'V_arts': False, 'V_]': False, 'V_remake': False, 'V_wall': False, 'V_moral': False, 'V_color': False, 'V_ray': False, 'V_stuck': False, 'V_touching': False, 'V_wit': False, 'V_tony': False, 'V_hanks': False, 'V_continues': False, 'V_damn': False, 'V_nobody': False, 'V_cartoon': False, 'V_keeping': False, 'V_realized': False, 'V_criminal': False, 'V_unfunny': False, 'V_comedic': False, 'V_martial': False, 'V_disappointing': False, 'V_anti': False, 'V_graphic': False, 'V_stunning': False, 'V_actions': False, 'V_floor': False, 'V_emotion': False, 'V_soldiers': False, 'V_edward': False, 'V_comedies': False, 'V_driver': False, 'V_expectations': False, 'V_added': True, 'V_mad': False, 'V_angels': False, 'V_shallow': False, 'V_suspect': False, 'V_humorous': False, 'V_phantom': False, 'V_appealing': False, 'V_device': False, 'V_design': False, 'V_industry': False, 'V_reach': False, 'V_fat': False, 'V_blame': False, 'V_united': False, 'V_sign': False, 'V_portrayal': False, 'V_rocky': False, 'V_finale': False, 'V_grand': False, 'V_opposite': False, 'V_hotel': False, 'V_match': False, 'V_damme': False, 'V_speed': False, 'V_ok': False, 'V_loving': False, 'V_field': False, 'V_larry': False, 'V_urban': True, 'V_troopers': False, 'V_compared': False, 'V_apes': False, 'V_rose': False, 'V_falling': False, 'V_era': False, 'V_loses': False, 'V_adults': False, 'V_managed': False, 'V_dad': False, 'V_therefore': False, 'V_pg': False, 'V_results': False, 'V_guns': False, 'V_radio': False, 'V_lady': False, 'V_manage': False, 'V_spice': False, 'V_naked': False, 'V_started': False, 'V_intense': False, 'V_humanity': False, 'V_wonderfully': False, 'V_slasher': False, 'V_bland': False, 'V_imagination': False, 'V_walking': False, 'V_willing': False, 'V_horse': False, 'V_rent': False, 'V_mix': False, 'V_generated': False, 'V_g': False, 'V_utterly': False, 'V_scientist': False, 'V_washington': False, 'V_notice': False, 'V_players': False, 'V_teenagers': False, 'V_moore': False, 'V_board': False, 'V_price': False, 'V_frightening': False, 'V_tommy': False, 'V_spectacular': False, 'V_bored': False, 'V_jane': False, 'V_join': False, 'V_producers': True, 'V_johnny': False, 'V_zero': False, 'V_vampires': False, 'V_adaptation': False, 'V_dollars': False, 'V_parody': False, 'V_documentary': False, 'V_dvd': False, 'V_wayne': False, 'V_post': False, 'V_exist': False, 'V_matters': False, 'V_chosen': False, 'V_mel': False, 'V_attractive': False, 'V_plain': False, 'V_trust': False, 'V_safe': False, 'V_reading': False, 'V_hoping': False, 'V_protagonist': False, 'V_feelings': False, 'V_fate': False, 'V_finding': False, 'V_feet': False, 'V_visuals': False, 'V_spawn': False, 'V_compelling': False, 'V_hall': False, 'V_sympathetic': False, 'V_featuring': False, 'V_difference': False, 'V_professional': False, 'V_drugs': False, 'V_ford': False, 'V_shooting': False, 'V_gold': False, 'V_patch': False, 'V_build': False, 'V_boat': False, 'V_cruise': False, 'V_honest': False, 'V_media': False, 'V_flicks': False, 'V_bug': False, 'V_bringing': False, 'V_dangerous': False, 'V_watched': False, 'V_grant': False, 'V_smile': False, 'V_plus': False, 'V_shouldn': False, 'V_decision': False, 'V_visually': False, 'V_allow': False, 'V_starship': False, 'V_roberts': False, 'V_dying': False, 'V_portrayed': False, 'V_turning': False, 'V_believes': False, 'V_changed': False, 'V_shock': False, 'V_destroy': False, 'V_30': False, 'V_crowd': False, 'V_broken': False, 'V_tired': False, 'V_fail': False, 'V_south': False, 'V_died': False, 'V_cult': False, 'V_fake': False, 'V_vincent': False, 'V_identity': False, 'V_sexy': False, 'V_hunt': False, 'V_jedi': False, 'V_flynt': False, 'V_alex': False, 'V_engaging': False, 'V_serve': False, 'V_snake': False, 'V_yeah': False, 'V_expecting': False, 'V_100': False, 'V_decade': True, 'V_ups': False, 'V_constant': False, 'V_current': False, 'V_survive': False, 'V_jimmy': False, 'V_buddy': False, 'V_send': False, 'V_brooks': False, 'V_goofy': False, 'V_likable': False, 'V_humour': False, 'V_technology': False, 'V_files': False, 'V_babe': False, 'V_aspects': False, 'V_presents': False, 'V_kills': False, 'V_supposedly': False, 'V_eight': False, 'V_sandler': False, 'V_hospital': False, 'V_test': False, 'V_hidden': False, 'V_brian': False, 'V_books': False, 'V_promise': False, 'V_determined': False, 'V_professor': False, 'V_welcome': False, 'V_pleasure': False, 'V_succeeds': False, 'V_individual': False, 'V_annie': False, 'V_mob': False, 'V_ted': False, 'V_virus': False, 'V_content': False, 'V_gary': False, 'V_direct': False, 'V_contrived': False, 'V_carpenter': False, 'V_scale': False, 'V_sick': False, 'V_nasty': False, 'V_conflict': False, 'V_haunting': False, 'V_ghost': False, 'V_filmmaker': False, 'V_japanese': False, 'V_helps': False, 'V_fare': False, 'V_lucky': False, 'V_ultimate': False, 'V_window': False, 'V_support': False, 'V_goal': False, 'V_provided': False, 'V_genius': False, 'V_winner': True, 'V_taylor': False, 'V_fantastic': False, 'V_faith': False, 'V_lynch': False, 'V_fit': False, 'V_catherine': False, 'V_ms': False, 'V_paced': False, 'V_breaks': False, 'V_al': False, 'V_frame': False, 'V_travel': False, 'V_badly': False, 'V_available': False, 'V_cares': False, 'V_reeves': False, 'V_crash': False, 'V_driving': False, 'V_press': False, 'V_seagal': False, 'V_amy': False, 'V_9': False, 'V_headed': False, 'V_instance': False, 'V_excuse': False, 'V_offensive': False, 'V_narrative': False, 'V_fault': False, 'V_bus': False, 'V_f': False, 'V_extreme': False, 'V_miller': False, 'V_guilty': False, 'V_grows': False, 'V_overly': False, 'V_liners': False, 'V_forgotten': False, 'V_ahead': False, 'V_accept': False, 'V_porn': False, 'V_directly': False, 'V_helen': False, 'V_began': True, 'V_lord': False, 'V_folks': False, 'V_mediocre': False, 'V_bar': False, 'V_surface': False, 'V_super': False, 'V_failure': False, 'V_6': False, 'V_acted': False, 'V_quiet': False, 'V_laughable': False, 'V_sheer': False, 'V_security': False, 'V_emotionally': False, 'V_season': False, 'V_stuart': False, 'V_jail': False, 'V_deals': False, 'V_cheesy': False, 'V_court': False, 'V_beach': False, 'V_austin': False, 'V_model': False, 'V_outstanding': False, 'V_substance': False, 'V_nudity': False, 'V_slapstick': False, 'V_joan': False, 'V_reveal': False, 'V_placed': False, 'V_check': False, 'V_beast': False, 'V_hurt': False, 'V_bloody': False, 'V_acts': False, 'V_fame': False, 'V_meeting': False, 'V_nuclear': False, 'V_1996': False, 'V_strength': False, 'V_center': False, 'V_funniest': False, 'V_standing': False, 'V_damon': False, 'V_clich': False, 'V_position': False, 'V_desire': False, 'V_driven': False, 'V_seat': False, 'V_stock': False, 'V_wondering': False, 'V_realizes': False, 'V_dealing': False, 'V_taste': False, 'V_routine': False, 'V_comparison': False, 'V_cinematographer': False, 'V_seconds': False, 'V_singing': False, 'V_gangster': False, 'V_responsible': False, 'V_football': False, 'V_remarkable': False, 'V_hunting': False, 'V_adams': False, 'V_fly': False, 'V_suspects': False, 'V_treat': False, 'V_hopes': False, 'V_heaven': False, 'V_myers': False, 'V_struggle': False, 'V_costumes': False, 'V_beat': False, 'V_happening': False, 'V_skills': False, 'V_ugly': False, 'V_figures': False, 'V_thoroughly': False, 'V_ill': False, 'V_surprises': False, 'V_player': False, 'V_rival': False, 'V_guard': False, 'V_anthony': False, 'V_strike': False, 'V_community': False, 'V_streets': False, 'V_hopkins': False, 'V_ended': False, 'V_originally': False, 'V_sarah': False, 'V_creative': False, 'V_characterization': False, 'V_thankfully': False, 'V_growing': True, 'V_sharp': False, 'V_williamson': False, 'V_eccentric': False, 'V_explained': False, 'V_hey': False, 'V_claire': False, 'V_steal': False, 'V_inevitable': False, 'V_joel': False, 'V_core': False, 'V_weren': False, 'V_sorry': False, 'V_built': False, 'V_anne': False, 'V_breaking': False, 'V_villains': False, 'V_critic': False, 'V_lets': False, 'V_visit': True, 'V_followed': False, 'V_serial': False, 'V_value': False, 'V_missed': False, 'V_oliver': False, 'V_hollow': False, 'V_sea': False, 'V_animal': False, 'V_freeman': False, 'V_animals': False, 'V_crystal': False, 'V_sidney': False, 'V_lacking': False, 'V_students': False, 'V_continue': False, 'V_extra': False, 'V_scorsese': False, 'V_church': False, 'V_stick': False, 'V_explanation': False, 'V_below': False, 'V_hip': False, 'V_quest': False, 'V_mistake': False, 'V_jump': False, 'V_fights': False, 'V_cusack': False, 'V_included': False, 'V_draw': False, 'V_15': False, 'V_games': False, 'V_1995': False, 'V_judge': False, 'V_gotten': False, 'V_chief': False, 'V_derek': False, 'V_thirty': False, 'V_record': False, 'V_everybody': False, 'V_veteran': False, 'V_develop': False, 'V_knowledge': False, 'V_serves': False, 'V_boogie': False, 'V_arrives': False, 'V_clooney': False, 'V_enter': False, 'V_russian': False, 'V_obsessed': False, 'V_vision': False, 'V_screenwriters': False, 'V_luck': False, 'V_holes': False, 'V_religious': False, 'V_witness': False, 'V_flashbacks': False, 'V_heavily': False, 'V_frequently': False, 'V_capable': True, 'V_armageddon': False, 'V_pacing': False, 'V_rise': False, 'V_mainly': False, 'V_fill': False, 'V_barry': False, 'V_schwarzenegger': False, 'V_clean': False, 'V_previously': False, 'V_grow': False, 'V_keaton': False, 'V_empty': False, 'V_synopsis': False, 'V_victims': False, 'V_adam': False, 'V_bed': False, 'V_lawrence': False, 'V_stallone': False, 'V_hunter': False, 'V_memory': False, 'V_suit': False, 'V_bobby': False, 'V_tragedy': True, 'V_saved': False, 'V_spot': False, 'V_unexpected': False, 'V_encounter': False, 'V_hearted': False, 'V_bacon': False, 'V_disappointment': False, 'V_bigger': False, 'V_noir': False, 'V_nicholson': False, 'V_evidence': False, 'V_relatively': False, 'V_morning': False, 'V_andrew': False, 'V_range': False, 'V_numbers': False, 'V_walter': False, 'V_vehicle': False, 'V_pulled': False, 'V_describe': False, 'V_cliched': True, 'V_sky': False, 'V_efforts': False, 'V_logic': False, 'V_verhoeven': False, 'V_assistant': False, 'V_existence': False, 'V_worker': False, 'V_freedom': False, 'V_theatre': False, 'V_wood': False, 'V_warm': False, 'V_fish': False, 'V_ripley': False, 'V_mental': False, 'V_study': False, 'V_justice': False, 'V_cliche': False, 'V_foot': False, 'V_jonathan': False, 'V_grown': False, 'V_unnecessary': False, 'V_rip': False, 'V_learned': False, 'V_skin': False, 'V_talks': False, 'V_ball': False, 'V_alice': False, 'V_roll': False, 'V_weeks': False, 'V_jon': False, 'V_courtroom': False, 'V_positive': False, 'V_putting': False, 'V_connection': False, 'V_london': False, 'V_angel': False, 'V_contrast': False, 'V_exact': False, 'V_fifteen': False, 'V_eric': False, 'V_prince': False, 'V_bound': False, 'V_traditional': False, 'V_regular': False, 'V_eve': False, 'V_niro': False, 'V_las': False, 'V_remain': False, 'V_anna': False, 'V_moved': False, 'V_asking': False, 'V_genuinely': False, 'V_rain': False, 'V_path': False, 'V_aware': True, 'V_causes': False, 'V_international': False, 'V_naturally': False, 'V_bank': False, 'V_faced': False, 'V_elaborate': False, 'V_shocking': False, 'V_besides': False, 'V_trash': False, 'V_paris': False, 'V_captured': False, 'V_concerned': False, 'V_rule': False, 'V_held': False, 'V_claims': False, 'V_cell': False, 'V_daniel': False, 'V_pilot': False, 'V_independence': False, 'V_ocean': False, 'V_eat': False, 'V_punch': False, 'V_desperately': False, 'V_reminiscent': False, 'V_knowing': False, 'V_greater': False, 'V_largely': False, 'V_hundred': False, 'V_shrek': False, 'V_flash': False, 'V_occasional': False, 'V_danger': False, 'V_thrillers': False, 'V_essentially': False, 'V_terror': True, 'V_whenever': False, 'V_suggest': False, 'V_baseball': False, 'V_blockbuster': False, 'V_pie': False, 'V_starting': False, 'V_satisfying': False, 'V_allowed': False, 'V_minds': False, 'V_neil': False, 'V_hank': False, 'V_storm': False, 'V_disappointed': False, 'V_jake': False, 'V_wearing': False, 'V_explains': False, 'V_marry': False, 'V_national': False, 'V_psychological': False, 'V_pig': False, 'V_nomination': False, 'V_critique': False, 'V_theatrical': False, 'V_psycho': False, 'V_necessarily': False, 'V_threatening': False, 'V_fallen': False, 'V_dogs': False, 'V_painful': False, 'V_plots': False, 'V_sends': False, 'V_department': False, 'V_agrees': False, 'V_league': False, 'V_covered': False, 'V_source': False, 'V_revealed': False, 'V_historical': False, 'V_perspective': False, 'V_patient': False, 'V_17': False, 'V_arquette': False, 'V_successfully': False, 'V_n': False, 'V_afraid': False, 'V_mom': False, 'V_murders': False, 'V_jar': False, 'V_suicide': False, 'V_joy': False, 'V_nightmare': False, 'V_halloween': False, 'V_cuts': False, 'V_wilson': False, 'V_nine': False, 'V_80': False, 'V_drunken': False, 'V_painfully': False, 'V_opera': False, 'V_wrestling': False, 'V_references': False, 'V_cars': False, 'V_bomb': False, 'V_luke': False, 'V_pulls': False, 'V_passion': False, 'V_sleep': False, 'V_structure': False, 'V_agree': False, 'V_occur': False, 'V_brad': False, 'V_chicago': False, 'V_con': False, 'V_attacks': False, 'V_service': False, 'V_stunts': False, 'V_bulworth': False, 'V_murray': False, 'V_trilogy': False, 'V_intensity': False, 'V_tradition': False, 'V_unfortunate': False, 'V_seth': False, 'V_stiller': False, 'V_suspenseful': False, 'V_steals': False, 'V_cat': False, 'V_investigation': False, 'V_lovely': False, 'V_stops': False, 'V_hanging': False, 'V_oddly': False, 'V_choices': False, 'V_warning': False, 'V_climactic': False, 'V_ape': False, 'V_jungle': False, 'V_england': False, 'V_sole': False, 'V_conspiracy': False, 'V_surely': False, 'V_author': False, 'V_50': False, 'V_program': False, 'V_broderick': False, 'V_china': False, 'V_weapon': True, 'V_unbelievable': False, 'V_quaid': False, 'V_gross': False, 'V_ensemble': False, 'V_dinner': False, 'V_dirty': True, 'V_soft': False, 'V_loose': False, 'V_memories': False, 'V_harris': False, 'V_laughter': False, 'V_nevertheless': False, 'V_suffering': False, 'V_initially': False, 'V_narration': False, 'V_terribly': False, 'V_stolen': False, 'V_answers': False, 'V_wealthy': False, 'V_originality': False, 'V_reaction': False, 'V_foreign': False, 'V_significant': False, 'V_donnell': False, 'V_fu': False, 'V_reminded': False, 'V_subplots': False, 'V_risk': False, 'V_creatures': False, 'V_12': False, 'V_african': False, 'V_convinced': False, 'V_lewis': False, 'V_capture': False, 'V_makers': False, 'V_requires': False, 'V_friendship': False, 'V_spy': False, 'V_met': False, 'V_uninteresting': True, 'V_christian': False, 'V_deadly': True, 'V_critical': False, 'V_behavior': False, 'V_slave': False, 'V_thrills': False, 'V_duvall': False, 'V_segment': False, 'V_monkey': False, 'V_bitter': False, 'V_status': True, 'V_refuses': False, 'V_nbsp': False, 'V_understanding': False, 'V_described': False, 'V_voiced': False, 'V_accidentally': False, 'V_endless': False, 'V_showed': False, 'V_schumacher': False, 'V_hired': False, 'V_2001': False, 'V_terry': False, 'V_walks': False, 'V_li': False, 'V_complicated': False, 'V_weekend': False, 'V_sequels': True, 'V_excitement': False, 'V_commercial': False, 'V_performers': False, 'V_rick': False, 'V_beloved': False, 'V_unlikely': False, 'V_bag': False, 'V_keanu': False, 'V_tight': False, 'V_terminator': False, 'V_desert': False, 'V_german': False, 'V_wears': False, 'V_effectively': False, 'V_flaw': False, 'V_meg': False, 'V_thoughts': False, 'V_scare': False, 'V_prime': False, 'V_soldier': False, 'V_todd': False, 'V_suffers': False, 'V_greg': False, 'V_p': False, 'V_shop': False, 'V_discovered': False, 'V_bear': False, 'V_melvin': False, 'V_dragon': False, 'V_heroine': False, 'V_sudden': False, 'V_gag': False, 'V_killers': False, 'V_halfway': False, 'V_strikes': False, 'V_root': False, 'V_handled': False, 'V_handle': False, 'V_explosions': False, 'V_mickey': False, 'V_extraordinary': False, 'V_sports': False, 'V_collection': False, 'V_photography': False, 'V_speaks': False, 'V_issue': False, 'V_mid': False, 'V_proceedings': False, 'V_ethan': False, 'V_delightful': False, 'V_laughed': False, 'V_vacation': False, 'V_pitt': False, 'V_lebowski': False, 'V_lovers': False, 'V_quirky': False, 'V_texas': False, 'V_flesh': False, 'V_dean': False, 'V_crisis': False, 'V_touches': False, 'V_crow': False, 'V_dollar': False, 'V_attempting': False, 'V_murdered': False, 'V_0': False, 'V_fugitive': False, 'V_values': False, 'V_loser': False, 'V_unable': False, 'V_drunk': False, 'V_kung': False, 'V_western': False, 'V_childhood': False, 'V_anywhere': False, 'V_gift': False, 'V_destroyed': False, 'V_chicken': False, 'V_tense': False, 'V_gonna': False, 'V_embarrassing': False, 'V_selling': False, 'V_sing': False, 'V_tragic': True, 'V_irritating': False, 'V_wind': False, 'V_barrymore': False, 'V_extended': False, 'V_dozen': False, 'V_enemy': False, 'V_frankly': False, 'V_drop': False, 'V_norton': False, 'V_hype': False, 'V_unusual': False, 'V_ordinary': False, 'V_finest': False, 'V_losing': True, 'V_ian': False, 'V_phil': False, 'V_cole': False, 'V_goodman': False, 'V_insight': False, 'V_promising': False, 'V_loss': False, 'V_weapons': False, 'V_twisted': False, 'V_deeper': False, 'V_nonetheless': False, 'V_advice': False, 'V_lake': False, 'V_campaign': False, 'V_lonely': False, 'V_karen': False, 'V_unknown': False, 'V_knock': False, 'V_blow': False, 'V_%': False, 'V_80s': False, 'V_magazine': False, 'V_instinct': False, 'V_italian': False, 'V_throwing': False, 'V_costume': False, 'V_friendly': False, 'V_fairy': False, 'V_voices': False, 'V_helped': False, 'V_lived': False, 'V_universe': False, 'V_entertain': False, 'V_adventures': False, 'V_table': False}, 'neg')\n"
     ]
    }
   ],
   "source": [
    "# the feature sets are 2000 words long - so this is optional\n",
    "print(featuresets[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81\n",
      "Most Informative Features\n",
      "                V_seagal = True              neg : pos    =     12.3 : 1.0\n",
      "           V_outstanding = True              pos : neg    =     10.9 : 1.0\n",
      "                V_finest = True              pos : neg    =      9.3 : 1.0\n",
      "                 V_mulan = True              pos : neg    =      7.7 : 1.0\n",
      "           V_wonderfully = True              pos : neg    =      7.4 : 1.0\n",
      "            V_schumacher = True              neg : pos    =      7.4 : 1.0\n",
      "                 V_damon = True              pos : neg    =      6.1 : 1.0\n",
      "                V_wasted = True              neg : pos    =      5.7 : 1.0\n",
      "                 V_flynt = True              pos : neg    =      5.7 : 1.0\n",
      "              V_lebowski = True              pos : neg    =      5.7 : 1.0\n",
      "                V_poorly = True              neg : pos    =      5.1 : 1.0\n",
      "                  V_lame = True              neg : pos    =      5.1 : 1.0\n",
      "                 V_waste = True              neg : pos    =      5.1 : 1.0\n",
      "                 V_awful = True              neg : pos    =      4.9 : 1.0\n",
      "              V_ordinary = True              pos : neg    =      4.8 : 1.0\n",
      "            V_ridiculous = True              neg : pos    =      4.7 : 1.0\n",
      "                 V_worst = True              neg : pos    =      4.4 : 1.0\n",
      "                   V_era = True              pos : neg    =      4.3 : 1.0\n",
      "            V_friendship = True              pos : neg    =      4.3 : 1.0\n",
      "                V_allows = True              pos : neg    =      4.1 : 1.0\n",
      "             V_laughable = True              neg : pos    =      4.1 : 1.0\n",
      "               V_unfunny = True              neg : pos    =      4.1 : 1.0\n",
      "            V_nomination = True              pos : neg    =      4.1 : 1.0\n",
      "              V_anywhere = True              neg : pos    =      4.0 : 1.0\n",
      "             V_painfully = True              neg : pos    =      4.0 : 1.0\n",
      "                 V_bland = True              neg : pos    =      4.0 : 1.0\n",
      "            V_satisfying = True              pos : neg    =      4.0 : 1.0\n",
      "             V_memorable = True              pos : neg    =      4.0 : 1.0\n",
      "                  V_dull = True              neg : pos    =      3.9 : 1.0\n",
      "                V_stupid = True              neg : pos    =      3.9 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# training using naive Baysian classifier with a 95/5 split\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# evaluate the accuracy of the classifier\n",
    "print (nltk.classify.accuracy(classifier, test_set))\n",
    "# the accuracy result may vary since we randomized the documents\n",
    "\n",
    "# show which features of classifier are most informative\n",
    "print(classifier.show_most_informative_features(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10662\n",
      "<class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n",
      "['neg', 'pos']\n",
      "['simplistic', ',', 'silly', 'and', 'tedious', '.']\n",
      "[\"it's\", 'so', 'laddish', 'and', 'juvenile', ',', 'only', 'teenage', 'boys', 'could', 'possibly', 'find', 'it', 'funny', '.']\n",
      "['exploitative', 'and', 'largely', 'devoid', 'of', 'the', 'depth', 'or', 'sophistication', 'that', 'would', 'make', 'watching', 'such', 'a', 'graphic', 'treatment', 'of', 'the', 'crimes', 'bearable', '.']\n",
      "['[garbus]', 'discards', 'the', 'potential', 'for', 'pathological', 'study', ',', 'exhuming', 'instead', ',', 'the', 'skewed', 'melodrama', 'of', 'the', 'circumstantial', 'situation', '.']\n"
     ]
    }
   ],
   "source": [
    "# Week-8 labs\n",
    "import nltk\n",
    "\n",
    "#nltk.download('sentence_polarity')\n",
    "\n",
    "from nltk.corpus import sentence_polarity\n",
    "import random\n",
    "\n",
    "# get the sentence corpus and look at some sentences\n",
    "sentences = sentence_polarity.sents()\n",
    "print(len(sentences))\n",
    "print(type(sentences))\n",
    "print(sentence_polarity.categories())\n",
    "# sentences are already tokenized, print the first four sentences\n",
    "for sent in sentences[:4]:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5331\n",
      "5331\n",
      "(['simplistic', ',', 'silly', 'and', 'tedious', '.'], 'neg')\n",
      "(['provides', 'a', 'porthole', 'into', 'that', 'noble', ',', 'trembling', 'incoherence', 'that', 'defines', 'us', 'all', '.'], 'pos')\n"
     ]
    }
   ],
   "source": [
    "# look at the sentences by category to see how many positive and negative\n",
    "pos_sents = sentence_polarity.sents(categories='pos')\n",
    "print(len(pos_sents))\n",
    "neg_sents = sentence_polarity.sents(categories='neg')\n",
    "print(len(neg_sents))\n",
    "\n",
    "## setup the movie reviews sentences for classification\n",
    "# create a list of documents, each document is one sentence as a list of words paired with category\n",
    "documents = [(sent, cat) for cat in sentence_polarity.categories() \n",
    "\tfor sent in sentence_polarity.sents(categories=cat)]\n",
    "\n",
    "# look at the first and last documents - consists of all the words in the review\n",
    "# followed by the category\n",
    "print(documents[0])\n",
    "print(documents[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly reorder documents\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'the', ',', 'a', 'and', 'of', 'to', 'is', 'in', 'that', 'it', 'as', 'but', 'with', 'film', 'this', 'for', 'its', 'an', 'movie', \"it's\", 'be', 'on', 'you', 'not', 'by', 'about', 'one', 'more', 'like', 'has', 'are', 'at', 'from', 'than', '\"', 'all', '--', 'his', 'have', 'so', 'if', 'or', 'story', 'i', 'too', 'just', 'who', 'into', 'what']\n"
     ]
    }
   ],
   "source": [
    "# get all words from all movie_reviews and put into a frequency distribution\n",
    "# note lowercase, but no stemming or stopwords\n",
    "all_words_list = [word for (sent,cat) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "# get the 2000 most frequently appearing keywords in the corpus\n",
    "word_items = all_words.most_common(2000)\n",
    "word_features = [word for (word,count) in word_items]\n",
    "print(word_features[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features (keywords) of a document for a BOW/unigram baseline\n",
    "# each feature is 'contains(keyword)' and is true or false depending\n",
    "# on whether that keyword is in the document\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# get features sets for a document, including keyword features and category feature\n",
    "featuresets = [(document_features(d, word_features), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'V_.': True, 'V_the': False, 'V_,': True, 'V_a': False, 'V_and': False, 'V_of': False, 'V_to': False, 'V_is': False, 'V_in': False, 'V_that': False, 'V_it': False, 'V_as': False, 'V_but': False, 'V_with': False, 'V_film': False, 'V_this': False, 'V_for': False, 'V_its': False, 'V_an': False, 'V_movie': False, \"V_it's\": False, 'V_be': False, 'V_on': False, 'V_you': False, 'V_not': False, 'V_by': False, 'V_about': False, 'V_one': False, 'V_more': False, 'V_like': False, 'V_has': False, 'V_are': False, 'V_at': False, 'V_from': False, 'V_than': False, 'V_\"': False, 'V_all': False, 'V_--': False, 'V_his': False, 'V_have': False, 'V_so': False, 'V_if': False, 'V_or': False, 'V_story': False, 'V_i': False, 'V_too': False, 'V_just': False, 'V_who': False, 'V_into': False, 'V_what': False, 'V_most': False, 'V_out': False, 'V_no': False, 'V_much': False, 'V_even': False, 'V_good': False, 'V_up': False, 'V_will': False, 'V_comedy': False, 'V_time': False, 'V_can': False, 'V_some': False, 'V_characters': False, 'V_only': False, 'V_little': False, 'V_way': False, 'V_their': False, 'V_funny': False, 'V_make': False, 'V_enough': False, 'V_been': False, 'V_very': False, 'V_your': False, 'V_when': False, 'V_never': False, 'V_makes': False, 'V_there': False, 'V_may': False, 'V_which': False, 'V_us': False, 'V_work': False, 'V_best': False, 'V_he': False, 'V_bad': False, 'V_director': False, \"V_doesn't\": False, 'V_)': False, 'V_?': False, 'V_any': False, 'V_(': False, 'V_love': False, 'V_would': False, 'V_life': False, 'V_while': False, 'V_they': False, 'V_we': False, 'V_:': False, 'V_was': False, 'V_movies': False, \"V_there's\": False, 'V_new': False, 'V_well': False, 'V_her': False, 'V_through': False, 'V_could': False, 'V_really': False, 'V_something': False, 'V_how': False, 'V_made': False, 'V_them': False, 'V_does': False, 'V_performances': False, 'V_own': False, 'V_should': False, 'V_drama': False, \"V_that's\": False, 'V_many': False, 'V_those': False, 'V_films': False, 'V_look': False, 'V_plot': False, 'V_every': False, \"V_isn't\": False, 'V_still': False, 'V_see': False, 'V_two': False, 'V_nothing': False, 'V_people': False, 'V_better': False, 'V_without': False, 'V_long': False, 'V_other': False, 'V_get': False, 'V_off': False, 'V_fun': False, 'V_action': False, 'V_being': False, 'V_both': False, 'V_great': False, 'V_though': False, 'V_might': False, 'V_big': False, \"V_'\": False, 'V_also': False, 'V_another': False, 'V_cast': False, 'V_do': False, 'V_humor': False, 'V_kind': False, 'V_audience': False, 'V_between': False, 'V_first': False, 'V_sense': False, 'V_such': False, 'V_over': False, 'V_ever': False, 'V_character': False, \"V_don't\": False, 'V_performance': False, 'V_feels': False, 'V_;': False, 'V_few': False, 'V_because': False, 'V_script': False, 'V_here': False, \"V_film's\": False, 'V_far': False, 'V_often': False, 'V_thing': False, 'V_less': False, 'V_seems': False, 'V_real': False, 'V_minutes': False, 'V_feel': False, 'V_tale': False, 'V_almost': False, 'V_world': False, 'V_picture': False, 'V_thriller': False, \"V_can't\": False, 'V_down': False, 'V_documentary': False, 'V_quite': False, 'V_yet': False, 'V_interesting': False, 'V_!': False, 'V_these': False, 'V_entertaining': False, 'V_my': False, 'V_rather': False, \"V_you're\": False, 'V_screen': False, 'V_end': False, 'V_itself': False, 'V_seen': False, 'V_full': False, 'V_watching': False, 'V_hollywood': False, 'V_take': False, 'V_hard': False, 'V_go': False, 'V_ultimately': False, 'V_heart': False, 'V_de': True, 'V_comes': False, 'V_moments': False, 'V_romantic': False, 'V_despite': False, 'V_lot': False, 'V_american': False, 'V_were': False, 'V_me': False, 'V_where': False, 'V_after': False, 'V_family': False, 'V_acting': False, 'V_before': False, 'V_then': False, 'V_original': False, 'V_had': False, 'V_old': False, 'V_right': False, 'V_find': False, 'V_worth': False, 'V_human': False, 'V_gets': False, 'V_same': False, 'V_takes': False, 'V_come': False, 'V_things': False, 'V_times': False, 'V_dialogue': False, 'V_man': False, 'V_watch': False, 'V_actors': False, 'V_back': False, 'V_scenes': False, 'V_our': False, 'V_material': False, 'V_compelling': False, 'V_young': False, 'V_once': False, 'V_music': False, 'V_works': False, 'V_years': False, 'V_emotional': False, 'V_think': False, 'V_anyone': False, 'V_seem': False, 'V_gives': False, 'V_want': False, 'V_know': False, 'V_least': False, 'V_going': False, 'V_say': False, 'V_part': False, 'V_sometimes': False, 'V_again': False, 'V_piece': False, 'V_entertainment': False, 'V_cinematic': False, 'V_kids': False, 'V_give': False, 'V_last': False, \"V_you'll\": False, 'V_pretty': False, 'V_subject': False, 'V_point': False, 'V_bit': False, 'V_making': False, 'V_keep': False, 'V_special': False, 'V_cinema': False, 'V_fascinating': False, 'V_dull': False, 'V_whole': False, 'V_together': False, 'V_why': False, 'V_fans': False, 'V_anything': False, 'V_year': False, 'V_away': False, 'V_-': False, 'V_since': False, 'V_moving': False, 'V_manages': False, 'V_need': False, 'V_style': False, 'V_laughs': False, 'V_show': False, 'V_star': False, 'V_true': False, 'V_offers': False, 'V_history': False, 'V_clever': False, 'V_experience': False, 'V_sweet': False, 'V_always': False, 'V_simply': False, 'V_direction': False, 'V_mr': False, 'V_high': False, 'V_dark': False, 'V_instead': False, 'V_silly': False, 'V_him': False, 'V_charm': False, 'V_actually': False, 'V_whose': False, 'V_care': False, 'V_predictable': False, 'V_flick': False, 'V_nearly': False, 'V_art': False, 'V_visual': False, 'V_everything': False, 'V_around': False, 'V_series': False, 'V_title': False, 'V_matter': False, 'V_video': False, 'V_place': False, 'V_comic': False, \"V_he's\": False, 'V_idea': False, 'V_narrative': False, 'V_screenplay': False, 'V_short': False, 'V_done': False, 'V_goes': False, 'V_trying': False, 'V_war': False, 'V_genre': False, 'V_probably': False, 'V_now': False, 'V_women': False, 'V_familiar': False, 'V_under': False, 'V_plays': False, 'V_premise': False, \"V_movie's\": False, 'V_enjoyable': False, 'V_horror': False, 'V_turns': False, 'V_filmmakers': False, 'V_although': False, 'V_she': False, 'V_lacks': False, 'V_home': False, 'V_engaging': False, 'V_set': False, 'V_becomes': False, 'V_three': False, 'V_smart': False, 'V_feature': False, 'V_feeling': False, 'V_worst': False, 'V_enjoy': False, \"V_won't\": False, 'V_power': False, 'V_effects': False, 'V_amusing': False, 'V_strong': False, 'V_intelligent': False, 'V_half': False, 'V_charming': False, 'V_study': False, 'V_ending': False, 'V_day': False, 'V_effort': False, 'V_especially': False, 'V_theater': False, 'V_lack': False, 'V_debut': False, 'V_boring': False, 'V_likely': False, 'V_portrait': False, 'V_john': False, 'V_men': False, 'V_each': False, 'V_romance': False, 'V_version': False, 'V_put': False, 'V_certainly': False, 'V_beautiful': False, 'V_mostly': False, \"V_what's\": False, 'V_else': False, 'V_sort': False, 'V_problem': False, 'V_looking': False, 'V_next': False, 'V_message': False, 'V_easy': False, 'V_surprisingly': False, 'V_become': False, \"V_you've\": False, 'V_level': False, 'V_beautifully': False, 'V_solid': False, 'V_fine': False, 'V_quirky': False, 'V_rare': False, 'V_lives': False, 'V_along': False, 'V_exercise': False, 'V_wit': False, 'V_hours': False, 'V_mind': False, 'V_play': False, 'V_directed': False, 'V_ideas': False, 'V_mess': False, 'V_whether': False, 'V_obvious': False, 'V_looks': False, 'V_sure': False, 'V_face': False, 'V_fresh': False, 'V_leave': False, 'V_modern': False, 'V_energy': False, 'V_interest': False, 'V_powerful': False, 'V_past': False, 'V_either': False, 'V_completely': False, 'V_french': False, 'V_classic': False, 'V_dramatic': False, 'V_shot': False, 'V_beyond': False, 'V_fact': False, 'V_melodrama': False, 'V_reason': False, 'V_viewers': False, 'V_neither': False, 'V_shows': False, 'V_delivers': False, 'V_children': False, 'V_stuff': False, 'V_tone': False, 'V_fails': False, \"V_didn't\": False, 'V_suspense': False, 'V_filmmaking': False, 'V_recent': False, 'V_everyone': False, 'V_left': False, 'V_deeply': False, 'V_slow': False, 'V_believe': False, 'V_intriguing': False, 'V_culture': False, \"V_i'm\": False, 'V_truly': False, 'V_serious': False, 'V_himself': False, 'V_actor': False, 'V_jokes': False, 'V_death': False, 'V_ends': False, 'V_black': False, 'V_book': False, 'V_tries': False, 'V_ride': False, 'V_touching': False, 'V_sad': False, 'V_occasionally': False, 'V_light': False, 'V_audiences': False, 'V_tv': False, 'V_spirit': False, 'V_production': False, 'V_small': False, 'V_formula': False, 'V_exactly': False, 'V_proves': False, 'V_dumb': False, 'V_terrific': False, 'V_satisfying': False, 'V_passion': False, 'V_role': False, 'V_hilarious': False, 'V_camera': False, 'V_opera': False, 'V_storytelling': False, 'V_reality': False, 'V_adventure': False, 'V_remains': False, 'V_talent': False, 'V_scene': False, 'V_different': False, 'V_images': False, 'V_line': False, 'V_must': False, 'V_stories': False, 'V_impossible': False, 'V_already': False, 'V_nor': False, 'V_filmmaker': False, 'V_seeing': False, 'V_got': False, 'V_project': False, 'V_journey': False, 'V_particularly': False, 'V_girl': False, 'V_simple': False, 'V_perfect': False, 'V_gags': False, 'V_personal': False, 'V_easily': False, 'V_attempt': False, 'V_above': False, 'V_turn': False, 'V_complex': False, 'V_sex': False, 'V_against': False, 'V_summer': False, 'V_close': False, 'V_psychological': False, 'V_animation': False, 'V_given': False, 'V_pretentious': False, 'V_honest': False, 'V_pleasure': False, 'V_difficult': False, 'V_falls': False, 'V_michael': False, 'V_case': False, 'V_hour': False, 'V_intelligence': False, 'V_earnest': False, 'V_ways': False, 'V_found': False, 'V_flat': False, 'V_did': False, 'V_getting': False, 'V_coming-of-age': False, 'V_thought': False, 'V_head': False, 'V_social': False, 'V_sequel': False, 'V_mystery': False, 'V_leaves': False, 'V_visually': False, 'V_political': False, 'V_cliches': False, 'V_writing': False, 'V_written': False, 'V_none': False, 'V_game': False, 'V_violence': False, 'V_uses': False, 'V_memorable': False, 'V_tell': False, 'V_crime': False, 'V_que': False, 'V_gone': False, 'V_satire': False, 'V_finally': False, 'V_job': False, 'V_brilliant': False, 'V_laugh': False, 'V_told': False, 'V_rich': False, 'V_keeps': False, 'V_wrong': False, 'V_overall': False, 'V_help': False, 'V_side': False, 'V_live': False, 'V_second': False, 'V_boy': False, 'V_rarely': False, 'V_needs': False, 'V_novel': False, 'V_barely': False, 'V_days': False, 'V_otherwise': False, 'V_contrived': False, 'V_elements': False, 'V_lost': False, 'V_cool': False, 'V_spy': False, 'V_cold': False, 'V_several': False, 'V_imagine': False, 'V_entirely': False, 'V_having': False, 'V_starts': False, 'V_final': False, 'V_guys': False, 'V_taste': False, 'V_thoughtful': False, 'V_approach': False, 'V_nature': False, 'V_during': False, 'V_o': False, 'V_acted': False, 'V_appeal': False, 'V_possible': False, 'V_y': True, 'V_excellent': False, 'V_wild': False, 'V_soap': False, 'V_mood': False, 'V_lead': False, 'V_perhaps': False, 'V_wonderful': False, 'V_moral': False, 'V_eyes': False, 'V_creepy': False, 'V_act': False, 'V_running': False, 'V_tragedy': False, 'V_insight': False, 'V_warm': False, 'V_concept': False, 'V_fairly': False, 'V_truth': False, 'V_surprising': False, 'V_others': False, 'V_attention': False, 'V_behind': False, 'V_expect': False, 'V_adults': False, 'V_among': False, 'V_engrossing': False, 'V_viewer': False, 'V_hero': False, 'V_comedies': False, 'V_quality': False, 'V_call': False, 'V_thoroughly': False, 'V_david': False, 'V_form': False, 'V_entire': False, 'V_bring': False, 'V_remarkable': False, 'V_imagination': False, 'V_tedious': False, 'V_result': False, \"V_they're\": False, 'V_emotionally': False, 'V_themselves': False, 'V_sharp': False, 'V_witty': False, 'V_perfectly': False, 'V_rock': False, 'V_animated': False, 'V_disney': False, 'V_moment': False, 'V_\\x96': False, 'V_latest': False, 'V_teen': False, 'V_genuine': False, 'V_gorgeous': False, 'V_sci-fi': False, 'V_adaptation': False, 'V_quiet': False, 'V_nice': False, 'V_change': False, 'V_usual': False, 'V_knows': False, 'V_start': False, 'V_vision': False, 'V_remake': False, 'V_four': False, 'V_epic': False, 'V_strange': False, 'V_tired': False, 'V_future': False, 'V_bland': False, 'V_la': False, \"V_you'd\": False, \"V_i've\": False, 'V_add': False, 'V_dead': False, 'V_worse': False, 'V_parents': False, 'V_worthy': False, 'V_plenty': False, 'V_points': False, 'V_maybe': False, 'V_wonder': False, 'V_hope': False, 'V_appealing': False, 'V_thanks': False, 'V_merely': False, 'V_deep': False, 'V_effective': False, 'V_gentle': False, 'V_depth': False, 'V_taking': False, 'V_until': False, 'V_impressive': False, 'V_offer': False, 'V_somewhat': False, 'V_period': False, 'V_events': False, 'V_begins': False, 'V_beauty': False, 'V_decent': False, 'V_hit': False, 'V_important': False, 'V_try': False, 'V_emotions': False, 'V_awful': False, 'V_writer-director': False, 'V_air': False, 'V_woman': False, 'V_lots': False, 'V_someone': False, 'V_captures': False, 'V_pure': False, 'V_straight': False, 'V_however': False, 'V_ambitious': False, 'V_provides': False, 'V_guy': False, 'V_definitely': False, 'V_stupid': False, 'V_career': False, 'V_unfortunately': False, 'V_age': False, \"V_year's\": False, 'V_sit': False, 'V_fantasy': False, 'V_doing': False, 'V_surprise': False, 'V_clear': False, 'V_historical': False, 'V_scary': False, 'V_exciting': False, 'V_inside': False, 'V_pace': False, 'V_thin': False, 'V_brings': False, 'V_able': False, 'V_highly': False, 'V_sequences': False, 'V_magic': False, 'V_throughout': False, 'V_ii': False, 'V_pictures': False, 'V_school': False, 'V_run': False, 'V_ugly': False, 'V_robert': False, 'V_utterly': False, 'V_view': False, 'V_execution': False, 'V_subtle': False, 'V_welcome': False, 'V_suffers': False, 'V_sound': False, 'V_based': False, 'V_alone': False, 'V_ensemble': False, 'V_deserves': False, 'V_coming': False, 'V_process': False, 'V_certain': False, 'V_sexual': False, 'V_provocative': False, 'V_examination': False, 'V_lovely': False, 'V_relationship': False, 'V_sustain': False, 'V_cheap': False, 'V_female': False, 'V_read': False, 'V_fire': False, 'V_creative': False, 'V_use': False, 'V_city': False, 'V_felt': False, 'V_memory': False, 'V_single': False, 'V_cute': False, 'V_wants': False, 'V_hand': False, 'V_major': False, 'V_tension': False, 'V_question': False, 'V_playing': False, 'V_used': False, 'V_cartoon': False, 'V_delightful': False, 'V_poignant': False, 'V_low': False, 'V_potential': False, 'V_upon': False, 'V_chemistry': False, 'V_deal': False, 'V_words': False, 'V_middle': False, 'V_quickly': False, 'V_working': False, 'V_except': False, 'V_flaws': False, 'V_impact': False, 'V_across': False, 'V_touch': False, 'V_rest': False, 'V_actress': False, 'V_yes': False, 'V_stand': False, 'V_loud': False, 'V_generic': False, 'V_sentimental': False, 'V_2': False, 'V_taken': False, 'V_unfunny': False, 'V_odd': False, 'V_sensitive': False, 'V_bond': False, 'V_unexpected': False, 'V_master': False, 'V_mediocre': False, 'V_lacking': False, 'V_success': False, 'V_flawed': False, 'V_creates': False, 'V_winning': False, 'V_murder': False, 'V_filled': False, 'V_waste': False, 'V_talented': False, 'V_puts': False, 'V_situation': False, 'V_force': False, 'V_cultural': False, 'V_remember': False, 'V_sincere': False, 'V_unsettling': False, 'V_apart': False, 'V_giving': False, 'V_college': False, 'V_heavy': False, 'V_melodramatic': False, 'V_reveals': False, 'V_era': False, 'V_supposed': False, 'V_slightly': False, 'V_create': False, 'V_surprises': False, 'V_mildly': False, 'V_convincing': False, 'V_seriously': False, 'V_class': False, 'V_ultimate': False, 'V_weird': False, 'V_dog': False, 'V_hardly': False, 'V_previous': False, 'V_ago': False, 'V_mark': False, 'V_hell': False, 'V_saw': False, 'V_hold': False, 'V_formulaic': False, 'V_cut': False, 'V_routine': False, 'V_slight': False, 'V_extremely': False, 'V_issues': False, 'V_cannot': False, 'V_ability': False, 'V_house': False, 'V_relationships': False, 'V_eye': False, 'V_watchable': False, 'V_country': False, 'V_*': False, 'V_mix': False, 'V_sets': False, 'V_forgettable': False, 'V_episode': False, 'V_plain': False, 'V_ones': False, 'V_joke': False, 'V_largely': False, 'V_indeed': False, 'V_tragic': False, 'V_focus': False, 'V_thinking': False, 'V_attempts': False, 'V_terms': False, 'V_crush': False, 'V_old-fashioned': False, 'V_pleasant': False, 'V_george': False, 'V_contemporary': False, 'V_date': False, 'V_trouble': False, 'V_grant': False, 'V_grace': False, 'V_stars': False, 'V_fully': False, 'V_steven': False, 'V_stylish': False, 'V_inventive': False, 'V_gripping': False, 'V_involved': False, 'V_extreme': False, \"V_aren't\": False, 'V_twists': False, 'V_e': False, 'V_course': False, 'V_road': False, 'V_artist': False, 'V_uneven': False, 'V_money': False, 'V_intimate': False, 'V_five': False, 'V_living': False, 'V_leads': False, 'V_casting': False, 'V_thrills': False, 'V_treat': False, 'V_finds': False, 'V_pacing': False, 'V_pop': False, 'V_amount': False, 'V_succeeds': False, 'V_crazy': False, 'V_missing': False, 'V_york': False, 'V_couple': False, 'V_happens': False, 'V_disturbing': False, 'V_painful': False, 'V_goofy': False, 'V_involving': False, 'V_business': False, 'V_called': False, 'V_presents': False, 'V_problems': False, \"V_we've\": False, 'V_friendship': False, 'V_considerable': False, 'V_unique': False, 'V_appears': False, 'V_promise': False, 'V_recommend': False, 'V_heaven': False, 'V_successful': False, 'V_poetry': False, 'V_soundtrack': False, 'V_yourself': False, 'V_williams': False, 'V_absolutely': False, 'V_water': False, 'V_badly': False, 'V_drag': False, 'V_substance': False, 'V_es': False, 'V_masterpiece': False, 'V_runs': False, 'V_themes': False, 'V_complete': False, 'V_colorful': False, 'V_happy': False, \"V_she's\": False, 'V_[a]': False, 'V_oscar': False, 'V_gay': False, 'V_share': False, 'V_british': False, 'V_tom': False, 'V_bright': False, 'V_forced': False, 'V_manner': False, 'V_hits': False, 'V_flicks': False, 'V_played': False, 'V_triumph': False, 'V_strangely': False, 'V_equally': False, 'V_forget': False, 'V_absorbing': False, 'V_pieces': False, \"V_couldn't\": False, 'V_bizarre': False, 'V_refreshing': False, 'V_somehow': False, 'V_politics': False, 'V_blend': False, 'V_loses': False, 'V_mean': False, 'V_interested': False, 'V_originality': False, 'V_soul': False, 'V_effect': False, 'V_imax': False, 'V_typical': False, 'V_sentimentality': False, 'V_target': False, 'V_situations': False, 'V_parts': False, 'V_urban': False, 'V_viewing': False, 'V_person': False, \"V_man's\": False, 'V_evil': False, 'V_wanted': False, 'V_meditation': False, 'V_terrible': False, 'V_skin': False, 'V_el': True, 'V_night': False, 'V_dream': False, 'V_obviously': False, 'V_general': False, 'V_name': False, 'V_liked': False, 'V_said': False, 'V_loss': False, 'V_fast': False, 'V_derivative': False, 'V_lame': False, 'V_sophisticated': False, 'V_dry': False, 'V_grief': False, 'V_created': False, 'V_average': False, 'V_tells': False, 'V_clichés': False, 'V_haunting': False, 'V_somewhere': False, 'V_energetic': False, 'V_frame': False, 'V_standard': False, 'V_90': False, 'V_battle': False, 'V_television': False, 'V_please': False, 'V_depressing': False, 'V_jackson': False, 'V_setting': False, 'V_virtually': False, 'V_well-acted': False, 'V_franchise': False, 'V_alive': False, 'V_sexy': False, \"V_director's\": False, 'V_central': False, 'V_word': False, 'V_twist': False, 'V_hate': False, 'V_understand': False, 'V_means': False, 'V_monster': False, 'V_whatever': False, 'V_fare': False, 'V_poor': False, 'V_eventually': False, 'V_report': False, 'V_miss': False, 'V_conflict': False, 'V_questions': False, 'V_stunning': False, 'V_crafted': False, 'V_company': False, 'V_fan': False, 'V_painfully': False, 'V_doubt': False, 'V_brain': False, 'V_let': False, 'V_finish': False, 'V_sitting': False, 'V_ms': False, 'V_price': False, 'V_match': False, 'V_tough': False, 'V_slapstick': False, 'V_girls': False, 'V_opportunity': False, 'V_edge': False, 'V_imaginative': False, 'V_weak': False, 'V_remarkably': False, 'V_japanese': False, 'V_generally': False, 'V_blue': False, 'V_footage': False, 'V_holds': False, 'V_riveting': False, 'V_break': False, 'V_worthwhile': False, 'V_clearly': False, 'V_treasure': False, 'V_superficial': False, 'V_vivid': False, 'V_heavy-handed': False, 'V_toward': False, 'V_popcorn': False, 'V_possibly': False, 'V_red': False, 'V_unlike': False, 'V_genuinely': False, 'V_lines': False, 'V_allows': False, 'V_fat': False, 'V_wedding': False, 'V_huge': False, 'V_atmosphere': False, 'V_affecting': False, 'V_inspired': False, 'V_una': False, 'V_deliver': False, 'V_places': False, 'V_insightful': False, 'V_lets': False, 'V_amazing': False, 'V_white': False, 'V_niro': False, 'V_essentially': False, 'V_room': False, 'V_thought-provoking': False, 'V_conventional': False, 'V_faith': False, 'V_refreshingly': False, 'V_excitement': False, 'V_boys': False, 'V_warmth': False, 'V_delicate': False, 'V_needed': False, 'V_efforts': False, 'V_lazy': False, 'V_green': False, \"V_hasn't\": False, 'V_affair': False, 'V_writer': False, 'V_kid': False, 'V_fiction': False, 'V_unusual': False, 'V_powers': False, 'V_wish': False, 'V_heartfelt': False, 'V_fit': False, 'V_balance': False, 'V_aside': False, 'V_wait': False, 'V_treatment': False, 'V_x': False, \"V_we're\": False, 'V_simplistic': False, 'V_wildly': False, 'V_murphy': False, 'V_shallow': False, 'V_spielberg': False, 'V_humanity': False, 'V_large': False, 'V_pay': False, 'V_car': False, 'V_2002': False, 'V_guilty': False, 'V_crowd': False, 'V_minor': False, 'V_peter': False, 'V_spectacle': False, 'V_purpose': False, 'V_filmed': False, 'V_core': False, 'V_equivalent': False, 'V_usually': False, 'V_charisma': False, 'V_intended': False, 'V_trip': False, 'V_s': False, 'V_ice': False, 'V_particular': False, 'V_within': False, 'V_damned': False, 'V_slice': False, 'V_america': False, 'V_believable': False, 'V_fashion': False, 'V_wife': False, \"V_who's\": False, 'V_longer': False, 'V_open': False, 'V_sum': False, 'V_telling': False, 'V_bears': False, 'V_poorly': False, 'V_sequence': False, 'V_gangster': False, 'V_figure': False, 'V_vehicle': False, 'V_editing': False, 'V_realistic': False, \"V_wouldn't\": False, 'V_revenge': False, 'V_mindless': False, 'V_space': False, 'V_offering': False, 'V_combination': False, 'V_theme': False, 'V_further': False, 'V_stale': False, 'V_festival': False, 'V_exhilarating': False, 'V_party': False, 'V_directors': False, 'V_motion': False, 'V_skill': False, 'V_directorial': False, 'V_greatest': False, 'V_universal': False, 'V_indie': False, 'V_dreams': False, 'V_natural': False, 'V_sports': False, 'V_decades': False, 'V_stay': False, 'V_devoid': False, 'V_deeper': False, 'V_meaning': False, 'V_quietly': False, 'V_detail': False, 'V_funnier': False, 'V_empty': False, 'V_ridiculous': False, 'V_god': False, 'V_deftly': False, 'V_delight': False, 'V_promising': False, \"V_wasn't\": False, \"V_i'd\": False, 'V_spiritual': False, 'V_sounds': False, 'V_talk': False, 'V_fear': False, 'V_credits': False, 'V_extraordinary': False, 'V_mesmerizing': False, 'V_spectacular': False, 'V_sea': False, 'V_score': False, 'V_allen': False, 'V_\\x97': False, 'V_paced': False, 'V_strength': False, 'V_represents': False, 'V_gem': False, 'V_behavior': False, 'V_win': False, 'V_repetitive': False, 'V_pleasures': False, 'V_dreary': False, 'V_daring': False, 'V_10': False, 'V_surface': False, 'V_constructed': False, 'V_james': False, 'V_reading': False, 'V_complicated': False, 'V_affection': False, 'V_roger': False, 'V_public': False, 'V_desire': False, 'V_cinematography': False, 'V_stunts': False, 'V_disguise': False, 'V_endearing': False, 'V_chase': False, 'V_society': False, 'V_expected': False, 'V_manipulative': False, 'V_turned': False, 'V_king': False, 'V_value': False, 'V_justice': False, 'V_provide': False, 'V_blade': False, 'V_consider': False, 'V_irritating': False, 'V_achieves': False, 'V_pointless': False, 'V_holes': False, 'V_sight': False, 'V_striking': False, 'V_kevin': False, 'V_performers': False, 'V_exploration': False, 'V_awkward': False, 'V_intense': False, 'V_development': False, 'V_values': False, 'V_crimes': False, 'V_count': False, 'V_j': False, 'V_overly': False, 'V_perspective': False, 'V_talking': False, 'V_avoid': False, 'V_poetic': False, 'V_oddly': False, 'V_shots': False, \"V_haven't\": False, 'V_identity': False, 'V_century': False, 'V_release': False, 'V_martin': False, 'V_stage': False, 'V_farce': False, 'V_free': False, 'V_frequently': False, 'V_prove': False, 'V_profound': False, 'V_members': False, 'V_credit': False, 'V_move': False, 'V_number': False, 'V_generation': False, 'V_visuals': False, 'V_accessible': False, 'V_results': False, 'V_gross-out': False, 'V_shame': False, 'V_del': True, 'V_loved': False, 'V_nonsense': False, 'V_machine': False, 'V_screenwriter': False, 'V_adam': False, 'V_marks': False, 'V_damn': False, 'V_songs': False, 'V_walk': False, 'V_example': False, 'V_folks': False, 'V_nicely': False, 'V_predecessor': False, 'V_enjoyed': False, 'V_serves': False, 'V_offensive': False, 'V_beneath': False, 'V_delivered': False, 'V_friends': False, 'V_return': False, 'V_designed': False, 'V_diverting': False, 'V_tribute': False, 'V_subjects': False, 'V_produced': False, 'V_thinks': False, 'V_unless': False, 'V_top': False, 'V_games': False, \"V_story's\": False, 'V_ring': False, 'V_structure': False, 'V_community': False, 'V_determined': False, 'V_struggle': False, 'V_bits': False, 'V_follow': False, 'V_giant': False, 'V_voice': False, 'V_killer': False, 'V_challenging': False, 'V_trifle': False, 'V_friday': False, 'V_shake': False, 'V_buy': False, 'V_failure': False, 'V_imagery': False, 'V_concerned': False, 'V_raw': False, 'V_tiresome': False, 'V_waiting': False, 'V_um': False, 'V_beat': False, 'V_intrigue': False, 'V_intentions': False, 'V_happen': False, 'V_wise': False, 'V_brown': False, 'V_using': False, 'V_chance': False, 'V_levels': False, 'V_courage': False, 'V_incredibly': False, 'V_hot': False, 'V_ages': False, 'V_ground': False, 'V_musical': False, 'V_presence': False, 'V_artistic': False, 'V_numbers': False, 'V_clumsy': False, 'V_weight': False, 'V_necessary': False, 'V_group': False, 'V_emerges': False, 'V_pulls': False, 'V_fears': False, 'V_stands': False, 'V_ghost': False, 'V_attraction': False, 'V_chilling': False, 'V_overcome': False, 'V_blood': False, 'V_glimpse': False, 'V_save': False, 'V_nowhere': False, 'V_frustrating': False, 'V_yarn': False, 'V_commercial': False, 'V_moviemaking': False, 'V_hackneyed': False, 'V_main': False, 'V_admirable': False, 'V_un': True, 'V_damage': False, 'V_moviegoers': False, 'V_digital': False, 'V_bunch': False, 'V_excuse': False, 'V_disappointing': False, 'V_fall': False, 'V_bore': False, 'V_deserve': False, \"V_'the\": False, 'V_monty': False, 'V_jason': False, 'V_roles': False, 'V_list': False, 'V_favor': False, 'V_outrageous': False, 'V_meandering': False, 'V_land': False, 'V_standards': False, 'V_eight': False, 'V_creating': False, 'V_budget': False, 'V_tender': False, 'V_mainstream': False, 'V_overwrought': False, 'V_twice': False, 'V_maudlin': False, 'V_bittersweet': False, 'V_broad': False, 'V_realism': False, 'V_chris': False, 'V_unpleasant': False, 'V_storyline': False, 'V_well-made': False, 'V_wonderfully': False, 'V_desperate': False, 'V_funniest': False, 'V_wry': False, 'V_emotion': False, \"V_world's\": False, 'V_violent': False, 'V_dance': False, 'V_flair': False, 'V_hip-hop': False, 'V_jones': False, 'V_b-movie': False, 'V_directing': False, 'V_inspiring': False, 'V_accomplished': False, 'V_queen': False, 'V_chinese': False, 'V_precious': False, 'V_matters': False, 'V_scenario': False, 'V_annoying': False, 'V_modest': False, 'V_type': False, 'V_fair': False, 'V_mike': False, 'V_stock': False, 'V_industry': False, 'V_authentic': False, 'V_cause': False, 'V_surely': False, 'V_comedic': False, 'V_humorous': False, 'V_sheer': False, 'V_vibrant': False, 'V_early': False, 'V_parker': False, 'V_talents': False, \"V_i'll\": False, \"V_characters'\": False, 'V_winds': False, 'V_reach': False, 'V_studio': False, 'V_grand': False, 'V_sappy': False, 'V_apparently': False, 'V_search': False, 'V_father': False, 'V_national': False, 'V_delivery': False, 'V_present': False, 'V_superior': False, 'V_team': False, 'V_grows': False, 'V_document': False, 'V_first-time': False, 'V_dazzling': False, 'V_adolescent': False, 'V_body': False, 'V_adult': False, 'V_spend': False, 'V_seat': False, 'V_today': False, 'V_vs': False, 'V_woody': False, 'V_philosophical': False, 'V_song': False, 'V_lively': False, 'V_state': False, 'V_soon': False, 'V_sign': False, 'V_self-indulgent': False, 'V_cynical': False, 'V_italian': False, 'V_heartwarming': False, 'V_cautionary': False, 'V_unfolds': False, 'V_harvard': False, 'V_sloppy': False, 'V_beginning': False, 'V_sitcom': False, 'V_mild': False, 'V_conclusion': False, 'V_trick': False, 'V_faithful': False, \"V_here's\": False, 'V_holiday': False, 'V_playful': False, 'V_mile': False, 'V_notion': False, 'V_harmless': False, 'V_gritty': False, 'V_check': False, 'V_depiction': False, 'V_writer/director': False, 'V_experiences': False, 'V_stays': False, 'V_slick': False, 'V_increasingly': False, 'V_compassion': False, 'V_pokemon': False, 'V_consistently': False, 'V_foreign': False, 'V_greek': False, 'V_fantastic': False, 'V_chan': False, 'V_metaphor': False, 'V_morality': False, 'V_thrilling': False, 'V_cliche': False, 'V_addition': False, 'V_pass': False, 'V_featuring': False, 'V_dangerous': False, 'V_empathy': False, 'V_carries': False, 'V_male': False, 'V_lies': False, 'V_joy': False, 'V_showing': False, 'V_aspects': False, 'V_complexity': False, 'V_brothers': False, 'V_mayhem': False, 'V_including': False, 'V_xxx': False, 'V_near': False, 'V_romp': False, \"V_woman's\": False, 'V_die': False, 'V_escape': False, 'V_worked': False, 'V_arnold': False, 'V_fable': False, 'V_zone': False, 'V_growing': False, 'V_angst': False, 'V_territory': False, 'V_brought': False, 'V_confusing': False, 'V_source': False, 'V_reasonably': False, 'V_wears': False, 'V_bullock': False, 'V_mother': False, 'V_secrets': False, 'V_common': False, 'V_son': False, 'V_lesson': False, 'V_paid': False, 'V_pain': False, 'V_respect': False, 'V_fight': False, 'V_choices': False, 'V_mistake': False, 'V_feel-good': False, 'V_becoming': False, 'V_mention': False, 'V_&': False, 'V_served': False, 'V_reign': False, 'V_frank': False, 'V_relatively': False, 'V_surreal': False, 'V_disaster': False, 'V_roll': False, 'V_ryan': False, 'V_features': False, 'V_eccentric': False, 'V_moves': False, 'V_forgotten': False, 'V_late': False, 'V_personality': False, 'V_plotting': False, 'V_vividly': False, 'V_retread': False, 'V_low-budget': False, 'V_starring': False, 'V_revolution': False, 'V_finding': False, 'V_brother': False, 'V_fisher': False, 'V_mixed': False, 'V_belongs': False, 'V_constantly': False, 'V_horrible': False, 'V_ordinary': False, 'V_families': False, 'V_teacher': False, 'V_sympathy': False, 'V_strictly': False, \"V_children's\": False, 'V_speaking': False, 'V_appreciate': False, 'V_logic': False, 'V_martha': False, 'V_austin': False, 'V_over-the-top': False, 'V_safe': False, 'V_punch': False, 'V_utter': False, 'V_resonance': False, 'V_patience': False, 'V_caper': False, 'V_christmas': False, 'V_stirring': False, 'V_intellectual': False, 'V_step': False, 'V_sandler': False, 'V_myself': False, 'V_drawn': False, 'V_elegant': False, 'V_hey': False, 'V_reno': False, 'V_pinocchio': False, 'V_sick': False, 'V_leaving': False, 'V_went': False, 'V_likable': False, 'V_low-key': False, 'V_technical': False, 'V_intensity': False, 'V_band': False, 'V_capture': False, 'V_hearts': False, 'V_visceral': False, 'V_conviction': False, 'V_protagonist': False, 'V_vampire': False, 'V_plodding': False, 'V_iranian': False, 'V_captivating': False, 'V_literary': False, 'V_revealing': False, 'V_vulgar': False, 'V_willing': False, 'V_product': False, 'V_south': False, 'V_cloying': False, 'V_van': False, 'V_cheesy': False, 'V_capable': False, 'V_third': False, 'V_rises': False, 'V_sides': False, 'V_spoof': False, 'V_demands': False, 'V_potentially': False, 'V_solondz': False, 'V_20': False, 'V_sequels': False, 'V_bored': False, 'V_literally': False, 'V_laughter': False, 'V_collection': False, 'V_front': False, 'V_downright': False, 'V_real-life': False, 'V_stuck': False, 'V_polished': False, 'V_acts': False, 'V_totally': False, 'V_assured': False, 'V_amateurish': False, 'V_preposterous': False, 'V_cliché': False, 'V_center': False, 'V_pity': False, 'V_sade': False, 'V_event': False, 'V_t': False, 'V_snow': False, 'V_desperately': False, 'V_unlikely': False, 'V_suspenseful': False, 'V_science': False, 'V_length': False, 'V_detailed': False, 'V_following': False, 'V_biggest': False, 'V_asks': False, 'V_brutal': False, 'V_banal': False, 'V_spare': False, 'V_stomach': False, 'V_child': False, 'V_rhythms': False, 'V_nasty': False, 'V_took': False, 'V_creativity': False, 'V_persona': False, 'V_manage': False, 'V_spooky': False, 'V_makers': False, 'V_due': False, 'V_resonant': False, 'V_details': False, 'V_older': False, 'V_spark': False, 'V_wasted': False, 'V_messages': False, \"V_'70s\": False, 'V_glass': False, 'V_[the': False, 'V_whom': False, 'V_pretension': False, 'V_understanding': False, 'V_embarrassment': False, 'V_teens': False, 'V_realized': False, 'V_current': False, 'V_rap': False, 'V_issue': False, 'V_account': False, 'V_clarity': False, 'V_preachy': False, 'V_signs': False, 'V_bigger': False, 'V_pathetic': False, 'V_tears': False, 'V_seemingly': False, 'V_observations': False, 'V_predictably': False, 'V_bottom': False, 'V_poignancy': False, 'V_silliness': False, 'V_slowly': False, 'V_insights': False, 'V_pull': False, 'V_afraid': False, 'V_opening': False, 'V_skip': False, 'V_wars': False, 'V_happened': False, 'V_miller': False, 'V_degree': False, 'V_dealing': False, 'V_stereotypes': False, 'V_harry': False, 'V_commentary': False, 'V_record': False, 'V_theaters': False, 'V_trite': False, 'V_kiss': False, 'V_rewarding': False, 'V_test': False, 'V_admire': False, 'V_entertain': False, \"V_one's\": False, 'V_basic': False, 'V_sadly': False, 'V_frontal': False, 'V_books': False, 'V_dragon': False, 'V_evocative': False, 'V_inner': False, 'V_fly': False, 'V_packed': False, 'V_sugar': False, 'V_bitter': False, 'V_directs': False, 'V_southern': False, 'V_catch': False, 'V_camp': False, 'V_quick': False, 'V_reminds': False, 'V_memories': False, 'V_expectations': False, 'V_bag': False, 'V_false': False, 'V_sentiment': False, 'V_marriage': False, 'V_traditional': False, 'V_timely': False, 'V_realize': False, 'V_bite': False, 'V_homage': False, 'V_trek': False, 'V_food': False, 'V_closer': False, 'V_barbershop': False, 'V_touches': False, 'V_hybrid': False, 'V_brilliantly': False, 'V_lousy': False, 'V_finale': False, 'V_minute': False, 'V_charms': False, 'V_loose': False, 'V_spent': False, 'V_frida': False, 'V_race': False, 'V_lyrical': False, 'V_released': False, 'V_inept': False, 'V_rent': False, 'V_channel': False, 'V_key': False, 'V_guns': False, 'V_wacky': False, 'V_winner': False, 'V_screenwriting': False, 'V_animal': False, 'V_content': False, 'V_outside': False, 'V_uninspired': False, 'V_nostalgia': False, 'V_quest': False, 'V_ludicrous': False, 'V_offbeat': False, 'V_bold': False, 'V_lady': False, \"V_filmmaker's\": False, 'V_costumes': False, 'V_birthday': False, 'V_actresses': False, 'V_self-conscious': False, 'V_learn': False, 'V_b': False, 'V_hip': False, 'V_fatal': False, 'V_design': False, 'V_hill': False, 'V_honesty': False, 'V_flashy': False, 'V_subversive': False, 'V_blair': False, 'V_credibility': False, 'V_lifeless': False, 'V_wow': False, 'V_grow': False, 'V_predecessors': False, 'V_suggests': False, 'V_artists': False, 'V_cult': False, 'V_control': False, 'V_transcends': False, 'V_onto': False, 'V_teenage': False, 'V_helps': False, 'V_twisted': False, 'V_tales': False, 'V_mainly': False, 'V_seagal': False, 'V_shock': False, 'V_showtime': False, 'V_supporting': False, 'V_ask': False, 'V_knowing': False, 'V_tasty': False, 'V_required': False, 'V_rip-off': False, 'V_saturday': False, 'V_parody': False, 'V_dramatically': False, 'V_trash': False, 'V_smile': False, 'V_uplifting': False, 'V_earlier': False, 'V_graphic': False, 'V_shocking': False, 'V_smug': False, 'V_broken': False, 'V_americans': False, 'V_importance': False, 'V_earth': False, 'V_impression': False, 'V_junk': False, 'V_turning': False, 'V_cruel': False, 'V_stealing': False, 'V_unintentional': False, 'V_effectively': False, 'V_santa': False, 'V_achievement': False, 'V_stand-up': False, 'V_figures': False, 'V_relies': False, 'V_justify': False, 'V_excess': False, 'V_heard': False, 'V_pick': False, 'V_contrivances': False, 'V_menace': False, 'V_revelatory': False, 'V_season': False, 'V_continues': False, 'V_bother': False, 'V_hugh': False, 'V_undeniably': False, 'V_charlie': False, 'V_news': False, 'V_crude': False, 'V_bloody': False, 'V_changing': False, 'V_innocence': False, 'V_missed': False, 'V_davis': False, \"V_soderbergh's\": False, 'V_diversion': False, 'V_language': False, 'V_cuts': False, 'V_pathos': False, 'V_appear': False, 'V_absurd': False, 'V_disappointed': False, 'V_startling': False, 'V_committed': False, 'V_artificial': False, 'V_honestly': False, 'V_engage': False, 'V_mixture': False, 'V_scores': False, 'V_inoffensive': False, 'V_fill': False, 'V_parable': False, 'V_equal': False, 'V_format': False, 'V_mere': False, 'V_trapped': False, 'V_stylized': False, 'V_victims': False, 'V_heartbreaking': False, 'V_notice': False, 'V_humour': False, 'V_comfort': False, 'V_incoherent': False, 'V_bringing': False, 'V_unfaithful': False, 'V_u': False, 'V_understands': False, 'V_onscreen': False, 'V_drive': False, 'V_hair': False, 'V_amused': False, 'V_distinctive': False, 'V_park': False, 'V_background': False, 'V_chaotic': False, 'V_drugs': False, 'V_adds': False, 'V_spirited': False, 'V_enterprise': False, 'V_contains': False, 'V_meant': False, 'V_lawrence': False, 'V_somber': False, 'V_rush': False}, 'pos')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.753"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the feature sets are 2000 words long so you may not want to look at one\n",
    "print(featuresets[0])\n",
    "\n",
    "# training using naive Baysian classifier, training set is approximately 90% of data\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# evaluate the accuracy of the classifier\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             V_wonderful = True              pos : neg    =     19.6 : 1.0\n",
      "                V_boring = True              neg : pos    =     19.5 : 1.0\n",
      "                  V_flat = True              neg : pos    =     14.7 : 1.0\n",
      "               V_routine = True              neg : pos    =     14.4 : 1.0\n",
      "             V_inventive = True              pos : neg    =     14.3 : 1.0\n",
      "               V_generic = True              neg : pos    =     13.7 : 1.0\n",
      "            V_refreshing = True              pos : neg    =     12.9 : 1.0\n",
      "          V_refreshingly = True              pos : neg    =     12.9 : 1.0\n",
      "                    V_90 = True              neg : pos    =     12.4 : 1.0\n",
      "              V_supposed = True              neg : pos    =     12.4 : 1.0\n",
      "           V_mesmerizing = True              pos : neg    =     11.6 : 1.0\n",
      "               V_culture = True              pos : neg    =     11.0 : 1.0\n",
      "              V_humanity = True              pos : neg    =     11.0 : 1.0\n",
      "             V_realistic = True              pos : neg    =     11.0 : 1.0\n",
      "                  V_warm = True              pos : neg    =     11.0 : 1.0\n",
      "              V_touching = True              pos : neg    =     10.7 : 1.0\n",
      "              V_captures = True              pos : neg    =     10.6 : 1.0\n",
      "              V_provides = True              pos : neg    =     10.6 : 1.0\n",
      "              V_mindless = True              neg : pos    =     10.4 : 1.0\n",
      "             V_affecting = True              pos : neg    =     10.3 : 1.0\n",
      "              V_chilling = True              pos : neg    =     10.3 : 1.0\n",
      "              V_delicate = True              pos : neg    =     10.3 : 1.0\n",
      "                V_stupid = True              neg : pos    =     10.2 : 1.0\n",
      "                  V_thin = True              neg : pos    =      9.8 : 1.0\n",
      "                  V_dull = True              neg : pos    =      9.8 : 1.0\n",
      "              V_tiresome = True              neg : pos    =      9.7 : 1.0\n",
      "                  V_ages = True              pos : neg    =      9.6 : 1.0\n",
      "                 V_fails = True              neg : pos    =      9.5 : 1.0\n",
      "                 V_waste = True              neg : pos    =      9.4 : 1.0\n",
      "                 V_worse = True              neg : pos    =      9.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# show which features of classifier are most informative\n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6885"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From subjectivity.py\n",
    "def readSubjectivity(path):\n",
    "    flexicon = open(path, 'r')\n",
    "    # initialize an empty dictionary\n",
    "    sldict = { }\n",
    "    for line in flexicon:\n",
    "        fields = line.split()   # default is to split on whitespace\n",
    "        # split each field on the '=' and keep the second part as the value\n",
    "        strength = fields[0].split(\"=\")[1]\n",
    "        word = fields[2].split(\"=\")[1]\n",
    "        posTag = fields[3].split(\"=\")[1]\n",
    "        stemmed = fields[4].split(\"=\")[1]\n",
    "        polarity = fields[5].split(\"=\")[1]\n",
    "        if (stemmed == 'y'):\n",
    "            isStemmed = True\n",
    "        else:\n",
    "            isStemmed = False\n",
    "        # put a dictionary entry with the word as the keyword\n",
    "        #     and a list of the other values\n",
    "        sldict[word] = [strength, posTag, isStemmed, polarity]\n",
    "    return sldict\n",
    "\n",
    "SLpath = \"subjclueslen1-HLTEMNLP05.tff\"\n",
    "SL = readSubjectivity(SLpath)\n",
    "\n",
    "len(SL.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['strongsubj', 'adj', False, 'neutral']\n",
      "['strongsubj', 'adj', False, 'negative']\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "# look at words in the dictionary\n",
    "print(SL['absolute'])\n",
    "print(SL['shabby'])\n",
    "\n",
    "# note what happens if the word is not there\n",
    "# print(SL['dog'])\n",
    "\n",
    "strength, posTag, isStemmed, polarity = SL['absolute']\n",
    "print(polarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# define features that include word counts of subjectivity words\n",
    "# negative feature will have number of weakly negative words +\n",
    "#    2 * number of strongly negative words\n",
    "# positive feature has similar definition\n",
    "#    not counting neutral words\n",
    "def SL_features(document, word_features, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    # count variables for the 4 classes of subjectivity\n",
    "    weakPos = 0\n",
    "    strongPos = 0\n",
    "    weakNeg = 0\n",
    "    strongNeg = 0\n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, posTag, isStemmed, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            if strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            if strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            if strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "            features['positivecount'] = weakPos + (2 * strongPos)\n",
    "            features['negativecount'] = weakNeg + (2 * strongNeg)      \n",
    "    return features\n",
    "\n",
    "SL_featuresets = [(SL_features(d, word_features, SL), c) for (d, c) in documents]\n",
    "\n",
    "# show just the two sentiment lexicon features in document 0\n",
    "print(SL_featuresets[0][0]['positivecount'])\n",
    "print(SL_featuresets[0][0]['negativecount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "2002\n"
     ]
    }
   ],
   "source": [
    "# this gives the label of document 0\n",
    "print(SL_featuresets[0][1])\n",
    "# number of features for document 0\n",
    "print(len(SL_featuresets[0][0].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.759"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrain the classifier using these features\n",
    "train_set, test_set = SL_featuresets[1000:], SL_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'is', 'a', 'difference', 'between', 'movies', 'with', 'the', 'courage', 'to', 'go', 'over', 'the', 'top', 'and', 'movies', 'that', \"don't\", 'care', 'about', 'being', 'stupid']\n",
      "['a', 'farce', 'of', 'a', 'parody', 'of', 'a', 'comedy', 'of', 'a', 'premise', ',', 'it', \"isn't\", 'a', 'comparison', 'to', 'reality', 'so', 'much', 'as', 'it', 'is', 'a', 'commentary', 'about', 'our', 'knowledge', 'of', 'films', '.']\n",
      "['i', \"didn't\", 'laugh', '.', 'i', \"didn't\", 'smile', '.', 'i', 'survived', '.']\n",
      "['i', \"didn't\", 'laugh', '.', 'i', \"didn't\", 'smile', '.', 'i', 'survived', '.']\n",
      "['most', 'of', 'the', 'problems', 'with', 'the', 'film', \"don't\", 'derive', 'from', 'the', 'screenplay', ',', 'but', 'rather', 'the', 'mediocre', 'performances', 'by', 'most', 'of', 'the', 'actors', 'involved']\n",
      "['the', 'lack', 'of', 'naturalness', 'makes', 'everything', 'seem', 'self-consciously', 'poetic', 'and', 'forced', '.', '.', '.', \"it's\", 'a', 'pity', 'that', \"[nelson's]\", 'achievement', \"doesn't\", 'match', 'his', 'ambition', '.']\n"
     ]
    }
   ],
   "source": [
    "###  Negation words\n",
    "# Negation words \"not\", \"never\" and \"no\"\n",
    "# Not can appear in contractions of the form \"doesn't\"\n",
    "for sent in list(sentences)[:50]:\n",
    "   for word in sent:\n",
    "     if (word.endswith(\"n't\")):\n",
    "       print(sent)\n",
    "\n",
    "# this list of negation words includes some \"approximate negators\" like hardly and rarely\n",
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']\n",
    "\n",
    "# One strategy with negation words is to negate the word following the negation word\n",
    "#   other strategies negate all words up to the next punctuation\n",
    "# Strategy is to go through the document words in order adding the word features,\n",
    "#   but if the word follows a negation words, change the feature to negated word\n",
    "# Start the feature set with all 2000 word features and 2000 Not word features set to false\n",
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = False\n",
    "        features['V_NOT{}'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['V_NOT{}'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['V_{}'.format(word)] = (word in word_features)\n",
    "    return features\n",
    "\n",
    "# define the feature sets\n",
    "NOT_featuresets = [(NOT_features(d, word_features, negationwords), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.757"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the values of a couple of example features\n",
    "print(NOT_featuresets[0][0]['V_NOTcare'])\n",
    "print(NOT_featuresets[0][0]['V_always'])\n",
    "\n",
    "train_set, test_set = NOT_featuresets[1000:], NOT_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             V_wonderful = True              pos : neg    =     19.6 : 1.0\n",
      "                V_boring = True              neg : pos    =     19.5 : 1.0\n",
      "                  V_flat = True              neg : pos    =     14.7 : 1.0\n",
      "               V_routine = True              neg : pos    =     14.4 : 1.0\n",
      "             V_inventive = True              pos : neg    =     14.3 : 1.0\n",
      "               V_generic = True              neg : pos    =     13.7 : 1.0\n",
      "            V_refreshing = True              pos : neg    =     12.9 : 1.0\n",
      "          V_refreshingly = True              pos : neg    =     12.9 : 1.0\n",
      "                    V_90 = True              neg : pos    =     12.4 : 1.0\n",
      "              V_supposed = True              neg : pos    =     12.4 : 1.0\n",
      "             V_NOTenough = True              neg : pos    =     11.7 : 1.0\n",
      "           V_mesmerizing = True              pos : neg    =     11.6 : 1.0\n",
      "               V_culture = True              pos : neg    =     11.0 : 1.0\n",
      "              V_humanity = True              pos : neg    =     11.0 : 1.0\n",
      "             V_realistic = True              pos : neg    =     11.0 : 1.0\n",
      "                  V_warm = True              pos : neg    =     11.0 : 1.0\n",
      "              V_touching = True              pos : neg    =     10.7 : 1.0\n",
      "              V_captures = True              pos : neg    =     10.6 : 1.0\n",
      "              V_provides = True              pos : neg    =     10.6 : 1.0\n",
      "              V_mindless = True              neg : pos    =     10.4 : 1.0\n",
      "             V_affecting = True              pos : neg    =     10.3 : 1.0\n",
      "              V_chilling = True              pos : neg    =     10.3 : 1.0\n",
      "              V_delicate = True              pos : neg    =     10.3 : 1.0\n",
      "                V_stupid = True              neg : pos    =     10.2 : 1.0\n",
      "                  V_thin = True              neg : pos    =      9.8 : 1.0\n",
      "                  V_dull = True              neg : pos    =      9.8 : 1.0\n",
      "              V_tiresome = True              neg : pos    =      9.7 : 1.0\n",
      "                  V_ages = True              pos : neg    =      9.6 : 1.0\n",
      "                 V_fails = True              neg : pos    =      9.5 : 1.0\n",
      "                 V_waste = True              neg : pos    =      9.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "157\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', \"aren't\", \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\", \"needn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"won't\", \"wouldn't\"]\n",
      "['.', ',', 'film', 'movie', 'not', 'one', 'like', '\"', '--', 'story', 'no', 'much', 'even', 'good', 'comedy', 'time', 'characters', 'little', 'way', 'funny', 'make', 'enough', 'never', 'makes', 'may', 'us', 'work', 'best', 'bad', 'director']\n"
     ]
    }
   ],
   "source": [
    "# Exercise\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(len(stopwords))\n",
    "print(stopwords)\n",
    "\n",
    "# remove some negation words \n",
    "negationwords.extend(['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'])\n",
    "\n",
    "newstopwords = [word for word in stopwords if word not in negationwords]\n",
    "print(len(newstopwords))\n",
    "print(newstopwords)\n",
    "\n",
    "# remove stop words from the all words list\n",
    "new_all_words_list = [word for (sent,cat) in documents for word in sent if word not in newstopwords]\n",
    "\n",
    "# continue to define a new all words dictionary, get the 2000 most common as new_word_features\n",
    "new_all_words = nltk.FreqDist(new_all_words_list)\n",
    "new_word_items = new_all_words.most_common(2000)\n",
    "\n",
    "new_word_features = [word for (word,count) in new_word_items]\n",
    "print(new_word_features[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the feature sets\n",
    "NOT_featuresets = [(NOT_features(d, new_word_features, negationwords), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.761"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set = NOT_featuresets[1000:], NOT_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lab Week 9 - Bigram and POS tag features, cross-validation and other evaluation measures\n",
    "# This file has small examples that are meant to be run individually\n",
    "# in the Python shell\n",
    "\n",
    "import nltk\n",
    "\n",
    "# movie review sentences\n",
    "from nltk.corpus import sentence_polarity\n",
    "import random\n",
    "\n",
    "## repeat the setup of the movie review sentences for classification\n",
    "# for each sentence(document), get its words and category (positive/negative)\n",
    "documents = [(sent, cat) for cat in sentence_polarity.categories() \n",
    "    for sent in sentence_polarity.sents(categories=cat)]\n",
    "\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['eastwood', 'winces', ',', 'clutches', 'his', 'chest', 'and', 'gasps', 'for', 'breath', '.', \"it's\", 'a', 'spectacular', 'performance', '-', 'ahem', ',', 'we', 'hope', \"it's\", 'only', 'acting', '.'], 'neg'), (['the', 'reason', 'i', 'found', 'myself', 'finally', 'unmoved', 'by', 'this', 'film', ',', 'which', 'is', 'immaculately', 'produced', 'and', 'has', 'serious', 'things', 'to', 'say', ',', 'is', 'that', 'it', 'comes', 'across', 'rather', 'too', 'plainly', 'as', 'allegory', '.'], 'neg'), (['takes', 'a', 'clunky', 'tv-movie', 'approach', 'to', 'detailing', 'a', 'chapter', 'in', 'the', 'life', 'of', 'the', 'celebrated', 'irish', 'playwright', ',', 'poet', 'and', 'drinker', '.'], 'neg'), (['a', 'haunting', 'tale', 'of', 'murder', 'and', 'mayhem', '.'], 'pos'), (['despite', 'suffering', 'a', 'sense-of-humour', 'failure', ',', 'the', 'man', 'who', 'wrote', 'rocky', 'does', 'not', 'deserve', 'to', 'go', 'down', 'with', 'a', 'ship', 'as', 'leaky', 'as', 'this', '.'], 'neg')]\n",
      "21401\n"
     ]
    }
   ],
   "source": [
    "# get all words from all movie_reviews and put into a frequency distribution\n",
    "#   note lowercase, but no stemming or stopwords\n",
    "print(documents[:5])\n",
    "all_words_list = [word for (sent,cat) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 1500 most frequently appearing keywords in the corpus\n",
    "word_items = all_words.most_common(1500)\n",
    "word_features = [word for (word,count) in word_items]\n",
    "\n",
    "# define features (keywords) of a document for a BOW/unigram baseline\n",
    "# each feature is 'contains(keyword)' and is true or false depending\n",
    "# on whether that keyword is in the document\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# get features sets for a document, including keyword features and category feature\n",
    "featuresets = [(document_features(d, word_features), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.726"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training using naive Baysian classifier, training set is 90% of data\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# evaluate the accuracy of the classifier\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "####   adding Bigram features   ####\n",
    "# set up for using bigrams\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eastwood', 'winces', ',', 'clutches', 'his', 'chest', 'and', 'gasps', 'for', 'breath', '.', \"it's\", 'a', 'spectacular', 'performance', '-', 'ahem', ',', 'we', 'hope', \"it's\", 'only', 'acting', '.', 'the', 'reason', 'i', 'found', 'myself', 'finally', 'unmoved', 'by', 'this', 'film', ',', 'which', 'is', 'immaculately', 'produced', 'and', 'has', 'serious', 'things', 'to', 'say', ',', 'is', 'that', 'it', 'comes']\n",
      "[(\"''independent\", \"film''\"), (\"'60s-homage\", 'pokepie'), (\"'[the\", 'cockettes]'), (\"'ace\", \"ventura'\"), (\"'alternate\", \"reality'\"), (\"'aunque\", 'recurre'), (\"'black\", \"culture'\"), (\"'blue\", \"crush'\"), (\"'chan\", \"moment'\"), (\"'chick\", \"flicks'\"), (\"'date\", \"movie'\"), (\"'ethnic\", 'cleansing'), (\"'face\", \"value'\"), (\"'fully\", \"experienced'\"), (\"'jason\", \"x'\"), (\"'juvenile\", \"delinquent'\"), (\"'laugh\", \"therapy'\"), (\"'masterpiece\", \"theatre'\"), (\"'nicholas\", \"nickleby'\"), (\"'old\", \"neighborhood'\"), (\"'opening\", \"up'\"), (\"'rare\", \"birds'\"), (\"'sacre\", 'bleu'), (\"'science\", \"fiction'\"), (\"'shindler's\", \"list'\"), (\"'snow\", \"dogs'\"), (\"'some\", \"body'\"), (\"'special\", \"effects'\"), (\"'terrible\", \"filmmaking'\"), (\"'time\", \"waster'\"), (\"'true\", \"story'\"), (\"'unfaithful'\", 'cheats'), (\"'very\", \"sneaky'\"), (\"'we're\", '-doing-it-for'), (\"'who's\", \"who'\"), ('-after', 'spangle'), ('-as-it-', 'thinks-it-is'), ('-as-nasty', '-as-it-'), ('-doing-it-for', \"-the-cash'\"), ('10-course', 'banquet'), ('10-year', 'delay'), ('15-cent', 'stump'), ('18-year-old', 'mistress'), (\"1950's\", 'doris'), (\"1983's\", 'koyaanisqatsi'), ('1986', 'harlem'), (\"1988's\", 'powaqqatsi'), ('1992', 'malfitano-domingo'), (\"1992's\", 'unforgiven'), ('22-year-old', 'girlfriend')]\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder on all the words in sequence\n",
    "print(all_words_list[:50])\n",
    "finder = BigramCollocationFinder.from_words(all_words_list)\n",
    "\n",
    "# define the top 500 bigrams using the chi squared measure\n",
    "bigram_features = finder.nbest(bigram_measures.chi_sq, 500)\n",
    "print(bigram_features[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Arthur', 'carefully'), ('carefully', 'rode'), ('rode', 'the'), ('the', 'brown'), ('brown', 'horse'), ('horse', 'around'), ('around', 'the'), ('the', 'castle')]\n"
     ]
    }
   ],
   "source": [
    "# examples to demonstrate the bigram feature function definition\n",
    "sent = ['Arthur','carefully','rode','the','brown','horse','around','the','castle']\n",
    "sentbigrams = list(nltk.bigrams(sent))\n",
    "print(sentbigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "B_brown_horse\n"
     ]
    }
   ],
   "source": [
    "# for a single bigram, test if it's in the sentence bigrams and format the feature name\n",
    "bigram = ('brown','horse')\n",
    "print(bigram in sentbigrams)\n",
    "print('B_{}_{}'.format(bigram[0], bigram[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features that include words as before \n",
    "# add the most frequent significant bigrams\n",
    "# this function takes the list of words in a document as an argument and returns a feature dictionary\n",
    "# it depends on the variables word_features and bigram_features\n",
    "def bigram_document_features(document, word_features, bigram_features):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "{'V_.': True, 'V_the': False, 'V_,': True, 'V_a': True, 'V_and': True, 'V_of': False, 'V_to': False, 'V_is': False, 'V_in': False, 'V_that': False, 'V_it': False, 'V_as': False, 'V_but': False, 'V_with': False, 'V_film': False, 'V_this': False, 'V_for': True, 'V_its': False, 'V_an': False, 'V_movie': False, \"V_it's\": True, 'V_be': False, 'V_on': False, 'V_you': False, 'V_not': False, 'V_by': False, 'V_about': False, 'V_one': False, 'V_more': False, 'V_like': False, 'V_has': False, 'V_are': False, 'V_at': False, 'V_from': False, 'V_than': False, 'V_\"': False, 'V_all': False, 'V_--': False, 'V_his': True, 'V_have': False, 'V_so': False, 'V_if': False, 'V_or': False, 'V_story': False, 'V_i': False, 'V_too': False, 'V_just': False, 'V_who': False, 'V_into': False, 'V_what': False, 'V_most': False, 'V_out': False, 'V_no': False, 'V_much': False, 'V_even': False, 'V_good': False, 'V_up': False, 'V_will': False, 'V_comedy': False, 'V_time': False, 'V_can': False, 'V_some': False, 'V_characters': False, 'V_only': True, 'V_little': False, 'V_way': False, 'V_their': False, 'V_funny': False, 'V_make': False, 'V_enough': False, 'V_been': False, 'V_very': False, 'V_your': False, 'V_never': False, 'V_when': False, 'V_makes': False, 'V_there': False, 'V_may': False, 'V_which': False, 'V_us': False, 'V_work': False, 'V_best': False, 'V_he': False, 'V_bad': False, 'V_director': False, \"V_doesn't\": False, 'V_)': False, 'V_?': False, 'V_any': False, 'V_(': False, 'V_love': False, 'V_would': False, 'V_life': False, 'V_while': False, 'V_they': False, 'V_we': True, 'V_:': False, 'V_was': False, 'V_movies': False, \"V_there's\": False, 'V_new': False, 'V_well': False, 'V_her': False, 'V_through': False, 'V_could': False, 'V_something': False, 'V_really': False, 'V_how': False, 'V_made': False, 'V_them': False, 'V_does': False, 'V_own': False, 'V_performances': False, 'V_drama': False, 'V_many': False, 'V_should': False, \"V_that's\": False, 'V_those': False, 'V_films': False, 'V_plot': False, 'V_look': False, \"V_isn't\": False, 'V_see': False, 'V_every': False, 'V_still': False, 'V_two': False, 'V_nothing': False, 'V_people': False, 'V_better': False, 'V_long': False, 'V_without': False, 'V_other': False, 'V_get': False, 'V_off': False, 'V_fun': False, 'V_action': False, 'V_being': False, 'V_both': False, 'V_great': False, 'V_though': False, 'V_might': False, 'V_big': False, \"V_'\": False, 'V_also': False, 'V_another': False, 'V_cast': False, 'V_do': False, 'V_humor': False, 'V_between': False, 'V_first': False, 'V_audience': False, 'V_kind': False, 'V_sense': False, 'V_such': False, 'V_over': False, 'V_character': False, 'V_ever': False, \"V_don't\": False, 'V_performance': True, 'V_feels': False, 'V_;': False, 'V_few': False, 'V_because': False, 'V_script': False, \"V_film's\": False, 'V_here': False, 'V_far': False, 'V_often': False, 'V_seems': False, 'V_thing': False, 'V_less': False, 'V_minutes': False, 'V_real': False, 'V_feel': False, 'V_tale': False, 'V_world': False, 'V_almost': False, 'V_thriller': False, 'V_picture': False, \"V_can't\": False, 'V_down': False, 'V_quite': False, 'V_documentary': False, 'V_interesting': False, 'V_yet': False, 'V_!': False, 'V_these': False, 'V_entertaining': False, 'V_rather': False, 'V_my': False, 'V_screen': False, \"V_you're\": False, 'V_end': False, 'V_itself': False, 'V_hollywood': False, 'V_seen': False, 'V_watching': False, 'V_full': False, 'V_take': False, 'V_go': False, 'V_hard': False, 'V_ultimately': False, 'V_heart': False, 'V_comes': False, 'V_de': False, 'V_moments': False, 'V_romantic': False, 'V_despite': False, 'V_lot': False, 'V_american': False, 'V_were': False, 'V_me': False, 'V_where': False, 'V_after': False, 'V_acting': True, 'V_family': False, 'V_before': False, 'V_then': False, 'V_old': False, 'V_original': False, 'V_had': False, 'V_right': False, 'V_find': False, 'V_gets': False, 'V_human': False, 'V_worth': False, 'V_same': False, 'V_takes': False, 'V_things': False, 'V_times': False, 'V_come': False, 'V_dialogue': False, 'V_man': False, 'V_back': False, 'V_scenes': False, 'V_actors': False, 'V_watch': False, 'V_our': False, 'V_material': False, 'V_compelling': False, 'V_young': False, 'V_once': False, 'V_music': False, 'V_works': False, 'V_years': False, 'V_emotional': False, 'V_think': False, 'V_seem': False, 'V_anyone': False, 'V_want': False, 'V_gives': False, 'V_going': False, 'V_know': False, 'V_least': False, 'V_say': False, 'V_part': False, 'V_piece': False, 'V_sometimes': False, 'V_again': False, 'V_cinematic': False, 'V_entertainment': False, \"V_you'll\": False, 'V_last': False, 'V_point': False, 'V_kids': False, 'V_pretty': False, 'V_subject': False, 'V_give': False, 'V_making': False, 'V_special': False, 'V_keep': False, 'V_bit': False, 'V_fascinating': False, 'V_dull': False, 'V_together': False, 'V_whole': False, 'V_why': False, 'V_cinema': False, 'V_anything': False, 'V_fans': False, 'V_year': False, 'V_-': True, 'V_moving': False, 'V_away': False, 'V_since': False, 'V_style': False, 'V_manages': False, 'V_need': False, 'V_laughs': False, 'V_star': False, 'V_show': False, 'V_true': False, 'V_experience': False, 'V_history': False, 'V_offers': False, 'V_clever': False, 'V_sweet': False, 'V_always': False, 'V_mr': False, 'V_direction': False, 'V_simply': False, 'V_high': False, 'V_instead': False, 'V_dark': False, 'V_silly': False, 'V_whose': False, 'V_him': False, 'V_care': False, 'V_predictable': False, 'V_actually': False, 'V_charm': False, 'V_visual': False, 'V_everything': False, 'V_art': False, 'V_nearly': False, 'V_flick': False, 'V_series': False, 'V_around': False, 'V_matter': False, 'V_title': False, 'V_video': False, 'V_place': False, 'V_comic': False, \"V_he's\": False, 'V_narrative': False, 'V_idea': False, 'V_screenplay': False, 'V_war': False, 'V_goes': False, 'V_short': False, 'V_done': False, 'V_trying': False, 'V_now': False, 'V_genre': False, 'V_probably': False, 'V_women': False, 'V_premise': False, 'V_plays': False, 'V_under': False, 'V_familiar': False, \"V_movie's\": False, 'V_enjoyable': False, 'V_turns': False, 'V_horror': False, 'V_she': False, 'V_three': False, 'V_although': False, 'V_home': False, 'V_becomes': False, 'V_filmmakers': False, 'V_set': False, 'V_engaging': False, 'V_lacks': False, 'V_feeling': False, 'V_feature': False, 'V_smart': False, 'V_worst': False, \"V_won't\": False, 'V_effects': False, 'V_power': False, 'V_enjoy': False, 'V_strong': False, 'V_day': False, 'V_study': False, 'V_half': False, 'V_amusing': False, 'V_charming': False, 'V_effort': False, 'V_ending': False, 'V_especially': False, 'V_intelligent': False, 'V_lack': False, 'V_debut': False, 'V_theater': False, 'V_boring': False, 'V_likely': False, 'V_men': False, 'V_each': False, 'V_portrait': False, 'V_romance': False, 'V_john': False, 'V_mostly': False, 'V_version': False, 'V_else': False, 'V_certainly': False, 'V_beautiful': False, \"V_what's\": False, 'V_put': False, 'V_sort': False, 'V_surprisingly': False, 'V_level': False, 'V_message': False, 'V_looking': False, \"V_you've\": False, 'V_problem': False, 'V_easy': False, 'V_next': False, 'V_become': False, 'V_beautifully': False, 'V_hours': False, 'V_mind': False, 'V_fine': False, 'V_exercise': False, 'V_quirky': False, 'V_wit': False, 'V_lives': False, 'V_play': False, 'V_along': False, 'V_rare': False, 'V_solid': False, 'V_mess': False, 'V_directed': False, 'V_whether': False, 'V_sure': False, 'V_fresh': False, 'V_leave': False, 'V_ideas': False, 'V_obvious': False, 'V_looks': False, 'V_face': False, 'V_french': False, 'V_completely': False, 'V_interest': False, 'V_either': False, 'V_energy': False, 'V_powerful': False, 'V_modern': False, 'V_past': False, 'V_reason': False, 'V_neither': False, 'V_classic': False, 'V_beyond': False, 'V_dramatic': False, 'V_melodrama': False, 'V_shot': False, 'V_fact': False, 'V_viewers': False, 'V_shows': False, 'V_fails': False, 'V_tone': False, 'V_recent': False, 'V_filmmaking': False, \"V_didn't\": False, 'V_children': False, 'V_delivers': False, 'V_suspense': False, 'V_stuff': False, \"V_i'm\": False, 'V_deeply': False, 'V_truly': False, 'V_believe': False, 'V_intriguing': False, 'V_everyone': False, 'V_slow': False, 'V_left': False, 'V_culture': False, 'V_serious': False, 'V_ends': False, 'V_death': False, 'V_actor': False, 'V_himself': False, 'V_jokes': False, 'V_black': False, 'V_tries': False, 'V_book': False, 'V_ride': False, 'V_touching': False, 'V_light': False, 'V_spirit': False, 'V_production': False, 'V_sad': False, 'V_tv': False, 'V_audiences': False, 'V_small': False, 'V_occasionally': False, 'V_dumb': False, 'V_opera': False, 'V_passion': False, 'V_camera': False, 'V_adventure': False, 'V_storytelling': False, 'V_remains': False, 'V_formula': False, 'V_hilarious': False, 'V_reality': False, 'V_terrific': False, 'V_role': False, 'V_satisfying': False, 'V_exactly': False, 'V_proves': False, 'V_seeing': False, 'V_project': False, 'V_line': False, 'V_filmmaker': False, 'V_talent': False, 'V_scene': False, 'V_got': False, 'V_already': False, 'V_images': False, 'V_impossible': False, 'V_stories': False, 'V_must': False, 'V_different': False, 'V_nor': False, 'V_perfect': False, 'V_simple': False, 'V_easily': False, 'V_attempt': False, 'V_personal': False, 'V_particularly': False, 'V_girl': False, 'V_journey': False, 'V_gags': False, 'V_above': False, 'V_pretentious': False, 'V_close': False, 'V_sex': False, 'V_complex': False, 'V_honest': False, 'V_psychological': False, 'V_turn': False, 'V_given': False, 'V_pleasure': False, 'V_animation': False, 'V_summer': False, 'V_against': False, 'V_found': False, 'V_hour': False, 'V_intelligence': False, 'V_case': False, 'V_difficult': False, 'V_falls': False, 'V_earnest': False, 'V_flat': False, 'V_michael': False, 'V_ways': False, 'V_did': False, 'V_head': False, 'V_cliches': False, 'V_social': False, 'V_coming-of-age': False, 'V_sequel': False, 'V_mystery': False, 'V_visually': False, 'V_leaves': False, 'V_thought': False, 'V_political': False, 'V_getting': False, 'V_memorable': False, 'V_uses': False, 'V_game': False, 'V_writing': False, 'V_crime': False, 'V_violence': False, 'V_tell': False, 'V_que': False, 'V_written': False, 'V_none': False, 'V_finally': False, 'V_rich': False, 'V_keeps': False, 'V_laugh': False, 'V_told': False, 'V_help': False, 'V_brilliant': False, 'V_job': False, 'V_side': False, 'V_live': False, 'V_satire': False, 'V_overall': False, 'V_gone': False, 'V_wrong': False, 'V_boy': False, 'V_rarely': False, 'V_needs': False, 'V_elements': False, 'V_otherwise': False, 'V_barely': False, 'V_lost': False, 'V_days': False, 'V_contrived': False, 'V_cool': False, 'V_second': False, 'V_cold': False, 'V_spy': False, 'V_novel': False, 'V_approach': False, 'V_having': False, 'V_entirely': False, 'V_starts': False, 'V_final': False, 'V_during': False, 'V_acted': False, 'V_taste': False, 'V_thoughtful': False, 'V_several': False, 'V_possible': False, 'V_guys': False, 'V_nature': False, 'V_o': False, 'V_appeal': False, 'V_imagine': False, 'V_soap': False, 'V_tragedy': False, 'V_excellent': False, 'V_insight': False, 'V_creepy': False, 'V_warm': False, 'V_wonderful': False, 'V_y': False, 'V_concept': False, 'V_lead': False, 'V_moral': False, 'V_mood': False, 'V_surprising': False, 'V_fairly': False, 'V_perhaps': False, 'V_truth': False, 'V_wild': False, 'V_act': False, 'V_eyes': False, 'V_running': False, 'V_form': False, 'V_tedious': False, 'V_adults': False, 'V_remarkable': False, 'V_entire': False, 'V_imagination': False, 'V_behind': False, 'V_emotionally': False, 'V_thoroughly': False, 'V_engrossing': False, 'V_themselves': False, 'V_result': False, 'V_comedies': False, 'V_bring': False, 'V_hero': False, 'V_david': False, 'V_call': False, 'V_viewer': False, 'V_attention': False, 'V_among': False, 'V_expect': False, 'V_others': False, \"V_they're\": False, 'V_quality': False, 'V_remake': False, 'V_sharp': False, 'V_teen': False, 'V_add': False, 'V_witty': False, 'V_tired': False, 'V_disney': False, 'V_gorgeous': False, 'V_vision': False, 'V_perfectly': False, \"V_i've\": False, 'V_knows': False, 'V_moment': False, 'V_rock': False, 'V_sci-fi': False, 'V_usual': False, 'V_start': False, 'V_nice': False, 'V_change': False, 'V_future': False, 'V_four': False, 'V_genuine': False, 'V_strange': False, \"V_you'd\": False, 'V_latest': False, 'V_quiet': False, 'V_\\x96': False, 'V_epic': False, 'V_adaptation': False, 'V_animated': False, 'V_bland': False, 'V_la': False, 'V_hope': True, 'V_worse': False, 'V_plenty': False, 'V_dead': False, 'V_maybe': False, 'V_gentle': False, 'V_impressive': False, 'V_deep': False, 'V_thanks': False, 'V_beauty': False, 'V_parents': False, 'V_somewhat': False, 'V_worthy': False, 'V_events': False, 'V_offer': False, 'V_merely': False, 'V_taking': False, 'V_appealing': False, 'V_effective': False, 'V_period': False, 'V_until': False, 'V_depth': False, 'V_wonder': False, 'V_begins': False, 'V_points': False, 'V_stupid': False, 'V_age': False, 'V_clear': False, 'V_guy': False, 'V_provides': False, 'V_try': False, 'V_straight': False, 'V_surprise': False, 'V_hit': False, 'V_pure': False, 'V_sit': False, 'V_air': False, 'V_writer-director': False, 'V_emotions': False, 'V_captures': False, 'V_fantasy': False, 'V_someone': False, 'V_however': False, 'V_unfortunately': False, 'V_doing': False, 'V_ambitious': False, 'V_important': False, \"V_year's\": False, 'V_awful': False, 'V_lots': False, 'V_career': False, 'V_woman': False, 'V_definitely': False, 'V_decent': False, 'V_ii': False, 'V_pace': False, 'V_execution': False, 'V_sequences': False, 'V_exciting': False, 'V_inside': False, 'V_historical': False, 'V_highly': False, 'V_subtle': False, 'V_magic': False, 'V_thin': False, 'V_throughout': False, 'V_suffers': False, 'V_ugly': False, 'V_able': False, 'V_utterly': False, 'V_welcome': False, 'V_robert': False, 'V_brings': False, 'V_run': False, 'V_pictures': False, 'V_view': False, 'V_scary': False, 'V_school': False, 'V_sound': False, 'V_fire': False, 'V_sustain': False, 'V_coming': False, 'V_creative': False, 'V_cheap': False, 'V_process': False, 'V_sexual': False, 'V_lovely': False, 'V_ensemble': False, 'V_provocative': False, 'V_examination': False, 'V_deserves': False, 'V_based': False, 'V_female': False, 'V_certain': False, 'V_read': False, 'V_relationship': False, 'V_alone': False, 'V_across': False, 'V_playing': False, 'V_memory': False, 'V_cartoon': False, 'V_cute': False, 'V_except': False, 'V_tension': False, 'V_quickly': False, 'V_flaws': False, 'V_single': False, 'V_used': False, 'V_felt': False, 'V_poignant': False, 'V_chemistry': False, 'V_hand': False, 'V_working': False, 'V_words': False, 'V_low': False, 'V_city': False, 'V_question': False, 'V_impact': False, 'V_use': False, 'V_deal': False, 'V_upon': False, 'V_major': False, 'V_delightful': False, 'V_middle': False, 'V_wants': False, 'V_potential': False, 'V_murder': False, 'V_force': False, 'V_unfunny': False, 'V_master': False, 'V_creates': False, 'V_cultural': False, 'V_situation': False, 'V_2': False, 'V_bond': False, 'V_yes': False, 'V_odd': False, 'V_flawed': False, 'V_rest': False, 'V_puts': False, 'V_unexpected': False, 'V_winning': False, 'V_talented': False, 'V_taken': False, 'V_sentimental': False, 'V_mediocre': False, 'V_stand': False, 'V_actress': False, 'V_success': False, 'V_loud': False, 'V_filled': False, 'V_sensitive': False, 'V_waste': False, 'V_generic': False, 'V_touch': False, 'V_lacking': False, 'V_previous': False, 'V_cut': False, 'V_house': False, 'V_surprises': False, 'V_mark': False, 'V_create': False, 'V_slight': False, 'V_class': False, 'V_unsettling': False, 'V_giving': False, 'V_cannot': False, 'V_hardly': False, 'V_slightly': False, 'V_supposed': False, 'V_saw': False, 'V_era': False, 'V_sincere': False, 'V_hold': False, 'V_remember': False, 'V_eye': False, 'V_ultimate': False, 'V_relationships': False, 'V_reveals': False, 'V_routine': False, 'V_formulaic': False, 'V_extremely': False, 'V_convincing': False, 'V_country': False, 'V_ability': False, 'V_melodramatic': False, 'V_watchable': False, 'V_weird': False, 'V_heavy': False, 'V_ago': False, 'V_mildly': False, 'V_issues': False, 'V_dog': False, 'V_hell': False, 'V_apart': False, 'V_seriously': False, 'V_college': False, 'V_*': False, 'V_terms': False, 'V_ones': False, 'V_twists': False, 'V_grace': False, 'V_forgettable': False, 'V_succeeds': False, 'V_gripping': False, 'V_episode': False, 'V_uneven': False, 'V_money': False, 'V_largely': False, 'V_five': False, 'V_contemporary': False, 'V_pleasant': False, 'V_leads': False, 'V_attempts': False, 'V_tragic': False, 'V_sets': False, 'V_intimate': False, 'V_involved': False, 'V_steven': False, 'V_artist': False, 'V_thrills': False, 'V_plain': False, 'V_pop': False, 'V_pacing': False, 'V_thinking': False, 'V_crush': False, 'V_crazy': False, 'V_date': False, 'V_stars': False, 'V_inventive': False, 'V_focus': False, 'V_joke': False, 'V_finds': False, 'V_stylish': False, 'V_amount': False, 'V_course': False, 'V_road': False, 'V_old-fashioned': False, 'V_indeed': False, 'V_mix': False, 'V_trouble': False, 'V_living': False, 'V_george': False, \"V_aren't\": False, 'V_treat': False, 'V_extreme': False, 'V_fully': False, 'V_e': False, 'V_casting': False, 'V_grant': False, 'V_happens': False, 'V_promise': False, 'V_poetry': False, 'V_runs': False, \"V_we've\": False, 'V_involving': False, 'V_recommend': False, 'V_masterpiece': False, 'V_called': False, 'V_painful': False, 'V_successful': False, 'V_presents': False, 'V_goofy': False, 'V_couple': False, 'V_appears': False, 'V_missing': False, 'V_york': False, 'V_absolutely': False, 'V_considerable': False, 'V_drag': False, 'V_soundtrack': False, 'V_es': False, 'V_business': False, 'V_heaven': False, 'V_problems': False, 'V_friendship': False, 'V_unique': False, 'V_colorful': False, 'V_happy': False, 'V_yourself': False, 'V_themes': False, 'V_badly': False, 'V_complete': False, 'V_williams': False, 'V_disturbing': False, 'V_water': False, 'V_substance': False, \"V_she's\": False, 'V_parts': False, 'V_terrible': False, 'V_somehow': False, 'V_played': False, 'V_blend': False, 'V_absorbing': False, 'V_bizarre': False, 'V_situations': False, 'V_sentimentality': False, 'V_triumph': False, 'V_wanted': False, 'V_evil': False, 'V_share': False, 'V_soul': False, 'V_tom': False, 'V_politics': False, 'V_flicks': False, 'V_british': False, 'V_hits': False, 'V_target': False, 'V_oscar': False, 'V_equally': False, 'V_[a]': False, 'V_pieces': False, 'V_mean': False, 'V_forced': False, 'V_gay': False, 'V_skin': False, \"V_man's\": False, 'V_bright': False, 'V_viewing': False, 'V_meditation': False, 'V_refreshing': False, 'V_typical': False, \"V_couldn't\": False, 'V_strangely': False, 'V_urban': False, 'V_manner': False, 'V_forget': False, 'V_loses': False, 'V_interested': False, 'V_originality': False, 'V_effect': False, 'V_imax': False, 'V_person': False, 'V_haunting': False, 'V_hate': False, 'V_grief': False, 'V_poor': False, 'V_conflict': False, 'V_obviously': False, 'V_miss': False, 'V_fast': False, 'V_central': False, 'V_company': False, 'V_crafted': False, 'V_frame': False, 'V_depressing': False, 'V_90': False, 'V_el': False, 'V_sexy': False, 'V_setting': False, 'V_average': False, 'V_franchise': False, 'V_whatever': False, 'V_please': False, 'V_jackson': False, 'V_fan': False, 'V_battle': False, 'V_questions': False, 'V_eventually': False, 'V_sophisticated': False, 'V_liked': False, 'V_clichés': False, 'V_means': False, 'V_created': False, 'V_somewhere': False, 'V_word': False, 'V_monster': False, 'V_standard': False, 'V_well-acted': False, 'V_understand': False, 'V_twist': False, 'V_doubt': False, 'V_derivative': False, 'V_dream': False, 'V_name': False, 'V_dry': False, 'V_said': False, 'V_tells': False, 'V_alive': False, 'V_general': False, 'V_loss': False, 'V_painfully': False, 'V_lame': False, \"V_director's\": False, 'V_stunning': False, 'V_fare': False, 'V_virtually': False, 'V_energetic': False, 'V_television': False, 'V_report': False, 'V_night': False, 'V_generally': False, 'V_genuinely': False, 'V_room': False, 'V_affecting': False, 'V_girls': False, 'V_brain': False, 'V_weak': False, 'V_insightful': False, 'V_japanese': False, 'V_footage': False, 'V_clearly': False, 'V_treasure': False, 'V_riveting': False, 'V_boys': False, 'V_amazing': False, 'V_vivid': False, 'V_superficial': False, 'V_lines': False, 'V_toward': False, 'V_break': False, 'V_warmth': False, 'V_essentially': False, 'V_places': False, 'V_excitement': False, 'V_blue': False, 'V_sitting': False, 'V_finish': False, 'V_imaginative': False, 'V_fat': False, 'V_wedding': False, 'V_red': False, 'V_worthwhile': False, 'V_una': False, 'V_deliver': False, 'V_opportunity': False, 'V_edge': False, 'V_atmosphere': False, 'V_holds': False, 'V_unlike': False, 'V_allows': False, 'V_tough': False, 'V_lets': False, 'V_huge': False, 'V_heavy-handed': False, 'V_niro': False, 'V_slapstick': False, 'V_price': False, 'V_let': False, 'V_faith': False, 'V_inspired': False, 'V_remarkably': False, 'V_thought-provoking': False, 'V_ms': False, 'V_possibly': False, 'V_refreshingly': False, 'V_white': False, 'V_conventional': False, 'V_popcorn': False, 'V_match': False, 'V_equivalent': False, 'V_fit': False, 'V_core': False, 'V_simplistic': False, 'V_intended': False, 'V_wildly': False, 'V_trip': False, 'V_purpose': False, 'V_slice': False, 'V_charisma': False, 'V_efforts': False, 'V_lazy': False, 'V_wish': False, 'V_fiction': False, 'V_peter': False, 'V_within': False, 'V_writer': False, 'V_ice': False, 'V_shallow': False, 'V_s': False, 'V_aside': False, 'V_guilty': False, 'V_delicate': False, 'V_heartfelt': False, 'V_believable': False, \"V_we're\": False, 'V_x': False, 'V_affair': False, 'V_humanity': False, 'V_needed': False, 'V_america': False, 'V_spielberg': False, 'V_particular': False, 'V_large': False, 'V_2002': False, 'V_car': False, \"V_hasn't\": False, 'V_murphy': False, 'V_crowd': False, 'V_wait': False, 'V_filmed': False, 'V_pay': False, 'V_kid': False, 'V_minor': False, 'V_powers': False, 'V_spectacle': False, 'V_treatment': False, 'V_usually': False, 'V_damned': False, 'V_unusual': False, 'V_balance': False, 'V_green': False, 'V_spectacular': True, 'V_funnier': False, 'V_editing': False, 'V_god': False, 'V_ridiculous': False, 'V_telling': False, 'V_quietly': False, 'V_skill': False, 'V_bears': False, 'V_longer': False, 'V_figure': False, 'V_realistic': False, 'V_extraordinary': False, 'V_revenge': False, 'V_indie': False, 'V_credits': False, 'V_score': False, 'V_combination': False, 'V_delight': False, 'V_promising': False, 'V_poorly': False, 'V_sports': False, 'V_deeper': False, 'V_sequence': False, 'V_motion': False, \"V_who's\": False, 'V_party': False, 'V_meaning': False, 'V_greatest': False, 'V_open': False, 'V_sum': False, 'V_dreams': False, 'V_mesmerizing': False, 'V_exhilarating': False, 'V_detail': False, 'V_stale': False, 'V_empty': False, 'V_directorial': False, 'V_devoid': False, 'V_vehicle': False, 'V_festival': False, \"V_i'd\": False, 'V_fear': False, 'V_spiritual': False, 'V_space': False, 'V_theme': False, 'V_natural': False, 'V_stay': False, 'V_mindless': False, 'V_decades': False, 'V_further': False, \"V_wasn't\": False, 'V_directors': False, 'V_talk': False, \"V_wouldn't\": False, 'V_wife': False, 'V_deftly': False, 'V_sounds': False, 'V_fashion': False, 'V_gangster': False, 'V_sea': False, 'V_universal': False, 'V_offering': False, 'V_values': False, 'V_allen': False, 'V_oddly': False, 'V_surface': False, 'V_value': False, 'V_disguise': False, 'V_farce': False, 'V_development': False, 'V_kevin': False, 'V_10': False, 'V_james': False, 'V_provide': False, 'V_exploration': False, 'V_century': False, 'V_blade': False, 'V_number': False, 'V_poetic': False, 'V_stage': False, 'V_justice': False, 'V_reading': False, 'V_gross-out': False, 'V_expected': False, 'V_accessible': False, 'V_profound': False, 'V_affection': False, 'V_results': False, 'V_chase': False, 'V_society': False, 'V_prove': False, 'V_represents': False, 'V_shame': False, 'V_count': False, 'V_avoid': False, 'V_frequently': False, 'V_move': False, 'V_repetitive': False, 'V_endearing': False, 'V_behavior': False, 'V_overly': False, 'V_roger': False, 'V_credit': False, 'V_desire': False, 'V_performers': False, 'V_king': False, 'V_striking': False, 'V_public': False, 'V_awkward': False, 'V_manipulative': False, 'V_intense': False, 'V_shots': False, 'V_paced': False, 'V_visuals': False, 'V_irritating': False, 'V_achieves': False, 'V_dreary': False, 'V_j': False, 'V_daring': False, \"V_haven't\": False, 'V_cinematography': False, 'V_identity': False, 'V_strength': False, 'V_talking': False, 'V_members': False, 'V_win': False, 'V_pointless': False, 'V_\\x97': False, 'V_turned': False, 'V_crimes': False, 'V_complicated': False, 'V_release': False, 'V_stunts': False, 'V_holes': False, 'V_consider': False, 'V_sight': False, 'V_free': False, 'V_gem': False, 'V_constructed': False, 'V_pleasures': False, 'V_generation': False, 'V_martin': False, 'V_perspective': False, 'V_produced': False, 'V_failure': False, 'V_ground': False, 'V_predecessor': False, 'V_commercial': False, 'V_intrigue': False, 'V_weight': False, 'V_delivered': False, 'V_bits': False, \"V_story's\": False, 'V_loved': False, 'V_top': False, 'V_return': False, 'V_frustrating': False, 'V_group': False, 'V_determined': False, 'V_serves': False, 'V_structure': False, 'V_courage': False, 'V_ages': False, 'V_enjoyed': False, 'V_example': False, 'V_del': False, 'V_nonsense': False, 'V_tiresome': False, 'V_offensive': False, 'V_marks': False, 'V_presence': False, 'V_giant': False, 'V_thinks': False, 'V_nowhere': False, 'V_brown': False, 'V_community': False, 'V_machine': False, 'V_songs': False, 'V_concerned': False, 'V_clumsy': False, 'V_beneath': False, 'V_glimpse': False, 'V_save': False, 'V_walk': False, 'V_diverting': False, 'V_killer': False, 'V_numbers': False, 'V_raw': False, 'V_ghost': False, 'V_subjects': False, 'V_imagery': False, 'V_overcome': False, 'V_tribute': False, 'V_nicely': False, 'V_waiting': False, 'V_challenging': False, 'V_hackneyed': False, 'V_struggle': False, 'V_fears': False, 'V_musical': False, 'V_intentions': False, 'V_unless': False, 'V_admirable': False, 'V_friends': False, 'V_happen': False, 'V_moviemaking': False, 'V_friday': False, 'V_stands': False, 'V_hot': False, 'V_incredibly': False, 'V_using': False, 'V_folks': False, 'V_trifle': False, 'V_blood': False, 'V_yarn': False, 'V_emerges': False, 'V_um': False, 'V_ring': False, 'V_chance': False, 'V_follow': False, 'V_shake': False, 'V_chilling': False, 'V_screenwriter': False, 'V_adam': False, 'V_pulls': False, 'V_wise': False, 'V_necessary': False, 'V_voice': False, 'V_games': False, 'V_buy': False, 'V_main': False, 'V_levels': False, 'V_artistic': False, 'V_designed': False, 'V_beat': False, 'V_attraction': False, 'V_damn': False, 'V_deserve': False, 'V_excuse': False, 'V_fall': False, 'V_broad': False, 'V_realism': False, 'V_surely': False, 'V_bore': False, 'V_studio': False, 'V_present': False, 'V_parker': False, 'V_bunch': False, 'V_reach': False, 'V_apparently': False, 'V_directing': False, 'V_un': False, 'V_creating': False, 'V_wonderfully': False, 'V_national': False, 'V_first-time': False, 'V_wry': False, 'V_b-movie': False, 'V_matters': False, 'V_overwrought': False, 'V_hip-hop': False, 'V_mike': False, 'V_land': False, 'V_inspiring': False, 'V_chinese': False, 'V_type': False, 'V_monty': False, 'V_team': False, 'V_comedic': False, 'V_storyline': False, 'V_funniest': False, 'V_fair': False, 'V_outrageous': False, 'V_emotion': False, 'V_body': False, 'V_vs': False, 'V_modest': False, 'V_roles': False, 'V_vibrant': False, 'V_well-made': False, 'V_dance': False, 'V_dazzling': False, 'V_mainstream': False, 'V_digital': False, 'V_winds': False, 'V_bittersweet': False, 'V_twice': False, 'V_industry': False, 'V_delivery': False, 'V_spend': False, 'V_grows': False, 'V_tender': False, 'V_stock': False, 'V_chris': False, 'V_flair': False, 'V_talents': False, 'V_disappointing': False, \"V_world's\": False, 'V_violent': False, 'V_favor': False, 'V_jones': False, 'V_search': False, 'V_unpleasant': False, 'V_cause': False, \"V_'the\": False, 'V_maudlin': False, \"V_characters'\": False, 'V_today': False, 'V_early': False, 'V_adolescent': False, 'V_moviegoers': False, 'V_authentic': False, 'V_seat': False, 'V_list': False, 'V_adult': False, 'V_sappy': False, 'V_jason': False, 'V_precious': False, 'V_scenario': False, \"B_''independent_film''\": False, \"B_'60s-homage_pokepie\": False, \"B_'[the_cockettes]\": False, \"B_'ace_ventura'\": False, \"B_'alternate_reality'\": False, \"B_'aunque_recurre\": False, \"B_'black_culture'\": False, \"B_'blue_crush'\": False, \"B_'chan_moment'\": False, \"B_'chick_flicks'\": False, \"B_'date_movie'\": False, \"B_'ethnic_cleansing\": False, \"B_'face_value'\": False, \"B_'fully_experienced'\": False, \"B_'jason_x'\": False, \"B_'juvenile_delinquent'\": False, \"B_'laugh_therapy'\": False, \"B_'masterpiece_theatre'\": False, \"B_'nicholas_nickleby'\": False, \"B_'old_neighborhood'\": False, \"B_'opening_up'\": False, \"B_'rare_birds'\": False, \"B_'sacre_bleu\": False, \"B_'science_fiction'\": False, \"B_'shindler's_list'\": False, \"B_'snow_dogs'\": False, \"B_'some_body'\": False, \"B_'special_effects'\": False, \"B_'terrible_filmmaking'\": False, \"B_'time_waster'\": False, \"B_'true_story'\": False, \"B_'unfaithful'_cheats\": False, \"B_'very_sneaky'\": False, \"B_'we're_-doing-it-for\": False, \"B_'who's_who'\": False, 'B_-after_spangle': False, 'B_-as-it-_thinks-it-is': False, 'B_-as-nasty_-as-it-': False, \"B_-doing-it-for_-the-cash'\": False, 'B_10-course_banquet': False, 'B_10-year_delay': False, 'B_15-cent_stump': False, 'B_18-year-old_mistress': False, \"B_1950's_doris\": False, \"B_1983's_koyaanisqatsi\": False, 'B_1986_harlem': False, \"B_1988's_powaqqatsi\": False, 'B_1992_malfitano-domingo': False, \"B_1992's_unforgiven\": False, 'B_22-year-old_girlfriend': False, 'B_50-something_lovebirds': False, 'B_75-minute_sample': False, 'B_93-minute_condensation': False, 'B_99-minute_stink': False, 'B_<em>ash_wednesday</em>': False, 'B_[at_least]': False, \"B_[breheny's]_lensing\": False, \"B_[but_it's]\": False, 'B_[cuarón_has]': False, 'B_[fiji_diver': False, 'B_[garbus]_discards': False, \"B_[gayton's_script]\": False, \"B_[hawn's_character]is\": False, \"B_[jack_nicholson's]\": False, 'B_[jaglom]_pursued': False, 'B_[jason_bourne]': False, 'B_[lawrence_bounces]': False, \"B_[lin_chung's]\": False, 'B_[macdowell]_ventures': False, 'B_[monsoon_wedding]': False, \"B_[raimi's]_matured\": False, 'B_[roman_coppola]': False, \"B_[sam's]_self-flagellation\": False, \"B_[seinfeld's]_revered\": False, 'B_[taymor]_utilizes': False, 'B_[teen_comedy]': False, 'B_[than_leon]': False, 'B_[woody_allen]': False, \"B_`matrix'-style_massacres\": False, \"B_abderrahmane_sissako's\": False, 'B_abdul_malik': False, 'B_absolutamente_predecible': False, 'B_accomodates_practical': False, 'B_ace_japanimator': False, 'B_aceitou_dirigir': False, \"B_achero_manas's\": False, \"B_achilles'_heel\": False, 'B_acting-workshop_exercises': False, 'B_action-and-popcorn_obsessed': False, 'B_activate_girlish': False, 'B_actores_ofrecen': False, 'B_actorish_notations': False, 'B_actory_concoctions': False, 'B_actuaciones_verdaderamente': False, 'B_adapted-_from-television': False, \"B_adjective_'gentle'\": False, 'B_adrien_brody': False, 'B_advanced_prozac': False, 'B_advises_denlopp': False, 'B_ageism_afflicting': False, 'B_ages--a_trend': False, 'B_agnostic_carnivore': False, 'B_agreeable_time-wasting': False, \"B_ain't-_she-cute\": False, 'B_aircraft_carrier': False, 'B_airport_security': False, \"B_aisle's_walker\": False, \"B_akira_kurosawa's\": False, \"B_alain_choquart's\": False, 'B_alchemical_transmogrification': False, 'B_alcoholic_beverage': False, \"B_ali's_graduation\": False, \"B_aliens'_laser\": False, 'B_all-night_tequila': False, 'B_all-too_-inevitable': False, 'B_ally_mcbeal-style': False, 'B_american-russian_armageddon': False, 'B_anarchist_maxim': False, 'B_ancillary_products': False, \"B_andersson's_rigorous\": False, \"B_andrew's_turnabout\": False, 'B_andré_turpin': False, 'B_angela_gheorghiu': False, \"B_ankle-deep_'epic\": False, 'B_annual_riviera': False, \"B_anton_chekhov's\": False, \"B_argentinean_'dramedy'\": False, 'B_armenian_genocide': False, 'B_arrancar_lágrimas': False, 'B_artless_sytle': False, 'B_ash_wednesday': False, \"B_asinine_'twist'\": False, \"B_attal's_hang-ups\": False, 'B_avant_garde': False, 'B_avuncular_chortles': False, 'B_axel_hellstenius': False, 'B_b-film_thuggery': False, \"B_babak_payami's\": False, \"B_bailly's_rom-com\": False, 'B_bar-scrapping_doggedness': False, 'B_barrett_browning': False, 'B_baseball-playing_monkey': False, 'B_basest_desires': False, 'B_baz_luhrmann': False, 'B_beachcombing_verismo': False, 'B_beached_grouper': False, 'B_beau_travil': False, 'B_bebe_neuwirth': False, 'B_beckett_applied': False, 'B_bedside_vigils': False, 'B_been-told-a-_thousand-times-': False, \"B_bela_lugosi's\": False, 'B_beloved-major-_character-who-shall-': False, \"B_belushi's_easy-going\": False, \"B_berkley's_flopping\": False, \"B_bettany/mcdowell's_hard-eyed\": False, 'B_binary_oppositions': False, 'B_bitchy_frolic': False, 'B_black-&-white_freeze': False, 'B_blending_entrepreneurial': False, 'B_blessedly_curse-free': False, 'B_blond_honeys': False, 'B_bluto_blutarsky': False, \"B_boat's_malediction\": False, 'B_bob_crane': False, 'B_bona_fide': False, 'B_bone-crushing_screwups': False, 'B_booming_bass-heavy': False, \"B_borchardt's_coven\": False, 'B_breezily_apolitical': False, 'B_brent_hanley': False, 'B_broaches_neo-augustinian': False, \"B_brooks'_borscht\": False, \"B_brothers'_oeuvre\": False, 'B_brussels_sprouts': False, 'B_buckaroo_banzai': False, 'B_buena_oportunidad': False, 'B_bump-in_-the-night': False, 'B_burkina_faso': False, 'B_busby_berkeley': False, \"B_buñuel's_casings\": False, \"B_bv's_re-voiced\": False, 'B_cafeteria_goulash': False, \"B_cagney's_'top\": False, 'B_camareras_italianas': False, \"B_canada's_arctic\": False, 'B_cannier_doppelganger': False, 'B_casey_kasem-furnished': False, 'B_casi_siempre': False, 'B_caso_você': False, \"B_cat's_meow\": False, 'B_cattle_prod': False, 'B_certamente_fica': False, 'B_channel-style_anthology': False, 'B_character-_and-': False, 'B_character-who-shall-_remain-nameless': False, 'B_cheese-laced_spectacles': False, 'B_chefs_fussing': False, 'B_cherry_orchard': False, 'B_chirpy_songbird': False, 'B_chloroform-soaked_handkerchief': False, 'B_chop_suey': False, 'B_christina_ricci': False, 'B_christmas-tree_flocking': False, 'B_christophe_honoré': False, 'B_cierta_realidad': False, \"B_city's_old-world\": False, 'B_civics_classes': False, 'B_clarissa_dalloway': False, 'B_cleopatra_2525': False, 'B_cleveland_sites': False, 'B_cliche-bound_epic-horror': False, 'B_clive_barker': False, 'B_closed-door_hanky-panky': False, 'B_closed-off_nationalist': False, \"B_clyde_barrow's\": False, 'B_coldly_self-interested': False, \"B_columbia_pictures'\": False, 'B_comatose_ballerinas': False, 'B_combustion_engine': False, 'B_comeback_curlers': False, 'B_comedias_románticas': False, 'B_comienza_intentando': False, 'B_communion_wafer': False, 'B_community-college_advertisement': False, 'B_competing_lawyers': False, 'B_computerized_yoda': False, 'B_confidently_orchestrated': False, 'B_constrictive_eisenhower': False, 'B_contemptible_imitator': False, 'B_contentious_configurations': False, 'B_continental_divides': False, 'B_contrária_à': False, \"B_cooper's_agency\": False, 'B_coos_beseechingly': False, 'B_cop-flick_subgenre': False, 'B_cor-blimey-luv-a-duck_cockney': False, 'B_cosa_nostra': False, \"B_costner's_warm-milk\": False, 'B_costumer_jived': False, 'B_coupling_disgracefully': False, \"B_court's_extradition\": False, 'B_cowardly_autocritique': False, 'B_creepy-crawly_bug': False, 'B_crematorium_chimney': False, 'B_crowded_cities': False, 'B_crème_brûlée': False, 'B_cuerpo_desnudo': False, 'B_culprit_early-on': False, 'B_cyndi_lauper': False, \"B_danilo_donati's\": False, \"B_danis_tanovic's\": False, \"B_dante_spinotti's\": False, 'B_das_boot': False, \"B_dawson's_creek\": False, 'B_debating_societies': False, 'B_decent-enough_nail-biter': False, 'B_deceptive_grimness': False, 'B_decidiram_assistir': False, \"B_def_leppard's\": False, 'B_defunct_cleopatra': False, 'B_delibrately_obtuse': False, \"B_delinquent'_paperbacks\": False, 'B_densest_distillation': False, 'B_dentist_drill': False, 'B_der_groen': False, 'B_deve_ter': False, 'B_developmentally_disabled': False, 'B_di_napoli': False, \"B_dignified_ceo's\": False, 'B_dilithium_crystals': False, 'B_dim_echo': False, 'B_dingy_backrooms': False, 'B_diop_gai': False, 'B_director-chef_gabriele': False, 'B_dirigir_esta': False, 'B_disciplined_grade-grubbers': False, 'B_disease-of-_the-week': False, 'B_disfrutable_trabajo': False, 'B_disloyal_satyr': False, 'B_diver_rusi': False, 'B_diverting--if_predictable--adventure': False, 'B_djeinaba_diop': False, 'B_doce_lar': False, 'B_docu-dogma_plainness': False, 'B_doggie_winks': False, 'B_dolly_parton': False, \"B_domineering_mother's\": False, 'B_doodled_steamboat': False, 'B_doofus-on-_the-loose': False, 'B_dos_musicais': False, 'B_doubling_subtexts': False, 'B_down-and-dirty_laugher': False, 'B_downy-cheeked_moppets': False, 'B_drabness_endemic': False, 'B_drinking_twelve': False, 'B_du_sarcasm': False, 'B_duas_torres': False, \"B_dubya's_axis\": False, 'B_duda_cabe': False, 'B_duplicate_bela': False, 'B_dust-caked_stagnation': False, \"B_duvall's_throbbing\": False, \"B_eagle's_carpets\": False, \"B_early-'80s_suburbia\": False, 'B_easy-going_likableness': False, 'B_eddy_crooned': False, 'B_edged_tome': False, \"B_edmund_mcwilliams's\": False, 'B_egregious_lip-non-synching': False, 'B_egyptian_demigod': False, \"B_elder_bueller's\": False, 'B_elegante_abandono': False, 'B_ellen_pompeo': False, 'B_elliptically_loops': False, \"B_elm_street'\": False, \"B_elmer_bernstein's\": False, 'B_elmo_touts': False, \"B_elvira's_hooters\": False, 'B_emocionalmente_frio': False, 'B_end-of-year_401': False, 'B_enforced_hiatus': False, 'B_enfrentará_cierta': False, 'B_entangled_interrelationships': False, 'B_entrance_exam': False, 'B_entretenimiento_puro': False, 'B_enviará_penas': False, \"B_era's_creme\": False, 'B_erin_brockovich': False, 'B_esas_películas': False, 'B_esfera_adicional': False, 'B_esos_filmes': False, 'B_espectáculo_digno': False, 'B_esta_continuação': False, 'B_estadounidense_patriotero': False, 'B_estava_feliz': False, 'B_esteemed_writer-actor': False, \"B_estela_bravo's\": False, 'B_estranhos_acontecimentos': False, 'B_esté_realizando': False, 'B_eternally_devoted': False, 'B_eu_estava': False, 'B_eudora_welty': False, 'B_everlyn_sampi': False, 'B_every-joke-has-_been-told-a-': False, 'B_evolução_dos': False, 'B_existent_anti-virus': False, 'B_extemporaneously_shouted': False, 'B_extradition_chess': False, 'B_extremist_name-calling': False, 'B_fabuleux_destin': False, 'B_fai_are]': False, 'B_faltering_half-step': False, 'B_fanatically_fetishized': False, 'B_fang-baring_lullaby': False, 'B_fangoria_subscriber': False, 'B_fatalist_worldview': False, 'B_faux-contemporary_gravy': False, 'B_ferris_bueller': False, 'B_fictions_peddled': False, 'B_fide_groaners': False, 'B_field-sized_oriental': False, \"B_fifteen-year-old's_suicidal\": False, 'B_fifties_teen-gang': False, 'B_figurative_port-of-call': False, 'B_fill-in-_the-blanks': False, \"B_films'_loose-jointed\": False, 'B_financial_extortion': False, 'B_fingernails_instinctively': False, 'B_flails_limply': False, 'B_flakeball_spouting': False, 'B_fledgling_democracies': False, 'B_flick-knife_diction': False, 'B_flopping_dolphin-gasm': False, 'B_foo_yung': False, \"B_foreman's_barking-mad\": False, 'B_forrest_gump': False, 'B_four-legged_herbivore': False, \"B_fracasso_'artístico'\": False, 'B_francamente_aburrido': False, 'B_frazzled_wackiness': False, 'B_freewheeling_trash-cinema': False, \"B_friend's_playboy-mansion\": False, 'B_frothing_ex-girlfriend': False, 'B_fruit_pies': False, 'B_future-world_holographic': False, 'B_fílmica_imposible': False, 'B_gabbiest_giant-screen': False, \"B_gallic_'tradition\": False, 'B_gambol_fluidly': False, 'B_ganha_força': False, 'B_garcía_bernal': False, 'B_garry_shandling': False, 'B_gelo_diverte': False, 'B_genevieve_leplouff': False, 'B_genuinamente_sorprendido': False, 'B_geographical_displacement': False, \"B_george's_haplessness\": False, 'B_gerald_bounce': False, \"B_gibson's_braveheart\": False, 'B_giles_nuttgens': False, \"B_glass's_dirgelike\": False, 'B_gone-to-pot_asbury': False, 'B_good-deed/bad-deed_reversals': False, 'B_gosta_muito': False, 'B_government/_marine/legal': False, 'B_grandes_revelaciones': False, 'B_graças_às': False, 'B_grouchy_ayatollah': False, 'B_gryffindor_scarf': False, \"B_guarantees_karmen's\": False, 'B_gushing---imamura_squirts': False, \"B_gutter_romancer's\": False, \"B_guys'_superhuman\": False, 'B_halle_berry': False, 'B_happily-ever_-after': False, 'B_hard-_boiled': False, 'B_hard-sell_image-mongering': False, 'B_harmlessly_naïve': False, 'B_harold_ramis': False, \"B_harvey_weinstein's\": False, 'B_haute_couture': False, 'B_havia_satirizado': False, 'B_heartstring_untugged': False, 'B_heartwarmingly_motivate': False, 'B_heavy-duty_ropes': False, 'B_heedless_impetuousness': False, 'B_helene_weigel': False, \"B_helms'_anti-\": False, \"B_henry's_<b>the\": False, 'B_heretofore_unfathomable': False, 'B_hibiscus_grandly': False, 'B_high-wattage_brainpower': False, 'B_hilary_birmingham': False, 'B_hippopotamus_ballerina': False, 'B_hit-_and-miss': False, 'B_hollywood-ized_austen': False, \"B_hornby's_drop-dead\": False, 'B_hossein_amini': False, \"B_howie_long's\": False, 'B_huckster_lapel': False, 'B_humorously_tendentious': False, 'B_i-heard-a-joke-_at-a-frat-party': False, 'B_iben_hjelje': False, 'B_idealistically_selfless': False, 'B_iditarod_lasts': False, 'B_idoosyncratic_terrain': False, 'B_igualmente_fascinantes': False, 'B_ill-timed_antitrust': False, 'B_ill-wrought_hypothesis': False, 'B_imogen_kimmel': False, 'B_improbably_forbearing': False, 'B_impudent_snickers': False, 'B_in-the-ring_match-up': False, 'B_indecent_proposal': False, 'B_independent-community_guiding': False, 'B_inhalant_blackout': False, 'B_initiation_rite': False, 'B_inside-show-biz_yarns': False, \"B_insider's_lingo\": False, 'B_instinctively_crawled': False, 'B_institutionalized_slavery': False, 'B_intelectualmente_retadora': False, 'B_interacting_eyeball-to-eyeball': False, 'B_interações_entre': False, 'B_internal_combustion': False, \"B_irvine_welsh's\": False, \"B_ismail_merchant's\": False, \"B_it's-surreal_dubbing\": False, 'B_italy_beckons': False, 'B_ivans_xtc': False, 'B_ja_rule': False, \"B_jae-eun_jeong's\": False, 'B_jagjit_singh': False, 'B_janszen_bungle': False, 'B_japanimator_hayao': False, 'B_jazz-playing_exterminator': False, 'B_jeanette_macdonald': False, \"B_jeff_foxworthy's\": False, 'B_jeweled_beads': False, 'B_jez_butterworth': False, \"B_jiri_menzel's\": False, \"B_jirí_hubac's\": False, 'B_joaquin_baca-asay': False, 'B_joint_promotion': False, \"B_jon_purdy's\": False, 'B_josef_bierbichler': False, 'B_joão_pedro': False, \"B_juliet_stevenon's\": False, 'B_june_cleaver': False, 'B_jungle_needing': False, 'B_just-above-average_off-': False, 'B_ka_fai': False, \"B_kaige's_assistant\": False, 'B_kappa_rho': False, \"B_karmen's_enthronement\": False, 'B_kathleen_soliah': False, \"B_kerrigan's_platinum-blonde\": False, 'B_killer-thrillers_revolve': False, \"B_kin's_reworked\": False}\n"
     ]
    }
   ],
   "source": [
    "# use this function to create feature sets for all sentences\n",
    "bigram_featuresets = [(bigram_document_features(d, word_features, bigram_features), c) for (d, c) in documents]\n",
    "\n",
    "# number of features for document 0\n",
    "print(len(bigram_featuresets[0][0].keys()))\n",
    "\n",
    "# features in document 0\n",
    "print(bigram_featuresets[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.726"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a classifier and report accuracy\n",
    "train_set, test_set = bigram_featuresets[1000:], bigram_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Arthur', 'carefully', 'rode', 'the', 'brown', 'horse', 'around', 'the', 'castle']\n",
      "[('Arthur', 'NNP'), ('carefully', 'RB'), ('rode', 'VBD'), ('the', 'DT'), ('brown', 'JJ'), ('horse', 'NN'), ('around', 'IN'), ('the', 'DT'), ('castle', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "###  POS tag counts\n",
    "# using the default pos tagger in NLTK (the Stanford tagger)\n",
    "print(sent)\n",
    "print(nltk.pos_tag(sent))\n",
    "\n",
    "# this function takes a document list of words and returns a feature dictionary\n",
    "# it runs the default pos tagger (the Stanford tagger) on the document\n",
    "#   and counts 4 types of pos tags to use as features\n",
    "def POS_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    numNoun = 0\n",
    "    numVerb = 0\n",
    "    numAdj = 0\n",
    "    numAdverb = 0\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag.startswith('N'): numNoun += 1\n",
    "        if tag.startswith('V'): numVerb += 1\n",
    "        if tag.startswith('J'): numAdj += 1\n",
    "        if tag.startswith('R'): numAdverb += 1\n",
    "    features['nouns'] = numNoun\n",
    "    features['verbs'] = numVerb\n",
    "    features['adjectives'] = numAdj\n",
    "    features['adverbs'] = numAdverb\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1504\n"
     ]
    }
   ],
   "source": [
    "# define feature sets using this function\n",
    "POS_featuresets = [(POS_features(d, word_features), c) for (d, c) in documents]\n",
    "# number of features for document 0\n",
    "print(len(POS_featuresets[0][0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['eastwood', 'winces', ',', 'clutches', 'his', 'chest', 'and', 'gasps', 'for', 'breath', '.', \"it's\", 'a', 'spectacular', 'performance', '-', 'ahem', ',', 'we', 'hope', \"it's\", 'only', 'acting', '.'], 'neg')\n",
      "num nouns 7\n",
      "num verbs 4\n",
      "num adjectives 1\n",
      "num adverbs 1\n"
     ]
    }
   ],
   "source": [
    "# the first sentence\n",
    "print(documents[0])\n",
    "# the pos tag features for this sentence\n",
    "print('num nouns', POS_featuresets[0][0]['nouns'])\n",
    "print('num verbs', POS_featuresets[0][0]['verbs'])\n",
    "print('num adjectives', POS_featuresets[0][0]['adjectives'])\n",
    "print('num adverbs', POS_featuresets[0][0]['adverbs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.723"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and test the classifier\n",
    "train_set, test_set = POS_featuresets[1000:], POS_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2132\n",
      "0 0.7354596622889306\n",
      "1 0.7434333958724203\n",
      "2 0.726078799249531\n",
      "3 0.7551594746716698\n",
      "4 0.7382739212007504\n",
      "mean accuracy 0.7396810506566605\n"
     ]
    }
   ],
   "source": [
    "## cross-validation ##\n",
    "# this function takes the number of folds, the feature sets\n",
    "# it iterates over the folds, using different sections for training and testing in turn\n",
    "#   it prints the accuracy for each fold and the average accuracy at the end\n",
    "def cross_validation_accuracy(num_folds, featuresets):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    print('Each fold size:', subset_size)\n",
    "    accuracy_list = []\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[(i*subset_size):][:subset_size]\n",
    "        train_this_round = featuresets[:(i*subset_size)] + featuresets[((i+1)*subset_size):]\n",
    "        # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        # evaluate against test_this_round and save accuracy\n",
    "        accuracy_this_round = nltk.classify.accuracy(classifier, test_this_round)\n",
    "        print (i, accuracy_this_round)\n",
    "        accuracy_list.append(accuracy_this_round)\n",
    "    # find mean accuracy over all rounds\n",
    "    print ('mean accuracy', sum(accuracy_list) / num_folds)\n",
    "\n",
    "# perform the cross-validation on the featuresets with word features and generate accuracy\n",
    "num_folds = 5\n",
    "cross_validation_accuracy(num_folds, featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'pos', 'pos', 'pos', 'neg', 'pos', 'pos', 'neg', 'neg', 'pos', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg', 'pos', 'neg', 'neg']\n",
      "['pos', 'neg', 'pos', 'pos', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg', 'neg', 'pos', 'neg', 'pos', 'neg', 'neg', 'pos', 'neg', 'neg', 'pos', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg', 'pos', 'neg', 'neg']\n",
      "    |   n   p |\n",
      "    |   e   o |\n",
      "    |   g   s |\n",
      "----+---------+\n",
      "neg |<369>139 |\n",
      "pos | 138<354>|\n",
      "----+---------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "    |      n      p |\n",
      "    |      e      o |\n",
      "    |      g      s |\n",
      "----+---------------+\n",
      "neg | <36.9%> 13.9% |\n",
      "pos |  13.8% <35.4%>|\n",
      "----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\tPrecision\tRecall\t\tF1\n",
      "pos \t      0.720      0.718      0.719\n",
      "neg \t      0.726      0.728      0.727\n"
     ]
    }
   ],
   "source": [
    "## other evaluation measures:  confusion matrix, precision, recall, F1 ##\n",
    "\n",
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in test_set:\n",
    "    \tgoldlist.append(label)\n",
    "    \tpredictedlist.append(classifier.classify(features))\n",
    "\n",
    "# look at the first 30 examples\n",
    "print(goldlist[:30])\n",
    "print(predictedlist[:30])\n",
    "\n",
    "cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "print(cm.pretty_format(sort_by_count=True, truncate=9))\n",
    "\n",
    "# or show the results as percentages\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))\n",
    "\n",
    "# Function to compute precision, recall and F1 for each label\n",
    "#  and for any number of labels\n",
    "# Input: list of gold labels, list of predicted labels (in same order)\n",
    "# Output:  prints precision, recall and F1 for each label\n",
    "def eval_measures(gold, predicted):\n",
    "    # get a list of labels\n",
    "    labels = list(set(gold))\n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)\n",
    "        precision = TP / (TP + FN)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "\n",
    "# call the function with our data\n",
    "eval_measures(goldlist, predictedlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2132\n",
      "0 0.7354596622889306\n",
      "1 0.7434333958724203\n",
      "2 0.726078799249531\n",
      "3 0.7551594746716698\n",
      "4 0.7382739212007504\n",
      "mean accuracy 0.7396810506566605\n"
     ]
    }
   ],
   "source": [
    "# Exercise:\n",
    "# - cross-validation for the bigram feature-sets\n",
    "cross_validation_accuracy(num_folds, bigram_featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2132\n",
      "0 0.7303001876172608\n",
      "1 0.7401500938086304\n",
      "2 0.7274859287054409\n",
      "3 0.75093808630394\n",
      "4 0.7321763602251408\n",
      "mean accuracy 0.7362101313320826\n"
     ]
    }
   ],
   "source": [
    "# Exercise:\n",
    "# - cross-validation for the POS feature-sets\n",
    "cross_validation_accuracy(num_folds, POS_featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
